//
// Generated by LLVM NVPTX Back-End
//

.version 6.4
.target sm_70
.address_size 64

	// .globl	fusion_broadcast_kernel
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};

.visible .entry fusion_broadcast_kernel(
	.param .u64 fusion_broadcast_kernel_param_0,
	.param .u64 fusion_broadcast_kernel_param_1,
	.param .u64 fusion_broadcast_kernel_param_2,
	.param .u64 fusion_broadcast_kernel_param_3,
	.param .u64 fusion_broadcast_kernel_param_4,
	.param .u64 fusion_broadcast_kernel_param_5,
	.param .u64 fusion_broadcast_kernel_param_6,
	.param .u64 fusion_broadcast_kernel_param_7,
	.param .u64 fusion_broadcast_kernel_param_8,
	.param .u64 fusion_broadcast_kernel_param_9,
	.param .u64 fusion_broadcast_kernel_param_10,
	.param .u64 fusion_broadcast_kernel_param_11,
	.param .u64 fusion_broadcast_kernel_param_12,
	.param .u64 fusion_broadcast_kernel_param_13,
	.param .u64 fusion_broadcast_kernel_param_14,
	.param .u64 fusion_broadcast_kernel_param_15,
	.param .u64 fusion_broadcast_kernel_param_16,
	.param .u64 fusion_broadcast_kernel_param_17,
	.param .u64 fusion_broadcast_kernel_param_18,
	.param .u64 fusion_broadcast_kernel_param_19,
	.param .u64 fusion_broadcast_kernel_param_20,
	.param .u64 fusion_broadcast_kernel_param_21,
	.param .u64 fusion_broadcast_kernel_param_22,
	.param .u64 fusion_broadcast_kernel_param_23,
	.param .u64 fusion_broadcast_kernel_param_24,
	.param .u64 fusion_broadcast_kernel_param_25,
	.param .u64 fusion_broadcast_kernel_param_26,
	.param .u64 fusion_broadcast_kernel_param_27,
	.param .u64 fusion_broadcast_kernel_param_28,
	.param .u64 fusion_broadcast_kernel_param_29,
	.param .u64 fusion_broadcast_kernel_param_30
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<5>;
	.reg .f32 	%f<20>;
	.reg .b64 	%rd<37>;

	ld.param.u64 	%rd17, [fusion_broadcast_kernel_param_12];
	ld.param.u64 	%rd19, [fusion_broadcast_kernel_param_1];
	ld.param.u64 	%rd20, [fusion_broadcast_kernel_param_21];
	mov.u32 	%r1, %ctaid.x;
	cvt.s64.s32 	%rd1, %r1;
	mov.u32 	%r2, %tid.x;
	cvt.s64.s32 	%rd2, %r2;
	mul.wide.s32 	%rd21, %r1, 442368;
	mul.wide.s32 	%rd22, %r2, 36864;
	add.s64 	%rd23, %rd21, %rd22;
	add.s64 	%rd32, %rd20, %rd23;
	add.s64 	%rd31, %rd19, %rd23;
	mov.u64 	%rd33, 0;
	mul.lo.s64 	%rd25, %rd1, 1152;
	mul.lo.s64 	%rd26, %rd2, 96;
	add.s64 	%rd27, %rd25, %rd26;
	mov.f32 	%f4, 0f3F000000;
	mov.f32 	%f5, 0f3BBB989D;
	mov.f32 	%f8, 0f4B400001;
	mov.f32 	%f9, 0f437C0000;
	bra.uni 	$L__BB0_1;
$L__BB0_5:
	add.s64 	%rd33, %rd33, 1;
	add.s64 	%rd32, %rd32, 384;
	add.s64 	%rd31, %rd31, 384;
$L__BB0_1:
	setp.gt.s64 	%p1, %rd33, 95;
	@%p1 bra 	$L__BB0_6;
	mov.u64 	%rd36, 0;
	add.s64 	%rd28, %rd27, %rd33;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd17, %rd29;
	mov.u64 	%rd34, %rd31;
	mov.u64 	%rd35, %rd32;
$L__BB0_3:
	setp.gt.s64 	%p2, %rd36, 95;
	@%p2 bra 	$L__BB0_5;
	ld.f32 	%f1, [%rd34];
	ld.f32 	%f2, [%rd30];
	sub.rn.f32 	%f3, %f1, %f2;
	fma.rn.f32 	%f6, %f3, %f5, %f4;
	cvt.sat.f32.f32 	%f7, %f6;
	fma.rm.f32 	%f10, %f7, %f9, %f8;
	add.rn.f32 	%f11, %f10, 0fCB40007F;
	neg.f32 	%f12, %f11;
	mov.f32 	%f13, 0f3FB8AA3B;
	fma.rn.f32 	%f14, %f3, %f13, %f12;
	mov.f32 	%f15, 0f32A57060;
	fma.rn.f32 	%f16, %f3, %f15, %f14;
	mov.b32 	%r3, %f10;
	shl.b32 	%r4, %r3, 23;
	mov.b32 	%f17, %r4;
	ex2.approx.ftz.f32 	%f18, %f16;
	mul.rn.f32 	%f19, %f18, %f17;
	st.f32 	[%rd35], %f19;
	add.s64 	%rd36, %rd36, 1;
	add.s64 	%rd35, %rd35, 4;
	add.s64 	%rd34, %rd34, 4;
	bra.uni 	$L__BB0_3;
$L__BB0_6:
	ret;

}
