From 3cf8a37a71465f9c466b1fdea5b8d0b5e97114af Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Wed, 20 Dec 2023 03:17:15 -0500
Subject: [PATCH 01/11] [StableHlo] add more ops lowering to StableHlo

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 src/Conversion/ONNXToStableHlo/CMakeLists.txt |   4 +
 .../ConvertONNXToStableHlo.cpp                |  31 +-
 .../ONNXToStableHlo/DialectBuilder.cpp        |  57 +
 .../ONNXToStableHlo/DialectBuilder.hpp        |  43 +
 .../ONNXToStableHlo/Math/Elementwise.cpp      |  49 +-
 .../ONNXToStableHlo/ONNXToStableHloCommon.cpp | 160 ++-
 .../ONNXToStableHlo/ONNXToStableHloCommon.hpp |  43 +-
 src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp   | 830 +++++++++++++++
 .../ONNXToStableHlo/RNN/RNNBase.cpp           | 234 +++++
 .../ONNXToStableHlo/RNN/RNNBase.hpp           | 203 ++++
 .../ONNXToStableHlo/Tensor/OneHot.cpp         | 129 +++
 src/Conversion/ONNXToStableHlo/Tensor/Pad.cpp |   2 +-
 .../ONNXToStableHlo/Tensor/ScatterND.cpp      |  92 ++
 src/Tools/onnx-mlir-opt/RegisterPasses.cpp    |   4 +
 .../onnx_to_stablehlo/Math/Elementwise.mlir   |  52 +
 .../onnx_to_stablehlo/RNN/LSTM-loop.mlir      | 621 +++++++++++
 .../onnx_to_stablehlo/RNN/LSTM.mlir           | 991 ++++++++++++++++++
 .../onnx_to_stablehlo/Tensor/OneHot.mlir      |  31 +
 .../onnx_to_stablehlo/Tensor/ScatterND.mlir   |  29 +
 19 files changed, 3583 insertions(+), 22 deletions(-)
 create mode 100644 src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp
 create mode 100644 src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp
 create mode 100644 src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp
 create mode 100644 src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
 create mode 100644 src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp
 create mode 100644 test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM-loop.mlir
 create mode 100644 test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM.mlir
 create mode 100644 test/mlir/conversion/onnx_to_stablehlo/Tensor/OneHot.mlir
 create mode 100644 test/mlir/conversion/onnx_to_stablehlo/Tensor/ScatterND.mlir

diff --git a/src/Conversion/ONNXToStableHlo/CMakeLists.txt b/src/Conversion/ONNXToStableHlo/CMakeLists.txt
index 16bfd487fe..e1cd1b2157 100644
--- a/src/Conversion/ONNXToStableHlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToStableHlo/CMakeLists.txt
@@ -50,6 +50,8 @@ add_onnx_mlir_library(OMONNXToStableHlo
   NN/ConvTranspose.cpp
   NN/Normalization.cpp
   NN/Pooling.cpp
+  RNN/LSTM.cpp
+  RNN/RNNBase.cpp
   Tensor/ArgMax.cpp
   Tensor/Concat.cpp
   Tensor/Constant.cpp
@@ -58,8 +60,10 @@ add_onnx_mlir_library(OMONNXToStableHlo
   Tensor/Gather.cpp
   Tensor/GatherElements.cpp
   Tensor/Identity.cpp
+  Tensor/OneHot.cpp
   Tensor/Pad.cpp
   Tensor/Reshape.cpp
+  Tensor/ScatterND.cpp
   Tensor/Shape.cpp
   Tensor/Slice.cpp
   Tensor/Split.cpp
diff --git a/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp b/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp
index 4f5d15b09c..03a97790d5 100644
--- a/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp
+++ b/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp
@@ -23,7 +23,7 @@ using namespace mlir;
 namespace onnx_mlir {

 void populateONNXToStableHloConversionPattern(
-    RewritePatternSet &patterns, MLIRContext *ctx) {
+    RewritePatternSet &patterns, MLIRContext *ctx, bool enableUnroll) {
   // Math
   populateLoweringONNXClipOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXElementwiseOpToStableHloPattern(patterns, ctx);
@@ -35,6 +35,8 @@ void populateONNXToStableHloConversionPattern(
   populateLoweringONNXConvTransposeOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXNormalizationOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXPoolingOpToStableHloPattern(patterns, ctx);
+  // Recurrent neural network
+  populateLoweringONNXLSTMOpToStableHloPattern(patterns, ctx, enableUnroll);
   // Tensor
   populateLoweringONNXArgMaxOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXConcatOpToStableHloPattern(patterns, ctx);
@@ -44,8 +46,10 @@ void populateONNXToStableHloConversionPattern(
   populateLoweringONNXGatherOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXGatherElementsOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXIdentityOpToStableHloPattern(patterns, ctx);
+  populateLoweringONNXOneHotOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXPadOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXReshapeOpToStableHloPattern(patterns, ctx);
+  populateLoweringONNXScatterNDOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXShapeOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXSliceOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXSplitOpToStableHloPattern(patterns, ctx);
@@ -75,12 +79,27 @@ struct FrontendToStableHloLoweringPass
   FrontendToStableHloLoweringPass(const FrontendToStableHloLoweringPass &pass)
       : PassWrapper<FrontendToStableHloLoweringPass,
             OperationPass<ModuleOp>>() {}
+  FrontendToStableHloLoweringPass(bool enableUnroll) {
+    // Below, need explicit assignment to enable implicit conversion of bool
+    // to Option<bool>.
+    this->enableUnroll = enableUnroll;
+  }

   void getDependentDialects(DialectRegistry &registry) const override {
     registry.insert<mlir::stablehlo::StablehloDialect>();
+    registry.insert<shape::ShapeDialect>();
   }

   void runOnOperation() final;
+
+public:
+  // Some ops (RNN ops for example) will have loops inside them. We can
+  // choose to unroll the loop, which means to expand the loops completely
+  // so there are no loops left, or to rewrite the loop into stablehlo::WhileOp
+  Option<bool> enableUnroll{*this, "enable-unroll",
+      llvm::cl::desc(
+          "Enable unroll rather than lowering to stablehlo::WhileOp."),
+      llvm::cl::init(true)};
 };

 void FrontendToStableHloLoweringPass::runOnOperation() {
@@ -96,16 +115,14 @@ void FrontendToStableHloLoweringPass::runOnOperation() {
   target.addLegalDialect<stablehlo::StablehloDialect, func::FuncDialect,
       arith::ArithDialect, shape::ShapeDialect, mlir::affine::AffineDialect,
       tensor::TensorDialect>();
-  // Needed to support unsigned int computations. To be removed if we use a
-  // scheme that does not rely on the UnrealizedConversionCastOp.
-  target.addLegalOp<::mlir::UnrealizedConversionCastOp>();

   // Now that the conversion target has been defined, we just need to provide
   // the set of patterns that will lower the frontend operations.
   RewritePatternSet patterns(&getContext());

   // Define patterns.
-  populateONNXToStableHloConversionPattern(patterns, &getContext());
+  populateONNXToStableHloConversionPattern(
+      patterns, &getContext(), enableUnroll);

   // add illegal op
   target.addIllegalOp<ONNXSoftmaxOp>();
@@ -122,4 +139,8 @@ std::unique_ptr<Pass> createLowerToStableHloPass() {
   return std::make_unique<FrontendToStableHloLoweringPass>();
 }

+std::unique_ptr<Pass> createLowerToStableHloPass(bool enableUnroll) {
+  return std::make_unique<FrontendToStableHloLoweringPass>(enableUnroll);
+}
+
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp b/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
index 9193b3cf38..10f959097e 100644
--- a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
+++ b/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
@@ -14,6 +14,7 @@
 //===----------------------------------------------------------------------===//

 #include "mlir/Dialect/Arith/IR/Arith.h"
+#include "llvm/ADT/TypeSwitch.h"
 #include "stablehlo/dialect/StablehloOps.h"

 #include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
@@ -24,6 +25,62 @@ using namespace mlir;

 namespace onnx_mlir {

+Value StablehloBuilder::constant(mlir::Type type, double val) const {
+  Value constant = nullptr;
+  // Could be a vector type; look at the element type.
+  Type elementType = type;
+  VectorType vectorType = type.dyn_cast<VectorType>();
+  if (vectorType)
+    elementType = vectorType.getElementType();
+  TypeSwitch<Type>(elementType)
+      .Case<Float16Type>([&](Type) {
+        constant =
+            b().create<stablehlo::ConstantOp>(loc(), b().getF16FloatAttr(val));
+      })
+      .Case<Float32Type>([&](Type) {
+        constant =
+            b().create<stablehlo::ConstantOp>(loc(), b().getF32FloatAttr(val));
+      })
+      .Case<Float64Type>([&](Type) {
+        constant =
+            b().create<stablehlo::ConstantOp>(loc(), b().getF64FloatAttr(val));
+      })
+      .Case<IntegerType>([&](IntegerType elementType) {
+        assert(val == (int64_t)val && "value is ambiguous");
+        unsigned width = elementType.getWidth();
+
+        if (width == 1)
+          constant =
+              b().create<stablehlo::ConstantOp>(loc(), b().getBoolAttr(val != 0));
+        else {
+          if (elementType.isUnsignedInteger()) {
+            constant = b().create<stablehlo::ConstantOp>(loc(),
+                b().getIntegerAttr(elementType, APInt(width, (uint64_t)val, false)));
+          } else {
+            constant = b().create<stablehlo::ConstantOp>(loc(),
+                b().getIntegerAttr(elementType, APInt(width, (int64_t)val, true)));
+          }
+        }
+      })
+      .Case<IndexType>([&](Type elementType) {
+        constant = b().create<stablehlo::ConstantOp>(
+            loc(), b().getIntegerAttr(elementType, val));
+      })
+      .Default([](Type) { llvm_unreachable("unsupported element type"); });
+
+  assert(constant != nullptr && "Expecting valid constant value");
+  return constant;
+}
+
+Value StablehloBuilder::constantIndex(int64_t val) const {
+  IntegerAttr constantAttr = b().getIntegerAttr(b().getIndexType(), val);
+  return b().create<stablehlo::ConstantOp>(loc(), constantAttr);
+}
+
+Value StablehloBuilder::shaped_zero(mlir::Type type) const {
+  return b().create<stablehlo::ConstantOp>(loc(), b().getZeroAttr(type));
+}
+
 // =============================================================================
 // IndexExpr Builder for Lowering using Shape/StableHlo Dialect.
 // =============================================================================
diff --git a/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp b/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
index 7f182ae09d..820c94d58a 100644
--- a/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
+++ b/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
@@ -27,6 +27,33 @@

 namespace onnx_mlir {

+// =============================================================================
+// stablehlo Builder
+// =============================================================================
+
+struct StablehloBuilder : DialectBuilder {
+  StablehloBuilder(mlir::Location loc) : DialectBuilder(loc) {}
+  StablehloBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : DialectBuilder(b, loc), patternRewriter(&b) {}
+  StablehloBuilder(const DialectBuilder &db) : DialectBuilder(db) {}
+  virtual ~StablehloBuilder() {}
+
+  // ConstantOp
+  mlir::Value constant(mlir::Type type, double val) const;
+  mlir::Value constantIndex(int64_t val) const;
+  mlir::Value shaped_zero(mlir::Type type) const;
+
+protected:
+  // Private getters of builder (concise version).
+  mlir::OpBuilder &rewriter() const {
+    assert(patternRewriter && "rewriter is null");
+    return *patternRewriter;
+  }
+
+private:
+  mlir::OpBuilder *patternRewriter;
+};
+
 // =============================================================================
 // IndexExpr Builder for Shape lowering
 // =============================================================================
@@ -45,6 +72,22 @@ struct IndexExprBuilderForStableHlo : IndexExprBuilder {
   mlir::Value getShapeVal(mlir::Value tensorOrMemrefValue, uint64_t i) final;
 };

+// =============================================================================
+// MultiDialectBuilder for Stablehlo
+// =============================================================================
+
+// Recursive class specialized for StablehloBuilder referred to as
+// stablehlo.
+template <class... Ts>
+struct MultiDialectBuilder<StablehloBuilder, Ts...>
+    : MultiDialectBuilder<Ts...> {
+  MultiDialectBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : MultiDialectBuilder<Ts...>(b, loc), stablehlo(b, loc) {}
+  MultiDialectBuilder(const DialectBuilder &db)
+      : MultiDialectBuilder<Ts...>(db), stablehlo(db) {}
+  StablehloBuilder stablehlo;
+};
+
 // Recursive class specialized for AffineBuilder refereed to as affine.
 template <class... Ts>
 struct MultiDialectBuilder<IndexExprBuilderForStableHlo, Ts...>
diff --git a/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp b/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
index 5970fdf68b..a161d0bea4 100644
--- a/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
@@ -66,6 +66,11 @@ struct StableHloDialectOp<ONNXMaxOp> {
   using Op = stablehlo::MaxOp;
 };

+template <>
+struct StableHloDialectOp<ONNXMinOp> {
+  using Op = stablehlo::MinOp;
+};
+
 template <>
 struct StableHloDialectOp<ONNXMulOp> {
   using Op = stablehlo::MulOp;
@@ -106,6 +111,11 @@ struct StableHloDialectOp<ONNXTanhOp> {
   using Op = stablehlo::TanhOp;
 };

+template <>
+struct StableHloDialectOp<ONNXWhereOp> {
+  using Op = stablehlo::SelectOp;
+};
+
 namespace {

 template <typename ONNXOp>
@@ -296,6 +306,40 @@ struct ONNXElementwiseBinaryOpLoweringToStableHlo : public ConversionPattern {
   }
 };

+// ONNXPReluOp(x) = alpha * x if x < 0 else x.
+template <>
+struct ONNXElementwiseBinaryOpLoweringToStableHlo<ONNXPReluOp>
+    : public ConversionPattern {
+  ONNXElementwiseBinaryOpLoweringToStableHlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXPReluOp::getOperationName(), 1, ctx) {}
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    // Prior code here used the "analysis" version that did not generate code.
+    // Since code is actually not needed here at this time, one could use
+    // IndexExprBuilderForAnalysis createIE(loc) instead.
+    IndexExprBuilderForStableHlo createShapeIE(rewriter, loc);
+    ONNXBroadcastOpShapeHelper shapeHelper(op, operands, &createShapeIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    int64_t outputRank = shapeHelper.outputRank;
+    llvm::SmallVector<Value, 4> broadcastedOperands =
+        getBroadcastedOperands(op, rewriter, loc, outputRank);
+    Value inp = broadcastedOperands[0];
+    Value broadcastedSlope = broadcastedOperands[1];
+    Type resultType = *op->result_type_begin();
+    Value PReluActivationVal =
+        rewriter.create<stablehlo::MulOp>(loc, inp, broadcastedSlope);
+    Value broadcastedZero = getShapedZero(loc, rewriter, inp);
+    Value compareGtZero = rewriter.create<stablehlo::CompareOp>(
+        loc, inp, broadcastedZero, stablehlo::ComparisonDirection::GT);
+    Value resultOp = rewriter.create<stablehlo::SelectOp>(
+        loc, resultType, compareGtZero, inp, PReluActivationVal);
+    rewriter.replaceOp(op, resultOp);
+    return success();
+  }
+};
+
 // Element-wise variadic ops lowering to StableHlo dialect.
 //===----------------------------------------------------------------------===//
 template <typename ElementwiseVariadicOp>
@@ -346,12 +390,15 @@ void populateLoweringONNXElementwiseOpToStableHloPattern(
       ONNXElementwiseCompareBinaryOpLoweringToStableHlo<ONNXLessOp>,
       ONNXElementwiseCompareBinaryOpLoweringToStableHlo<ONNXLessOrEqualOp>,
       ONNXElementwiseBinaryOpLoweringToStableHlo<ONNXPowOp>,
+      ONNXElementwiseBinaryOpLoweringToStableHlo<ONNXPReluOp>,
       ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXAddOp>,
       ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXAndOp>,
       ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXDivOp>,
       ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXMaxOp>,
+      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXMinOp>,
       ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXMulOp>,
-      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXSubOp>>(ctx);
+      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXSubOp>,
+      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXWhereOp>>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.cpp b/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.cpp
index 697968403e..c5b4c006bc 100644
--- a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.cpp
+++ b/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.cpp
@@ -15,6 +15,9 @@
 //===----------------------------------------------------------------------===//

 #include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/OpHelper.hpp"
+#include "src/Dialect/ONNX/OnnxElementsAttrBuilder.hpp"
+
 #include "stablehlo/dialect/BroadcastUtils.h"

 using namespace mlir;
@@ -45,11 +48,6 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(Operation *op,
   Type outputType = *op->result_type_begin();
   assert(outputType.isa<ShapedType>() && "output type is not shaped");
   ShapedType outputShapedType = outputType.cast<ShapedType>();
-  Type elementType =
-      op->getOperands()[0].getType().dyn_cast<ShapedType>().getElementType();
-  RankedTensorType broadcastedOutputType =
-      RankedTensorType::get(outputShapedType.getShape(), elementType);
-
   Value resultExtents =
       mlir::hlo::computeNaryElementwiseBroadcastingResultExtents(
           loc, op->getOperands(), rewriter);
@@ -59,6 +57,10 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(Operation *op,
     assert(operandType != nullptr && "operand type is not ranked");
     SmallVector<int64_t, 4> broadcastDimensions = llvm::to_vector<4>(
         llvm::seq<int64_t>(outputRank - operandType.getRank(), outputRank));
+    Type elementType =
+        operand.getType().dyn_cast<ShapedType>().getElementType();
+    RankedTensorType broadcastedOutputType =
+        RankedTensorType::get(outputShapedType.getShape(), elementType);
     Value broadcast = rewriter.create<stablehlo::DynamicBroadcastInDimOp>(loc,
         broadcastedOutputType, operand, resultExtents,
         rewriter.getI64TensorAttr(broadcastDimensions));
@@ -73,11 +75,6 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   llvm::SmallVector<Value, 4> broadcastedOperands;
   assert(outputType.isa<ShapedType>() && "output type is not shaped");
   ShapedType outputShapedType = outputType.cast<ShapedType>();
-  Type elementType =
-      operands[0].getType().dyn_cast<ShapedType>().getElementType();
-  RankedTensorType broadcastedOutputType =
-      RankedTensorType::get(outputShapedType.getShape(), elementType);
-
   Value resultExtents =
       mlir::hlo::computeNaryElementwiseBroadcastingResultExtents(
           loc, operands, rewriter);
@@ -87,6 +84,10 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     assert(operandType != nullptr && "operand type is not ranked");
     SmallVector<int64_t, 4> broadcastDimensions = llvm::to_vector<4>(
         llvm::seq<int64_t>(outputRank - operandType.getRank(), outputRank));
+    Type elementType =
+        operands[0].getType().dyn_cast<ShapedType>().getElementType();
+    RankedTensorType broadcastedOutputType =
+        RankedTensorType::get(outputShapedType.getShape(), elementType);
     Value broadcast = rewriter.create<stablehlo::DynamicBroadcastInDimOp>(loc,
         broadcastedOutputType, operand, resultExtents,
         rewriter.getI64TensorAttr(broadcastDimensions));
@@ -95,16 +96,147 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   return broadcastedOperands;
 }

-ElementsAttr getElementAttributeFromStablehloValue(Value value) {
+ElementsAttr getElementAttributeFromConstValue(Value value) {
   auto definingOp = value.getDefiningOp();
-  if (auto constantOp = dyn_cast_or_null<stablehlo::ConstantOp>(definingOp))
+  if (auto constantOp = dyn_cast_or_null<stablehlo::ConstantOp>(definingOp)) {
     return constantOp.getValue().dyn_cast<ElementsAttr>();
-  else if (auto constantOp =
-               dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
+  } else if (auto constantOp =
+                 dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
     if (constantOp.getValue().has_value())
       return constantOp.getValueAttr().dyn_cast<ElementsAttr>();
   }
   return nullptr;
 }

+DenseIntElementsAttr GetI64ElementsAttr(
+    ArrayRef<int64_t> values, Builder *builder) {
+  RankedTensorType ty = RankedTensorType::get(
+      {static_cast<int64_t>(values.size())}, builder->getIntegerType(64));
+  return DenseIntElementsAttr::get(ty, values);
+}
+
+namespace {
+// Returns the DenseElementsAttr of input if it's a stablehlo constant or
+// onnx.Constant. Otherwise returns a nullptr attribute.
+DenseElementsAttr getDenseElementAttrFromConstValue(mlir::Value value) {
+  Operation *definingOp = value.getDefiningOp();
+  if (auto globalOp = dyn_cast_or_null<stablehlo::ConstantOp>(definingOp)) {
+    return globalOp.getValueAttr().dyn_cast<DenseElementsAttr>();
+  } else if (auto constOp =
+                 dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
+    if (constOp.getValue().has_value())
+      return constOp.getValueAttr().dyn_cast<DenseElementsAttr>();
+  }
+  return nullptr;
+}
+} // namespace
+
+// Emit an ONNXSqueezeOp. If the input is constant, do const propagation,
+/// and return a constant.
+Value foldOrEmitONNXSqueezeOpStableHlo(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, int64_t axis) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr squeezedElements = inputElements.reshape(tensorType);
+    Value constVal = create.onnx.constant(squeezedElements);
+    return constVal;
+  } else {
+    return rewriter
+        .create<ONNXSqueezeOp>(loc, tensorType, create.onnx.toTensor(input),
+            create.onnx.constantInt64({axis}))
+        .getResult();
+  }
+}
+
+/// Emit an ONNXUnsqueezeOp. If the input is constant, do const
+/// propagation, and return a constant.
+Value foldOrEmitONNXUnsqueezeOpStableHlo(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, int64_t axis) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr unsqueezedElements = inputElements.reshape(tensorType);
+    Value constVal = create.onnx.constant(unsqueezedElements);
+    return constVal;
+  } else {
+    return rewriter
+        .create<ONNXUnsqueezeOp>(loc, tensorType, create.onnx.toTensor(input),
+            create.onnx.constantInt64({axis}))
+        .getResult();
+  }
+}
+
+/// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
+/// return constants.
+/// Only support evenly splitting.
+std::vector<Value> foldOrEmitONNXSplitOpStableHlo(
+    ConversionPatternRewriter &rewriter, Location loc,
+    ArrayRef<Type> resultTypes, Value input, int64_t axis) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  std::vector<Value> resVals;
+  int outputNum = resultTypes.size();
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    auto inputShape = inputElements.getType().getShape();
+    assert(outputNum == 0 || inputShape[axis] % outputNum == 0);
+    int64_t sizeOfEachSplit = outputNum != 0 ? inputShape[axis] / outputNum : 0;
+    SmallVector<int64_t, 4> sizes(outputNum, sizeOfEachSplit);
+
+    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
+    std::vector<ElementsAttr> splits =
+        elementsBuilder.split(inputElements, axis, sizes);
+    for (ElementsAttr splitElements : splits) {
+      // Avoid DisposableElementsAttr during conversion.
+      DenseElementsAttr denseSplitElements =
+          elementsBuilder.toDenseElementsAttr(splitElements);
+      Value constVal = create.onnx.constant(denseSplitElements);
+      resVals.emplace_back(constVal);
+    }
+  } else {
+    SmallVector<Type, 4> convertedTypes;
+    SmallVector<int64_t> splitSizesI64;
+    for (auto t : resultTypes) {
+      convertedTypes.emplace_back(create.onnx.toTensor(t));
+      splitSizesI64.emplace_back(t.cast<ShapedType>().getShape()[axis]);
+    }
+    Value splitSizes = create.onnx.constantInt64(splitSizesI64);
+    ONNXSplitOp split = rewriter.create<ONNXSplitOp>(loc, convertedTypes,
+        create.onnx.toTensor(input), splitSizes,
+        /*axis=*/axis, nullptr);
+    for (int i = 0; i < outputNum; ++i)
+      resVals.emplace_back(split.getOutputs()[i]);
+  }
+  return resVals;
+}
+
+/// Emit an ONNXTransposeOp. If the input is constant, do const propagation,
+/// and return a constant.
+Value foldOrEmitONNXTransposeOpStableHlo(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, ArrayAttr permAttr) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    SmallVector<uint64_t, 4> perm;
+    for (auto permVal : permAttr.getValue())
+      perm.emplace_back(permVal.cast<IntegerAttr>().getInt());
+
+    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
+    ElementsAttr transposedElements =
+        elementsBuilder.transpose(inputElements, perm);
+    // Avoid DisposableElementsAttr during conversion.
+    DenseElementsAttr denseTransposedElements =
+        elementsBuilder.toDenseElementsAttr(transposedElements);
+    Value constVal = create.onnx.constant(denseTransposedElements);
+    return constVal;
+  } else {
+    return rewriter
+        .create<ONNXTransposeOp>(loc, create.onnx.toTensor(resultType),
+            create.onnx.toTensor(input), permAttr)
+        .getResult();
+  }
+}
+
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp b/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp
index 824df1e1b0..411c598ed6 100644
--- a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp
+++ b/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp
@@ -30,6 +30,8 @@

 #include "stablehlo/dialect/StablehloOps.h"

+#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
+#include "src/Dialect/Mlir/DialectBuilder.hpp"
 #include "src/Dialect/Mlir/IndexExpr.hpp"
 #include "src/Dialect/ONNX/DialectBuilder.hpp"
 #include "src/Dialect/ONNX/ONNXOps.hpp"
@@ -116,7 +118,39 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     llvm::SmallVector<Value, 4> &operands, Type outputType,
     ConversionPatternRewriter &rewriter, Location loc, int64_t outputRank);

-mlir::ElementsAttr getElementAttributeFromStablehloValue(mlir::Value value);
+mlir::ElementsAttr getElementAttributeFromConstValue(mlir::Value value);
+
+DenseIntElementsAttr GetI64ElementsAttr(
+    ArrayRef<int64_t> values, Builder *builder);
+
+//===----------------------------------------------------------------------===//
+// Fold and emit support.
+//===----------------------------------------------------------------------===//
+
+/// Emit an ONNXSqueezeOp. If the input is constant, do const propagation, and
+/// return a constant.
+mlir::Value foldOrEmitONNXSqueezeOpStableHlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    mlir::Type resultType, mlir::Value input, int64_t axis);
+
+/// Emit an ONNXUnsqueezeOp. If the input is constant, do const propagation, and
+/// return a constant.
+mlir::Value foldOrEmitONNXUnsqueezeOpStableHlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    mlir::Type resultType, mlir::Value input, int64_t axis);
+
+/// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
+/// return constants.
+/// Only support evenly splitting.
+std::vector<mlir::Value> foldOrEmitONNXSplitOpStableHlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    llvm::ArrayRef<mlir::Type> resultTypes, mlir::Value input, int64_t axis);
+
+/// Emit an ONNXTransposeOp. If the input is constant, do const propagation, and
+/// return a constant.
+mlir::Value foldOrEmitONNXTransposeOpStableHlo(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    mlir::Type resultType, mlir::Value input, mlir::ArrayAttr permAttr);

 // `Math` directory methods:
 void populateLoweringONNXClipOpToStableHloPattern(
@@ -138,6 +172,9 @@ void populateLoweringONNXNormalizationOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXPoolingOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
+// `RNN` directory methods:
+void populateLoweringONNXLSTMOpToStableHloPattern(
+    RewritePatternSet &, MLIRContext *, bool);
 // `Tensor` directory methods:
 void populateLoweringONNXArgMaxOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
@@ -157,10 +194,14 @@ void populateLoweringONNXGatherElementsOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXIdentityOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXOneHotOpToStableHloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXPadOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXReshapeOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXScatterNDOpToStableHloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXShapeOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXSliceOpToStableHloPattern(
diff --git a/src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp b/src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp
new file mode 100644
index 0000000000..cdeafcee7e
--- /dev/null
+++ b/src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp
@@ -0,0 +1,830 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- LSTM.cpp - Lowering LSTM Op --------------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX LSTM Operators to StableHlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp"
+#include "src/Dialect/Mlir/DialectBuilder.hpp"
+
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "lstm"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace stablehlo {
+
+struct LstmState {
+  SmallVector<Value, 512> forwardAllH;
+  SmallVector<Value, 512> reverseAllH;
+
+  Value allHForward;
+  Value allHReverse;
+
+  Value ht;
+  Value ct;
+  // intermediate states.
+  Value forwardHt;
+  Value reverseHt;
+  Value forwardCt;
+  Value reverseCt;
+};
+
+struct LstmActivationPack {
+  RNNActivation f;
+  RNNActivation g;
+  RNNActivation h;
+};
+
+struct LstmWeightPack {
+  Value WT;
+  Value RT;
+};
+
+struct LstmBiasPack {
+  bool hasBias = false;
+  Value Wbi;
+  Value Wbo;
+  Value Wbf;
+  Value Wbc;
+  Value Rbi;
+  Value Rbo;
+  Value Rbf;
+  Value Rbc;
+  // Put peephole here.
+  bool hasPeephole = false;
+  Value Pi;
+  Value Po;
+  Value Pf;
+};
+
+template <>
+bool hasAllNoneOutput<ONNXLSTMOp>(ONNXLSTMOp *op) {
+  return (isNoneValue(op->getY()) && isNoneValue(op->getYH()) &&
+          isNoneValue(op->getYC()));
+}
+
+template <>
+std::tuple<LstmActivationPack, LstmActivationPack>
+getActivationPack<ONNXLSTMOp, LstmActivationPack>(ONNXLSTMOp *op) {
+  auto direction = op->getDirection();
+  auto activations = op->getActivations();
+  auto activationAlpha = op->getActivationAlpha();
+  auto activationBeta = op->getActivationBeta();
+
+  LstmActivationPack activationForward, activationReverse;
+
+  // Get activation function name.
+  // Default forward functions
+  activationForward.f.name = "sigmoid";
+  activationForward.g.name = "tanh";
+  activationForward.h.name = "tanh";
+  // Default backward functions
+  activationReverse.f.name = "sigmoid";
+  activationReverse.g.name = "tanh";
+  activationReverse.h.name = "tanh";
+  if (activations) {
+    ArrayAttr activationArrAttr = activations.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.name =
+            activationArrAttr[0].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.name =
+            activationArrAttr[1].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.name =
+            activationArrAttr[2].cast<StringAttr>().getValue();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.name =
+            activationArrAttr[startIndex].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.name =
+            activationArrAttr[startIndex + 1].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.name =
+            activationArrAttr[startIndex + 2].cast<StringAttr>().getValue();
+      }
+    }
+  }
+
+  // Get alpha attributes.
+  if (activationAlpha) {
+    ArrayAttr activationArrAttr = activationAlpha.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.alpha = activationArrAttr[0].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.alpha = activationArrAttr[1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.alpha = activationArrAttr[2].cast<FloatAttr>();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.alpha =
+            activationArrAttr[startIndex].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.alpha =
+            activationArrAttr[startIndex + 1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.alpha =
+            activationArrAttr[startIndex + 2].cast<FloatAttr>();
+      }
+    }
+  }
+
+  // Get beta attributes.
+  if (activationBeta) {
+    ArrayAttr activationArrAttr = activationBeta.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.beta = activationArrAttr[0].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.beta = activationArrAttr[1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.beta = activationArrAttr[2].cast<FloatAttr>();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.beta =
+            activationArrAttr[startIndex].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.beta =
+            activationArrAttr[startIndex + 1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.beta =
+            activationArrAttr[startIndex + 2].cast<FloatAttr>();
+      }
+    }
+  }
+
+  return std::make_tuple(activationForward, activationReverse);
+}
+
+template <>
+std::tuple<LstmWeightPack, LstmWeightPack>
+getWeightPack<ONNXLSTMOp, LstmWeightPack>(
+    ConversionPatternRewriter &rewriter, Location loc, ONNXLSTMOp *op) {
+  // Return values.
+  LstmWeightPack weightForward, weightReverse;
+
+  // parameter weight: [direction, 4*hiddenSize, inputSize]
+  Value W = op->getW();
+  // recurrence weight: [direction, 4*hiddenSize, hiddenSize]
+  Value R = op->getR();
+  // direction
+  StringRef direction = op->getDirection();
+
+  ArrayRef<int64_t> wShape = W.getType().cast<ShapedType>().getShape();
+  Type elementType = W.getType().cast<ShapedType>().getElementType();
+  int64_t hiddenSize = wShape[1] / 4;
+  int64_t inputSize = wShape[2];
+
+  // RankedTensorType types for parameter weights.
+  auto w3DTy =
+      RankedTensorType::get({1, 4 * hiddenSize, inputSize}, elementType);
+  auto w2DTy = RankedTensorType::get({4 * hiddenSize, inputSize}, elementType);
+  auto wTranspose2DTy =
+      RankedTensorType::get({inputSize, 4 * hiddenSize}, elementType);
+  SmallVector<Type, 4> w3D2Ty(2, w3DTy);
+
+  // RankedTensorType types for recurrence weights.
+  auto r3DTy =
+      RankedTensorType::get({1, 4 * hiddenSize, hiddenSize}, elementType);
+  auto r2DTy = RankedTensorType::get({4 * hiddenSize, hiddenSize}, elementType);
+  auto rTranspose2DTy =
+      RankedTensorType::get({hiddenSize, 4 * hiddenSize}, elementType);
+  SmallVector<Type, 4> r3D2Ty(2, r3DTy);
+
+  // Squeeze the direction axis from W and R.
+  Value fW, bW, fR, bR;
+  if (direction == FORWARD) {
+    fW = foldOrEmitONNXSqueezeOpStableHlo(rewriter, loc, w2DTy, W, /*axis=*/0);
+    fR = foldOrEmitONNXSqueezeOpStableHlo(rewriter, loc, r2DTy, R, /*axis=*/0);
+  } else if (direction == REVERSE) {
+    bW = foldOrEmitONNXSqueezeOpStableHlo(rewriter, loc, w2DTy, W, /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeOpStableHlo(rewriter, loc, r2DTy, R, /*axis=*/0);
+  } else { // BIDIRECTIONAL
+    // W
+    std::vector<Value> vals =
+        foldOrEmitONNXSplitOpStableHlo(rewriter, loc, w3D2Ty, W, 0);
+    fW = foldOrEmitONNXSqueezeOpStableHlo(
+        rewriter, loc, w2DTy, vals[0], /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeOpStableHlo(
+        rewriter, loc, w2DTy, vals[1], /*axis=*/0);
+    // R
+    vals.clear();
+    vals = foldOrEmitONNXSplitOpStableHlo(rewriter, loc, r3D2Ty, R, 0);
+    fR = foldOrEmitONNXSqueezeOpStableHlo(
+        rewriter, loc, r2DTy, vals[0], /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeOpStableHlo(
+        rewriter, loc, r2DTy, vals[1], /*axis=*/0);
+  }
+
+  // Transpose W and R.
+  ArrayAttr permAttr = rewriter.getI64ArrayAttr({1, 0});
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    // W
+    weightForward.WT = foldOrEmitONNXTransposeOpStableHlo(
+        rewriter, loc, wTranspose2DTy, fW, permAttr);
+    // R
+    weightForward.RT = foldOrEmitONNXTransposeOpStableHlo(
+        rewriter, loc, rTranspose2DTy, fR, permAttr);
+  }
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    // W
+    weightReverse.WT = foldOrEmitONNXTransposeOpStableHlo(
+        rewriter, loc, wTranspose2DTy, bW, permAttr);
+    // R
+    weightReverse.RT = foldOrEmitONNXTransposeOpStableHlo(
+        rewriter, loc, rTranspose2DTy, bR, permAttr);
+  }
+  return std::make_tuple(weightForward, weightReverse);
+}
+
+template <>
+std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
+    ConversionPatternRewriter &rewriter, Location loc, ONNXLSTMOp *op) {
+  // Return values.
+  LstmBiasPack biasForward, biasReverse;
+
+  // bias: [direction, 8*hiddenSize] for both parameter and recurrence weights.
+  Value B = op->getB();
+  // peephold: [direction, 3*hiddenSize] for input, output and forget gates.
+  Value P = op->getP();
+
+  // direction
+  StringRef direction = op->getDirection();
+
+  // Split B.
+  if (!isNoneValue(B)) {
+    ArrayRef<int64_t> bShape = B.getType().cast<ShapedType>().getShape();
+    Type elementType = B.getType().cast<ShapedType>().getElementType();
+    int64_t hiddenSize = bShape[1] / 8;
+
+    // MemRef types.
+    auto bType2D = RankedTensorType::get({1, 8 * hiddenSize}, elementType);
+    auto bType1D = RankedTensorType::get({8 * hiddenSize}, elementType);
+    auto bSplitType1D = RankedTensorType::get({hiddenSize}, elementType);
+    SmallVector<Type, 4> split1D8Ty(8, bSplitType1D);
+    SmallVector<Type, 4> split2D2Ty(2, bType2D);
+
+    // Squeeze the direction axis from B.
+    Value fB, bB;
+    if (direction == FORWARD) {
+      fB = foldOrEmitONNXSqueezeOpStableHlo(
+          rewriter, loc, bType1D, B, /*axis=*/0);
+    } else if (direction == REVERSE) {
+      bB = foldOrEmitONNXSqueezeOpStableHlo(
+          rewriter, loc, bType1D, B, /*axis=*/0);
+    } else { // BIDIRECTIONAL
+      std::vector<Value> vals;
+      vals = foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split2D2Ty, B, 0);
+      fB = foldOrEmitONNXSqueezeOpStableHlo(
+          rewriter, loc, bType1D, vals[0], /*axis=*/0);
+      bB = foldOrEmitONNXSqueezeOpStableHlo(
+          rewriter, loc, bType1D, vals[1], /*axis=*/0);
+    }
+
+    // Split B into individual bias tensors.
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split1D8Ty, fB, 0);
+      biasForward.Wbi = vals[0];
+      biasForward.Wbo = vals[1];
+      biasForward.Wbf = vals[2];
+      biasForward.Wbc = vals[3];
+      biasForward.Rbi = vals[4];
+      biasForward.Rbo = vals[5];
+      biasForward.Rbf = vals[6];
+      biasForward.Rbc = vals[7];
+      biasForward.hasBias = true;
+    }
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split1D8Ty, bB, 0);
+      biasReverse.Wbi = vals[0];
+      biasReverse.Wbo = vals[1];
+      biasReverse.Wbf = vals[2];
+      biasReverse.Wbc = vals[3];
+      biasReverse.Rbi = vals[4];
+      biasReverse.Rbo = vals[5];
+      biasReverse.Rbf = vals[6];
+      biasReverse.Rbc = vals[7];
+      biasReverse.hasBias = true;
+    }
+  }
+
+  // Split P.
+  if (!isNoneValue(P)) {
+    ArrayRef<int64_t> pShape = P.getType().cast<ShapedType>().getShape();
+    Type elementType = P.getType().cast<ShapedType>().getElementType();
+    int64_t hiddenSize = pShape[1] / 3;
+
+    // MemRef types.
+    auto pType2D = RankedTensorType::get({1, 3 * hiddenSize}, elementType);
+    auto pType1D = RankedTensorType::get({3 * hiddenSize}, elementType);
+    auto pSplitType1D = RankedTensorType::get({hiddenSize}, elementType);
+    SmallVector<Type, 4> split1D3Ty(3, pSplitType1D);
+    SmallVector<Type, 4> split2D2Ty(2, pType2D);
+
+    // Squeeze the direction axis from P.
+    Value fP, bP;
+    if (direction == FORWARD) {
+      fP = foldOrEmitONNXSqueezeOpStableHlo(
+          rewriter, loc, pType1D, P, /*axis=*/0);
+    } else if (direction == REVERSE) {
+      bP = foldOrEmitONNXSqueezeOpStableHlo(
+          rewriter, loc, pType1D, P, /*axis=*/0);
+    } else { // BIDIRECTIONAL
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split2D2Ty, P, 0);
+      fP = foldOrEmitONNXSqueezeOpStableHlo(
+          rewriter, loc, pType1D, vals[0], /*axis=*/0);
+      bP = foldOrEmitONNXSqueezeOpStableHlo(
+          rewriter, loc, pType1D, vals[1], /*axis=*/0);
+    }
+
+    // Split P into individual tensors.
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split1D3Ty, fP, 0);
+      biasForward.Pi = vals[0];
+      biasForward.Po = vals[1];
+      biasForward.Pf = vals[2];
+      biasForward.hasPeephole = true;
+    }
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      std::vector<Value> vals =
+          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split1D3Ty, bP, 0);
+      biasReverse.Pi = vals[0];
+      biasReverse.Po = vals[1];
+      biasReverse.Pf = vals[2];
+      biasReverse.hasPeephole = true;
+    }
+  }
+
+  return std::make_tuple(biasForward, biasReverse);
+}
+
+template <>
+LstmState allocAndInitializeStates<ONNXLSTMOp, LstmState>(
+    ConversionPatternRewriter &rewriter, Location loc, ONNXLSTMOp *op,
+    typename ONNXLSTMOp::Adaptor operandAdaptor, bool enableUnroll) {
+  LstmState state;
+
+  // direction
+  StringRef direction = op->getDirection();
+
+  // allocation for the results of this operation.
+  // If the result is not returned, then no allocation happens.
+  if (!enableUnroll) {
+    if (direction == FORWARD || direction == BIDIRECTIONAL)
+      state.allHForward = allocAllHidden(
+          rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+    if (direction == REVERSE || direction == BIDIRECTIONAL)
+      state.allHReverse = allocAllHidden(
+          rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+  }
+  // Y :: [seq_length, num_directions, batch_size, hidden_size]
+  // Y_h :: [num_directions, batch_size, hidden_size]
+  state.ht = allocHiddenOrCell(rewriter, loc, operandAdaptor.getX(),
+      operandAdaptor.getW(), operandAdaptor.getR());
+  // Y_c :: [num_directions, batch_size, hidden_size]
+  state.ct = allocHiddenOrCell(rewriter, loc, operandAdaptor.getX(),
+      operandAdaptor.getW(), operandAdaptor.getR());
+
+  // Insert allocation and deallocation the intermediate Ht and Ct for the
+  // forward and reverse directions.
+  // Ht :: [batch_size, hidden_size]
+  // Ct :: [batch_size, hidden_size]
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    state.forwardHt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+    state.forwardCt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+  }
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    state.reverseHt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+    state.reverseCt = allocIntermediateState(
+        rewriter, loc, operandAdaptor.getX(), operandAdaptor.getR());
+  }
+
+  // Initialize Ht and Ct.
+  initializeIntermediateStates(rewriter, loc, state.forwardHt, state.reverseHt,
+      state.forwardCt, state.reverseCt, operandAdaptor.getInitialH(),
+      operandAdaptor.getInitialC(),
+      operandAdaptor.getX().getType().cast<RankedTensorType>().getElementType(),
+      direction, /*onlyHidden=*/false);
+  return state;
+}
+
+template <>
+void calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+    LstmBiasPack>(ConversionPatternRewriter &rewriter, Location loc, Value Xt,
+    LstmState &state, LstmActivationPack activationPack,
+    LstmWeightPack weightPack, LstmBiasPack biasPack, Value sequenceIV,
+    Value directionIV, Value sequenceLens, Value initialH, bool enableUnroll,
+    bool isForward) {
+  // Equations for LSTM.
+  // it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Pi (.) Ct-1 + Wbi + Rbi)
+  // ft = f(Xt*(Wf^T) + Ht-1*(Rf^T) + Pf (.) Ct-1 + Wbf + Rbf)
+  // ct = g(Xt*(Wc^T) + Ht-1*(Rc^T) + Wbc + Rbc)
+  // Ct = ft (.) Ct-1 + it (.) ct
+  // ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)
+  // Ht = ot (.) h(Ct)
+
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+
+  ArrayRef<int64_t> xtShape = Xt.getType().cast<ShapedType>().getShape();
+  int64_t batchSize = xtShape[0];
+
+  // Get Ht, Ct.
+  Value Ht = (isForward) ? state.forwardHt : state.reverseHt;
+  Value Ct = (isForward) ? state.forwardCt : state.reverseCt;
+
+  ArrayRef<int64_t> htShape = Ht.getType().cast<ShapedType>().getShape();
+  int64_t hiddenSize = htShape[1];
+
+  // Frequently used types.
+  RankedTensorType matrixType = Ht.getType().cast<RankedTensorType>();
+  Type elementType = matrixType.getElementType();
+  RankedTensorType matrixAllGatesType =
+      RankedTensorType::get({batchSize, 4 * hiddenSize}, elementType);
+
+  // Do matrix multiplications.
+  // Xt * (Wi^T ++ Wo^T ++ Wf^T ++ Wc^T)
+  // Ht * (Ri^T ++ Ro^T ++ Rf^T ++ Rc^T)
+  // where '++' is matrix concatenation.
+  // XtWT: [B, 4H], HtRT: [B, 4H]
+  Value XtWT = create.onnx.matmul(matrixAllGatesType, Xt, weightPack.WT);
+  Value HtRT = create.onnx.matmul(matrixAllGatesType, Ht, weightPack.RT);
+  Value commonSum = create.onnx.add(XtWT, HtRT);
+  RankedTensorType matrixSingleGateType =
+      RankedTensorType::get({batchSize, hiddenSize}, elementType);
+  Value zeroIndex = create.onnx.constantInt64({0});
+  Value oneIndex = create.onnx.constantInt64({1});
+  Value oneHiddenIndex = create.onnx.constantInt64({hiddenSize});
+  Value twoHiddenIndex = create.onnx.constantInt64({2 * hiddenSize});
+  Value threeHiddenIndex = create.onnx.constantInt64({3 * hiddenSize});
+  Value fourHiddenIndex = create.onnx.constantInt64({4 * hiddenSize});
+  Value it = create.onnx.slice(matrixSingleGateType, commonSum, zeroIndex,
+      oneHiddenIndex, oneIndex, oneIndex);
+  Value ot = create.onnx.slice(matrixSingleGateType, commonSum, oneHiddenIndex,
+      twoHiddenIndex, oneIndex, oneIndex);
+  Value ft = create.onnx.slice(matrixSingleGateType, commonSum, twoHiddenIndex,
+      threeHiddenIndex, oneIndex, oneIndex);
+  Value ct = create.onnx.slice(matrixSingleGateType, commonSum,
+      threeHiddenIndex, fourHiddenIndex, oneIndex, oneIndex);
+  if (biasPack.hasBias) {
+    it = create.onnx.add(it, biasPack.Wbi);
+    it = create.onnx.add(it, biasPack.Rbi);
+  }
+  if (biasPack.hasPeephole) {
+    Value PiCt = create.onnx.mul(biasPack.Pi, Ct);
+    it = create.onnx.add(it, PiCt);
+  }
+  it = applyActivation(rewriter, loc, activationPack.f, it);
+  if (biasPack.hasBias) {
+    ft = create.onnx.add(ft, biasPack.Wbf);
+    ft = create.onnx.add(ft, biasPack.Rbf);
+  }
+  if (biasPack.hasPeephole) {
+    Value PfCt = create.onnx.mul(biasPack.Pf, Ct);
+    ft = create.onnx.add(ft, PfCt);
+  }
+  ft = applyActivation(rewriter, loc, activationPack.f, ft);
+  if (biasPack.hasBias) {
+    ct = create.onnx.add(ct, biasPack.Wbc);
+    ct = create.onnx.add(ct, biasPack.Rbc);
+  }
+  ct = applyActivation(rewriter, loc, activationPack.g, ct);
+
+  Value ftCt = create.onnx.mul(ft, Ct);
+  Value itct = create.onnx.mul(it, ct);
+  Value nextCt = create.onnx.add(ftCt, itct);
+
+  if (biasPack.hasBias) {
+    ot = create.onnx.add(ot, biasPack.Wbo);
+    ot = create.onnx.add(ot, biasPack.Rbo);
+  }
+  if (biasPack.hasPeephole) {
+    Value PoCt = create.onnx.mul(biasPack.Po, nextCt);
+    ot = create.onnx.add(ot, PoCt);
+  }
+  ot = applyActivation(rewriter, loc, activationPack.f, ot);
+  // Ht = ot (.) h(Ct)
+  Value nextHt = applyActivation(rewriter, loc, activationPack.h, nextCt);
+  nextHt = create.onnx.mul(ot, nextHt);
+  if (isForward) {
+    state.forwardHt = nextHt;
+    state.forwardCt = nextCt;
+  } else {
+    state.reverseHt = nextHt;
+    state.reverseCt = nextCt;
+  }
+  if (enableUnroll) {
+    RankedTensorType unsqueezedHtType =
+        RankedTensorType::get({1, 1, batchSize, hiddenSize}, elementType);
+    if (isForward)
+      state.forwardAllH.emplace_back(create.onnx.unsqueeze(
+          unsqueezedHtType, nextHt, create.onnx.constantInt64({0, 1})));
+    else
+      state.reverseAllH.insert(state.reverseAllH.begin(),
+          create.onnx.unsqueeze(
+              unsqueezedHtType, nextHt, create.onnx.constantInt64({0, 1})));
+  } else {
+    RankedTensorType unsqueezedHtType =
+        RankedTensorType::get({1, 1, batchSize, hiddenSize}, elementType);
+    RankedTensorType unsqueezedIdxType =
+        RankedTensorType::get({1, 1}, rewriter.getI64Type());
+    Value unsqueezedHt = create.onnx.unsqueeze(
+        unsqueezedHtType, nextHt, create.onnx.constantInt64({0, 1}));
+    Value unsqueezedIdx = create.onnx.unsqueeze(
+        unsqueezedIdxType, sequenceIV, create.onnx.constantInt64({0}));
+    if (isForward)
+      state.allHForward =
+          rewriter.create<ONNXScatterNDOp>(loc, state.allHForward.getType(),
+              state.allHForward, unsqueezedIdx, unsqueezedHt);
+    else
+      state.allHReverse =
+          rewriter.create<ONNXScatterNDOp>(loc, state.allHReverse.getType(),
+              state.allHReverse, unsqueezedIdx, unsqueezedHt);
+  }
+}
+
+template <>
+void stateToOutput<ONNXLSTMOp, LstmState>(ConversionPatternRewriter &rewriter,
+    Location loc, ONNXLSTMOp *op, LstmState state, std::vector<Value> &outputs,
+    bool enableUnroll) {
+  Value noneValue;
+  auto direction = op->getDirection();
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  // First output: all sequences.
+  if (isNoneValue(op->getY()))
+    outputs.emplace_back(noneValue);
+  else {
+    if (enableUnroll) {
+      if (direction == FORWARD) {
+        outputs.emplace_back(create.onnx.concat(
+            op->getY().getType(), ValueRange(state.forwardAllH), 0));
+      } else if (direction == REVERSE) {
+        outputs.emplace_back(create.onnx.concat(
+            op->getY().getType(), ValueRange(state.reverseAllH), 0));
+      } else {
+        auto outputShape = op->getY().getType().cast<ShapedType>().getShape();
+        RankedTensorType singleDirectionType = RankedTensorType::get(
+            {outputShape[0], 1, outputShape[2], outputShape[3]},
+            op->getY().getType().cast<ShapedType>().getElementType());
+        outputs.emplace_back(create.onnx.concat(op->getY().getType(),
+            {create.onnx.concat(
+                 singleDirectionType, ValueRange(state.forwardAllH), 0),
+                create.onnx.concat(
+                    singleDirectionType, ValueRange(state.reverseAllH), 0)},
+            1));
+      }
+    } else {
+      if (direction == FORWARD) {
+        outputs.emplace_back(state.allHForward);
+      } else if (direction == REVERSE) {
+        outputs.emplace_back(state.allHReverse);
+      } else {
+        outputs.emplace_back(create.onnx.concat(
+            op->getY().getType(), {state.allHForward, state.allHReverse}, 1));
+      }
+    }
+  }
+  // Second output: hidden.
+  if (isNoneValue(op->getYH()))
+    outputs.emplace_back(noneValue);
+  else {
+    stateToOutputForHiddenOrCell(
+        rewriter, loc, state.forwardHt, state.reverseHt, direction, state.ht);
+    outputs.emplace_back(state.ht);
+  }
+  // Third output: cell.
+  if (isNoneValue(op->getYC()))
+    outputs.emplace_back(noneValue);
+  else {
+    stateToOutputForHiddenOrCell(
+        rewriter, loc, state.forwardCt, state.reverseCt, direction, state.ct);
+    outputs.emplace_back(state.ct);
+  }
+}
+
+template <>
+void calculateStateWithUnroll<ONNXLSTMOp, LstmState, LstmActivationPack,
+    LstmWeightPack, LstmBiasPack>(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, llvm::StringRef direction, int64_t sequenceDimSize,
+    Value X, LstmState &state, LstmActivationPack activationForward,
+    LstmActivationPack activationReverse, LstmWeightPack weightForward,
+    LstmWeightPack weightReverse, LstmBiasPack biasForward,
+    LstmBiasPack biasReverse, Value sequenceLens, Value initialH) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    for (int64_t i = 0; i < sequenceDimSize; i++) {
+      mlir::Value directionIV = create.onnx.constantInt64({0});
+      mlir::Value sequenceIV = create.onnx.constantInt64({i});
+      // Get a slice of X at the current timestep.
+      mlir::Value Xt = emitXSliceAt(rewriter, loc, X, sequenceIV);
+      // Emit calculation for one RNN step.
+      calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+          LstmBiasPack>(rewriter, loc, Xt, state, activationForward,
+          weightForward, biasForward, sequenceIV, directionIV, sequenceLens,
+          initialH, /*enableUnroll=*/true, /*isForward=*/true);
+    }
+  }
+
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    for (int64_t i = 0; i < sequenceDimSize; i++) {
+      mlir::Value directionIV =
+          create.onnx.constantInt64({(direction == REVERSE) ? 0 : 1});
+      mlir::Value reverseSequenceIV =
+          create.onnx.constantInt64({sequenceDimSize - i - 1});
+      // Get a slice of X at the current timestep.
+      mlir::Value Xt = emitXSliceAt(rewriter, loc, X, reverseSequenceIV);
+      // Emit calculation for one RNN step.
+      calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+          LstmBiasPack>(rewriter, loc, Xt, state, activationReverse,
+          weightReverse, biasReverse, reverseSequenceIV, directionIV,
+          sequenceLens, initialH, /*enableUnroll=*/true, /*isForward=*/false);
+    }
+  }
+}
+
+template <>
+void calculateStateWithLoop<ONNXLSTMOp, LstmState, LstmActivationPack,
+    LstmWeightPack, LstmBiasPack>(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, llvm::StringRef direction, int64_t sequenceDimSize,
+    Value X, LstmState &state, LstmActivationPack activationForward,
+    LstmActivationPack activationReverse, LstmWeightPack weightForward,
+    LstmWeightPack weightReverse, LstmBiasPack biasForward,
+    LstmBiasPack biasReverse, Value sequenceLens, Value initialH) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    mlir::Value directionIV = create.onnx.constantInt64({0});
+    mlir::Value sequenceIV = create.onnx.constantInt64({0});
+    SmallVector<Value> operands = {
+        sequenceIV, state.allHForward, state.forwardHt, state.forwardCt};
+    SmallVector<Type> returnedTypes = {sequenceIV.getType(),
+        state.allHForward.getType(), state.forwardHt.getType(),
+        state.forwardCt.getType()};
+    SmallVector<Location> locations(returnedTypes.size(), loc);
+    ::stablehlo::WhileOp whileLoopOp =
+        rewriter.create<::stablehlo::WhileOp>(loc, returnedTypes, operands);
+    Region &condRegion = whileLoopOp.getCond();
+    Region &bodyRegion = whileLoopOp.getBody();
+    Block &condBlock = condRegion.emplaceBlock();
+    Block &bodyBlock = bodyRegion.emplaceBlock();
+    condBlock.addArguments(returnedTypes, locations);
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&condBlock);
+      BlockArgument lhs = condBlock.getArgument(0);
+      mlir::Value rhs = create.onnx.constantInt64({sequenceDimSize});
+      Value compareResult = rewriter.create<::stablehlo::CompareOp>(
+          loc, lhs, rhs, ::stablehlo::ComparisonDirection::LT);
+      compareResult = rewriter.create<::stablehlo::ReshapeOp>(
+          loc, RankedTensorType::get({}, rewriter.getI1Type()), compareResult);
+      rewriter.create<::stablehlo::ReturnOp>(loc, compareResult);
+    }
+    bodyBlock.addArguments(returnedTypes, locations);
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&bodyBlock);
+      BlockArgument seqIV = bodyBlock.getArgument(0);
+      BlockArgument allH = bodyBlock.getArgument(1);
+      BlockArgument ht = bodyBlock.getArgument(2);
+      BlockArgument ct = bodyBlock.getArgument(3);
+      state.allHForward = allH;
+      state.forwardHt = ht;
+      state.forwardCt = ct;
+      mlir::Value Xt = emitXSliceAt(rewriter, loc, X, seqIV);
+      calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+          LstmBiasPack>(rewriter, loc, Xt, state, activationForward,
+          weightForward, biasForward, seqIV, directionIV, sequenceLens,
+          initialH, /*enableUnroll=*/false, /*isForward=*/true);
+      mlir::Value one = create.onnx.constantInt64({1});
+      Value newSeqIV = create.onnx.add(seqIV, one);
+      rewriter.create<::stablehlo::ReturnOp>(loc,
+          ValueRange(
+              {newSeqIV, state.allHForward, state.forwardHt, state.forwardCt}));
+    }
+    state.allHForward = whileLoopOp.getResults()[1];
+    state.forwardHt = whileLoopOp.getResults()[2];
+    state.forwardCt = whileLoopOp.getResults()[3];
+  }
+
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    mlir::Value directionIV =
+        create.onnx.constantInt64({(direction == REVERSE) ? 0 : 1});
+    mlir::Value reverseSequenceIV =
+        create.onnx.constantInt64({sequenceDimSize - 1});
+
+    SmallVector<Value> operands = {
+        reverseSequenceIV, state.allHReverse, state.reverseHt, state.reverseCt};
+    SmallVector<Type> returnedTypes = {reverseSequenceIV.getType(),
+        state.allHReverse.getType(), state.reverseHt.getType(),
+        state.reverseCt.getType()};
+    SmallVector<Location> locations(returnedTypes.size(), loc);
+    ::stablehlo::WhileOp whileLoopOp =
+        rewriter.create<::stablehlo::WhileOp>(loc, returnedTypes, operands);
+    Region &condRegion = whileLoopOp.getCond();
+    Region &bodyRegion = whileLoopOp.getBody();
+    Block &condBlock = condRegion.emplaceBlock();
+    Block &bodyBlock = bodyRegion.emplaceBlock();
+    condBlock.addArguments(returnedTypes, locations);
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&condBlock);
+      BlockArgument lhs = condBlock.getArgument(0);
+      mlir::Value rhs = create.onnx.constantInt64({0});
+      Value compareResult = rewriter.create<::stablehlo::CompareOp>(
+          loc, lhs, rhs, ::stablehlo::ComparisonDirection::GE);
+      compareResult = rewriter.create<::stablehlo::ReshapeOp>(
+          loc, RankedTensorType::get({}, rewriter.getI1Type()), compareResult);
+      rewriter.create<::stablehlo::ReturnOp>(loc, compareResult);
+    }
+    bodyBlock.addArguments(returnedTypes, locations);
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&bodyBlock);
+      BlockArgument revseqIV = bodyBlock.getArgument(0);
+      BlockArgument allH = bodyBlock.getArgument(1);
+      BlockArgument ht = bodyBlock.getArgument(2);
+      BlockArgument ct = bodyBlock.getArgument(3);
+      state.allHReverse = allH;
+      state.reverseHt = ht;
+      state.reverseCt = ct;
+      mlir::Value Xt = emitXSliceAt(rewriter, loc, X, revseqIV);
+      calculateState<LstmState, LstmActivationPack, LstmWeightPack,
+          LstmBiasPack>(rewriter, loc, Xt, state, activationReverse,
+          weightReverse, biasReverse, revseqIV, directionIV, sequenceLens,
+          initialH, /*enableUnroll=*/false, /*isForward=*/false);
+      mlir::Value one = create.onnx.constantInt64({1});
+      Value newrevseqIV = create.onnx.sub(revseqIV, one);
+      rewriter.create<::stablehlo::ReturnOp>(
+          loc, ValueRange({newrevseqIV, state.allHReverse, state.reverseHt,
+                   state.reverseCt}));
+    }
+    state.allHReverse = whileLoopOp.getResults()[1];
+    state.reverseHt = whileLoopOp.getResults()[2];
+    state.reverseCt = whileLoopOp.getResults()[3];
+  }
+}
+
+} // namespace stablehlo
+
+void populateLoweringONNXLSTMOpToStableHloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx, bool enableUnroll) {
+  patterns.insert<onnx_mlir::stablehlo::ONNXRNNOpLowering<ONNXLSTMOp,
+      onnx_mlir::stablehlo::LstmState, onnx_mlir::stablehlo::LstmActivationPack,
+      onnx_mlir::stablehlo::LstmWeightPack,
+      onnx_mlir::stablehlo::LstmBiasPack>>(ctx, enableUnroll);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp b/src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp
new file mode 100644
index 0000000000..6c771cfa0b
--- /dev/null
+++ b/src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp
@@ -0,0 +1,234 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- RNNBase.cpp - Lowering RNN Ops -----------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file defines base functions for lowering the ONNX RNN Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp"
+#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "lstm"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace stablehlo {
+
+// Get a dimension of the tensor's shape.
+int64_t dimAt(Value val, int index) {
+  return val.getType().cast<ShapedType>().getShape()[index];
+}
+
+/// Allocate the all hidden output.
+/// Shape :: [seq_length, num_directions, batch_size, hidden_size]
+Value allocAllHidden(
+    ConversionPatternRewriter &rewriter, Location loc, Value X, Value R) {
+  LLVM_DEBUG(llvm::dbgs() << "allocAllHidden\n");
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  RankedTensorType zeroType =
+      RankedTensorType::get({dimAt(X, 0), 1, dimAt(X, 1), dimAt(R, 2)},
+          X.getType().cast<ShapedType>().getElementType());
+  DenseElementsAttr zeroAttr = DenseElementsAttr::get(zeroType, 0.0f);
+  return create.onnx.constant(zeroAttr);
+}
+
+/// Allocate the hidden or cell output.
+mlir::Value allocHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value W, mlir::Value R) {
+  LLVM_DEBUG(llvm::dbgs() << "allocHiddenOrCell\n");
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  RankedTensorType zeroType = RankedTensorType::get(
+      {/*num_directions=*/dimAt(W, 0), /*batch_size=*/dimAt(X, 1),
+          /*hidden_size=*/dimAt(R, 2)},
+      X.getType().cast<ShapedType>().getElementType());
+  DenseElementsAttr zeroAttr = DenseElementsAttr::get(zeroType, 0.0f);
+  return create.onnx.constant(zeroAttr);
+}
+
+/// Allocate the intermediate hidden or cell states.
+/// Shape :: [batch_size, hidden_size]
+Value allocIntermediateState(
+    ConversionPatternRewriter &rewriter, Location loc, Value X, Value R) {
+  LLVM_DEBUG(llvm::dbgs() << "allocIntermediateState\n");
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  RankedTensorType zeroType =
+      RankedTensorType::get({/*batch_size=*/dimAt(X, 1),
+                                /*hidden_size=*/dimAt(R, 2)},
+          X.getType().cast<ShapedType>().getElementType());
+  DenseElementsAttr zeroAttr = DenseElementsAttr::get(zeroType, 0.0f);
+  return create.onnx.constant(zeroAttr);
+}
+
+/// Initialize the intermediate hidden and cell states.
+/// forward(reverse)Ht, forward(reverse)Ct
+void initializeIntermediateStates(ConversionPatternRewriter &rewriter,
+    Location loc, Value &forwardHt, Value &reverseHt, Value &forwardCt,
+    Value &reverseCt, Value initialH, Value initialC, Type elementType,
+    StringRef direction, bool onlyHidden) {
+  LLVM_DEBUG(llvm::dbgs() << "initializeIntermediateStates\n");
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+
+  Value zeroIndex = create.onnx.constantInt64({0});
+  Value oneIndex = create.onnx.constantInt64({1});
+  Value twoIndex = create.onnx.constantInt64({2});
+
+  Value boundVal = (direction == FORWARD || direction == BIDIRECTIONAL)
+                       ? forwardHt
+                       : reverseHt;
+  auto valShape = boundVal.getType().cast<ShapedType>().getShape();
+  RankedTensorType sliceType =
+      RankedTensorType::get({1, valShape[0], valShape[1]},
+          boundVal.getType().cast<RankedTensorType>().getElementType());
+  RankedTensorType valType = boundVal.getType().cast<RankedTensorType>();
+  if (direction == FORWARD || direction == BIDIRECTIONAL) {
+    if (!isNoneValue(initialH)) {
+      forwardHt = create.onnx.slice(
+          sliceType, initialH, zeroIndex, oneIndex, zeroIndex, oneIndex);
+      forwardHt = create.onnx.squeeze(valType, forwardHt, zeroIndex);
+    }
+    if (!onlyHidden && !isNoneValue(initialC)) {
+      forwardCt = create.onnx.slice(
+          sliceType, initialC, zeroIndex, oneIndex, zeroIndex, oneIndex);
+      forwardCt = create.onnx.squeeze(valType, forwardCt, zeroIndex);
+    }
+  }
+  if (direction == REVERSE || direction == BIDIRECTIONAL) {
+    if (!isNoneValue(initialH)) {
+      if (direction == REVERSE) {
+        reverseHt = create.onnx.slice(
+            sliceType, initialH, zeroIndex, oneIndex, zeroIndex, oneIndex);
+        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex);
+      } else {
+        reverseHt = create.onnx.slice(
+            sliceType, initialH, oneIndex, twoIndex, zeroIndex, oneIndex);
+        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex);
+      }
+    }
+    if (!onlyHidden and !isNoneValue(initialC)) {
+      if (direction == REVERSE) {
+        reverseCt = create.onnx.slice(
+            sliceType, initialC, zeroIndex, oneIndex, zeroIndex, oneIndex);
+        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex);
+      } else {
+        reverseCt = create.onnx.slice(
+            sliceType, initialC, oneIndex, twoIndex, zeroIndex, oneIndex);
+        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex);
+      }
+    }
+  }
+}
+
+/// Store a state into the output of the RNN op.
+/// The input state is 2D and the output state is 3D with '1' or '2' is
+/// pretended, depending on 'direction'.
+void stateToOutputForHiddenOrCell(ConversionPatternRewriter &rewriter,
+    Location loc, Value forwardVal, Value reverseVal, StringRef direction,
+    Value &output) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  if (direction == FORWARD || direction == REVERSE) {
+    Value val = (direction == FORWARD) ? forwardVal : reverseVal;
+    output = val;
+  } else { // BIDIRECTIONAL
+    SmallVector<int64_t, 4> bForwardValShape(
+        forwardVal.getType().cast<ShapedType>().getShape());
+    SmallVector<int64_t, 4> bValShape(
+        forwardVal.getType().cast<ShapedType>().getShape());
+    SmallVector<int64_t, 4> bReverseValShape(
+        reverseVal.getType().cast<ShapedType>().getShape());
+    bForwardValShape.insert(bForwardValShape.begin(), 1);
+    bReverseValShape.insert(bReverseValShape.begin(), 1);
+    bValShape.insert(bValShape.begin(), 2);
+    Type valElementType =
+        forwardVal.getType().cast<ShapedType>().getElementType();
+    Value zero = create.onnx.constantInt64({0});
+    Value bForwardVal = create.onnx.unsqueeze(
+        RankedTensorType::get(bForwardValShape, valElementType), forwardVal,
+        zero);
+    Value bReverseVal = create.onnx.unsqueeze(
+        RankedTensorType::get(bReverseValShape, valElementType), reverseVal,
+        zero);
+    output =
+        create.onnx.concat(RankedTensorType::get(bValShape, valElementType),
+            {bForwardVal, bReverseVal}, 0);
+  }
+}
+
+// Apply an activation function on a given scalar operand.
+Value applyActivation(OpBuilder &rewriter, Location loc,
+    RNNActivation activation, Value operand) {
+  Value res;
+
+  std::vector<mlir::NamedAttribute> attributes;
+  if (activation.alpha) {
+    attributes.emplace_back(
+        rewriter.getNamedAttr("alpha", activation.alpha.value()));
+  }
+  if (activation.beta) {
+    attributes.emplace_back(
+        rewriter.getNamedAttr("beta", activation.beta.value()));
+  }
+  Type resType = operand.getType();
+
+  // Change equality to be case insensitive.
+  if (activation.name.equals_insensitive("relu"))
+    res = rewriter.create<ONNXReluOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("tanh"))
+    res = rewriter.create<ONNXTanhOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("sigmoid"))
+    res = rewriter.create<ONNXSigmoidOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("affine"))
+    llvm_unreachable("Unsupported activation");
+  else if (activation.name.equals_insensitive("leakyrelu"))
+    res = rewriter.create<ONNXLeakyReluOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("thresholdedrelu"))
+    res = rewriter.create<ONNXThresholdedReluOp>(
+        loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("scaledtanh"))
+    llvm_unreachable("Unsupported activation");
+  else if (activation.name.equals_insensitive("hardsigmoid"))
+    res = rewriter.create<ONNXHardSigmoidOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("elu"))
+    res = rewriter.create<ONNXEluOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("softsign"))
+    res = rewriter.create<ONNXSoftsignOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("softplus"))
+    res = rewriter.create<ONNXSoftplusOp>(loc, resType, operand);
+  else
+    llvm_unreachable("Unsupported activation");
+
+  return res;
+}
+
+/// Create a copy of a slice of X at a specific timestep.
+Value emitXSliceAt(ConversionPatternRewriter &rewriter, Location loc, Value X,
+    Value timestepIV) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  int64_t batchSize = dimAt(X, 1);
+  int64_t inputSize = dimAt(X, 2);
+  Type elementType = X.getType().cast<ShapedType>().getElementType();
+  RankedTensorType sliceXType =
+      RankedTensorType::get({1, batchSize, inputSize}, elementType);
+  RankedTensorType squeezedXType =
+      RankedTensorType::get({batchSize, inputSize}, elementType);
+  Value sliceX = create.onnx.slice(sliceXType, X, timestepIV,
+      create.onnx.add(timestepIV, create.onnx.constantInt64({1})),
+      create.onnx.constantInt64({0}), create.onnx.constantInt64({1}));
+  sliceX = create.onnx.squeeze(
+      squeezedXType, sliceX, create.onnx.constantInt64({0}));
+  return sliceX;
+}
+
+} // namespace stablehlo
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp b/src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp
new file mode 100644
index 0000000000..a0dbde620a
--- /dev/null
+++ b/src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp
@@ -0,0 +1,203 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- RNNBase.hpp - Lowering RNN Ops -----------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file defines base functions for lowering the ONNX RNN Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+
+static constexpr llvm::StringRef FORWARD = "forward";
+static constexpr llvm::StringRef REVERSE = "reverse";
+static constexpr llvm::StringRef BIDIRECTIONAL = "bidirectional";
+
+namespace onnx_mlir {
+
+namespace stablehlo {
+
+struct RNNActivation {
+  llvm::StringRef name;
+  std::optional<mlir::FloatAttr> alpha;
+  std::optional<mlir::FloatAttr> beta;
+};
+
+/// Get a dimension of the tensor's shape.
+int64_t dimAt(mlir::Value val, int index);
+
+/// Allocate the all hidden output.
+mlir::Value allocAllHidden(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value R);
+
+/// Allocate the hidden or cell output.
+mlir::Value allocHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value W, mlir::Value R);
+
+/// Allocate the intermediate hidden or cell state.
+mlir::Value allocIntermediateState(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value R);
+
+/// Initialize the intermediate hidden and cell states.
+void initializeIntermediateStates(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value &forwardHt, mlir::Value &reverseHt,
+    mlir::Value &forwardCt, mlir::Value &reverseCt, mlir::Value initialH,
+    mlir::Value initialC, mlir::Type elementType, llvm::StringRef direction,
+    bool onlyHidden);
+
+/// Store a state into the output of the RNN op.
+/// The input state is 2D and the output state is 3D with '1' or '2' is
+/// pretended, depending on 'direction'.
+void stateToOutputForHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value forwardVal, mlir::Value reverseVal,
+    llvm::StringRef direction, mlir::Value &output);
+
+/// Apply an activation function on a given operand.
+mlir::Value applyActivation(mlir::OpBuilder &rewriter, mlir::Location loc,
+    RNNActivation activation, mlir::Value operand);
+
+/// Get a slice of X at a specific timestep.
+mlir::Value emitXSliceAt(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value X, mlir::Value timestep);
+
+// Override the following methods when lowering an RNN operation:
+// - hasAllNoneOutput
+// - getActivationPack
+// - getWeightPack
+// - getBiasPack
+// - allocAndInitializeStates
+// - calculateState
+// - stateToOutput
+
+// Check whether all outputs have NoneType or not.
+template <typename RNNOp>
+bool hasAllNoneOutput(RNNOp *op);
+
+// Obtain activations functions for a specific operation.
+template <typename RNNOp, typename A>
+std::tuple<A, A> getActivationPack(RNNOp *op);
+
+/// Obtain weight tensors in 2D for each gate.
+/// In ONNX, weights for gates and directions are combined in a single tensor.
+/// This function splits them into 2D tensors.
+template <typename RNNOp, typename W>
+std::tuple<W, W> getWeightPack(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc, RNNOp *op);
+
+/// Obtain biases in 1D for each gate.
+/// In ONNX, biases for gates and directions are combined in a single tensor.
+/// This function splits them into 1D tensors.
+template <typename RNNOp, typename B>
+std::tuple<B, B> getBiasPack(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc, RNNOp *op);
+
+// Allocate memory for RNN states and initialize them.
+template <typename RNNOp, typename S>
+S allocAndInitializeStates(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, RNNOp *op, typename RNNOp::Adaptor operandAdaptor,
+    bool enableUnroll);
+
+// Calculate new states from the current input and states.
+template <typename S, typename A, typename W, typename B>
+void calculateState(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, mlir::Value Xt, S &state, A activationSet, W weight,
+    B bias, mlir::Value sequenceIV, mlir::Value directionIV,
+    mlir::Value sequenceLens, mlir::Value initialH, bool enableUnroll,
+    bool isForward);
+
+// Write states to the RNN's outputs.
+template <typename RNNOp, typename S>
+void stateToOutput(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, RNNOp *op, S state, std::vector<mlir::Value> &outputs,
+    bool enableUnroll);
+
+// Calculate all states using unroll
+template <typename RNNOp, typename S, typename A, typename W, typename B>
+void calculateStateWithUnroll(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, llvm::StringRef direction, int64_t sequenceDimSize,
+    Value X, S &state, A activationForward, A activationReverse,
+    W weightForward, W weightReverse, B biasForward, B biasReverse,
+    Value sequenceLens, Value initialH);
+
+// Calculate all states using loop
+template <typename RNNOp, typename S, typename A, typename W, typename B>
+void calculateStateWithLoop(mlir::ConversionPatternRewriter &rewriter,
+    mlir::Location loc, llvm::StringRef direction, int64_t sequenceDimSize,
+    Value X, S &state, A activationForward, A activationReverse,
+    W weightForward, W weightReverse, B biasForward, B biasReverse,
+    Value sequenceLens, Value initialH);
+
+// A common template for lowering an RNN operation.
+template <typename RNNOp, typename S, typename A, typename W, typename B>
+struct ONNXRNNOpLowering : public mlir::OpConversionPattern<RNNOp> {
+  using OpAdaptor = typename RNNOp::Adaptor;
+  bool enableUnroll;
+
+  ONNXRNNOpLowering(mlir::MLIRContext *ctx, bool enableUnroll)
+      : mlir::OpConversionPattern<RNNOp>(ctx) {
+    this->enableUnroll = enableUnroll;
+  }
+
+  mlir::LogicalResult matchAndRewrite(RNNOp rnnOp, OpAdaptor adaptor,
+      mlir::ConversionPatternRewriter &rewriter) const final {
+    mlir::Operation *op = rnnOp.getOperation();
+    mlir::Location loc = ONNXLoc<RNNOp>(op);
+    mlir::Value X = adaptor.getX();
+    mlir::Value sequenceLens = adaptor.getSequenceLens();
+    mlir::Value initialH = adaptor.getInitialH();
+
+    if (hasAllNoneOutput<RNNOp>(&rnnOp)) {
+      rewriter.eraseOp(op);
+      return mlir::success();
+    }
+
+    // Initialize output states.
+    S state = allocAndInitializeStates<RNNOp, S>(
+        rewriter, loc, &rnnOp, adaptor, this->enableUnroll);
+
+    // Activation functions.
+    A activationForward, activationReverse;
+    std::tie(activationForward, activationReverse) =
+        getActivationPack<RNNOp, A>(&rnnOp);
+
+    // Prepare weights.
+    W weightForward, weightReverse;
+    std::tie(weightForward, weightReverse) =
+        getWeightPack<RNNOp, W>(rewriter, loc, &rnnOp);
+
+    // Prepare biases.
+    B biasForward, biasReverse;
+    std::tie(biasForward, biasReverse) =
+        getBiasPack<RNNOp, B>(rewriter, loc, &rnnOp);
+
+    int64_t sequenceDimSize = dimAt(rnnOp.getX(), 0);
+    auto direction = rnnOp.getDirection();
+
+    if (this->enableUnroll)
+      calculateStateWithUnroll<RNNOp, S, A, W, B>(rewriter, loc, direction,
+          sequenceDimSize, X, state, activationForward, activationReverse,
+          weightForward, weightReverse, biasForward, biasReverse, sequenceLens,
+          initialH);
+    else
+      calculateStateWithLoop<RNNOp, S, A, W, B>(rewriter, loc, direction,
+          sequenceDimSize, X, state, activationForward, activationReverse,
+          weightForward, weightReverse, biasForward, biasReverse, sequenceLens,
+          initialH);
+    std::vector<mlir::Value> outputs;
+    stateToOutput<RNNOp, S>(
+        rewriter, loc, &rnnOp, state, outputs, this->enableUnroll);
+    rewriter.replaceOp(op, outputs);
+    return mlir::success();
+  }
+};
+
+} // namespace stablehlo
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp b/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
new file mode 100644
index 0000000000..f69e0ac48f
--- /dev/null
+++ b/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
@@ -0,0 +1,129 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===---------------- OneHot.cpp - Lowering OneHot Op -------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX OneHot Operator to StableHlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+#include <numeric>
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+struct ONNXOneHotOpLoweringToStableHlo
+    : public OpConversionPattern<ONNXOneHotOp> {
+  ONNXOneHotOpLoweringToStableHlo(MLIRContext *ctx)
+      : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXOneHotOp onehotOp,
+      ONNXOneHotOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = onehotOp.getOperation();
+    Location loc = ONNXLoc<ONNXOneHotOp>(op);
+    ValueRange operands = adaptor.getOperands();
+    Value indices = adaptor.getIndices();
+    Value depthValue = adaptor.getDepth();
+    Value values = adaptor.getValues();
+    Type outputType = *op->result_type_begin();
+
+    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    ONNXOneHotOpShapeHelper shapeHelper(op, operands, &createIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+    int64_t axis = shapeHelper.axis;
+
+    RankedTensorType indicesType =
+        indices.getType().dyn_cast<RankedTensorType>();
+    if (!indicesType || !indicesType.hasStaticShape())
+      return failure();
+    ArrayRef<int64_t> indicesShape = indicesType.getShape();
+    Type indicesElementType = indicesType.getElementType();
+
+    DenseIntElementsAttr depthAttr;
+    if (!matchPattern(depthValue, m_Constant(&depthAttr))) {
+      return failure();
+    }
+
+    int64_t depth = depthAttr.getValues<APInt>()[0].getSExtValue();
+
+    llvm::SmallVector<int64_t, 4> broadcastDims(indicesShape.size());
+    std::iota(broadcastDims.begin(), broadcastDims.begin() + axis, 0);
+    std::iota(broadcastDims.begin() + axis, broadcastDims.end(), axis + 1);
+
+    llvm::SmallVector<int64_t, 4> outputDims = llvm::to_vector<4>(indicesShape);
+    outputDims.insert(outputDims.begin() + axis, depth);
+
+    RankedTensorType indexType =
+        RankedTensorType::get(llvm::ArrayRef(outputDims), indicesElementType);
+
+    Value iota = rewriter.create<stablehlo::IotaOp>(
+        loc, indexType, IntegerAttr::get(rewriter.getIntegerType(64), axis));
+    Value broadcastIndices = rewriter.create<stablehlo::BroadcastInDimOp>(
+        loc, indexType, indices, GetI64ElementsAttr(broadcastDims, &rewriter));
+    Value zero = rewriter.create<stablehlo::ConstantOp>(loc,
+        DenseIntElementsAttr::get(RankedTensorType::get({}, indicesElementType),
+            ArrayRef<int64_t>{0}));
+    Value broadcastZero = rewriter.create<stablehlo::BroadcastInDimOp>(
+        loc, indexType, zero, rewriter.getI64TensorAttr({}));
+    Value broadcastDepth;
+    int64_t depthRank = depthValue.getType().cast<RankedTensorType>().getRank();
+    if (depthRank == 1)
+      broadcastDepth = rewriter.create<stablehlo::BroadcastInDimOp>(
+          loc, indexType, depthValue, rewriter.getI64TensorAttr({0}));
+    else
+      broadcastDepth = rewriter.create<stablehlo::BroadcastInDimOp>(
+          loc, indexType, depthValue, rewriter.getI64TensorAttr({}));
+    Value compareGeZero = rewriter.create<stablehlo::CompareOp>(loc,
+        broadcastIndices, broadcastZero, stablehlo::ComparisonDirection::GE);
+    Value positiveIndices = rewriter.create<stablehlo::AddOp>(
+        loc, broadcastIndices, broadcastDepth);
+    Value normalizedIndices = rewriter.create<stablehlo::SelectOp>(
+        loc, indexType, compareGeZero, broadcastIndices, positiveIndices);
+    Value compare = rewriter.create<stablehlo::CompareOp>(
+        loc, normalizedIndices, iota, stablehlo::ComparisonDirection::EQ);
+    Type indexElementType = rewriter.getI64Type();
+    Type valueType = values.getType().cast<ShapedType>().getElementType();
+    Value offValue = rewriter.create<stablehlo::SliceOp>(loc,
+        RankedTensorType::get({1}, valueType), values,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{0}),
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
+        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
+            ArrayRef<int64_t>{1}));
+    Value onValue = rewriter.create<stablehlo::SliceOp>(loc,
+        RankedTensorType::get({1}, valueType), values,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{2}),
+        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
+            ArrayRef<int64_t>{1}));
+    Value offValueBroadcast = rewriter.create<stablehlo::BroadcastInDimOp>(
+        loc, outputType, offValue, rewriter.getI64TensorAttr({0}));
+    Value onValueBroadcast = rewriter.create<stablehlo::BroadcastInDimOp>(
+        loc, outputType, onValue, rewriter.getI64TensorAttr({0}));
+    Value result = rewriter.create<stablehlo::SelectOp>(
+        loc, outputType, compare, onValueBroadcast, offValueBroadcast);
+    rewriter.replaceOp(op, {result});
+    return success();
+  }
+};
+
+void populateLoweringONNXOneHotOpToStableHloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXOneHotOpLoweringToStableHlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Pad.cpp b/src/Conversion/ONNXToStableHlo/Tensor/Pad.cpp
index 7ccb44ceb0..55f279719a 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Pad.cpp
+++ b/src/Conversion/ONNXToStableHlo/Tensor/Pad.cpp
@@ -60,7 +60,7 @@ struct ONNXPadOpLoweringToStablehlo : public ConversionPattern {
     SmallVector<int64_t> edgePaddingLowVec(rank, 0);
     SmallVector<int64_t> edgePaddingHighVec(rank, 0);
     SmallVector<int64_t> interiorPaddingVec(rank, 0);
-    if (auto valueAttribute = getElementAttributeFromStablehloValue(pads)) {
+    if (auto valueAttribute = getElementAttributeFromConstValue(pads)) {
       // If `pads` are constants, read them."
       int64_t idx = 0;
       for (IntegerAttr value : valueAttribute.getValues<IntegerAttr>()) {
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp b/src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp
new file mode 100644
index 0000000000..aaa1d6bd3d
--- /dev/null
+++ b/src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp
@@ -0,0 +1,92 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- ScatterND.cpp - Lowering ScatterND Op ----------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX ScatterND Operator to StableHlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+struct ONNXScatterNDOpLoweringToStableHlo
+    : public OpConversionPattern<ONNXScatterNDOp> {
+  ONNXScatterNDOpLoweringToStableHlo(MLIRContext *ctx)
+      : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXScatterNDOp scatterNDOp,
+      ONNXScatterNDOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = scatterNDOp.getOperation();
+    Location loc = ONNXLoc<ONNXScatterNDOp>(op);
+
+    // Operands and attributes.
+    Value data = adaptor.getData();
+    Value updates = adaptor.getUpdates();
+    Value indices = adaptor.getIndices();
+    auto dataType = data.getType().cast<ShapedType>();
+    auto indicesType = indices.getType().cast<ShapedType>();
+    int64_t dataRank = dataType.getRank();
+    int64_t indicesRank = indicesType.getRank();
+    assert(indicesType.hasStaticShape() &&
+           "only support indices with static shape");
+    int64_t partialIdxDim = indicesType.getDimSize(indicesRank - 1);
+
+    assert(dataRank >= 1 && "The rank of 'data' must be >= 1");
+    assert(indicesRank >= 1 && "The rank of 'indices' must be >= 1");
+
+    Type outputType = *op->result_type_begin();
+    assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+    ShapedType outputShapedType = outputType.cast<ShapedType>();
+    int64_t outputRank = outputShapedType.getRank();
+    assert(outputRank == dataRank && "Output rank not equal to data rank");
+    auto scatter_dimension_numbers =
+        mlir::stablehlo::ScatterDimensionNumbersAttr::get(
+            /*context=*/rewriter.getContext(),
+            /*updateWindowDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(partialIdxDim, dataRank)),
+            /*insertedWindowDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(0, partialIdxDim)),
+            /*scatterDimsToOperandDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(0, partialIdxDim)),
+            /*indexVectorDim=*/indicesRank - 1);
+    auto scatterOp = rewriter.create<stablehlo::ScatterOp>(
+        loc, outputType, data, indices, updates, scatter_dimension_numbers);
+    // config update computation function: just return the element from src.
+    Block &block = scatterOp.getUpdateComputation().emplaceBlock();
+    // add block arguments
+    auto blockArgumentType =
+        RankedTensorType::get({}, dataType.getElementType());
+    block.addArgument(blockArgumentType, loc);
+    block.addArgument(blockArgumentType, loc);
+
+    auto *lhsArg = block.args_begin();
+    auto *rhsArg = std::next(lhsArg);
+
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&block);
+      rewriter.create<stablehlo::ReturnOp>(loc, *rhsArg);
+    }
+
+    rewriter.replaceOp(op, scatterOp.getResults());
+    return success();
+  }
+};
+
+void populateLoweringONNXScatterNDOpToStableHloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXScatterNDOpLoweringToStableHlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Tools/onnx-mlir-opt/RegisterPasses.cpp b/src/Tools/onnx-mlir-opt/RegisterPasses.cpp
index c45aa32922..a38469c649 100644
--- a/src/Tools/onnx-mlir-opt/RegisterPasses.cpp
+++ b/src/Tools/onnx-mlir-opt/RegisterPasses.cpp
@@ -124,6 +124,10 @@ void registerOMPasses(int optLevel) {
     return createSimplifyShapeRelatedOpsPass();
   });

+  mlir::registerPass([]() -> std::unique_ptr<mlir::Pass> {
+    return createStandardFuncReturnPass();
+  });
+
   mlir::registerPass([]() -> std::unique_ptr<mlir::Pass> {
     return createONNXDimAnalysisPass();
   });
diff --git a/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir b/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir
index 5bf3bf10d1..73ec128390 100644
--- a/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir
+++ b/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir
@@ -382,6 +382,21 @@ func.func @test_max(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x10xf32>) -> ten

 // -----

+func.func @test_min(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x10xf32>) -> tensor<10x10xf32> {
+  %0 = "onnx.Min"(%arg0, %arg1) : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<10x10xf32>
+  "func.return"(%0) : (tensor<10x10xf32>) -> ()
+// CHECK-LABEL:  func.func @test_min
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<10x10xf32>, [[PARAM_1_:%.+]]: tensor<10x10xf32>) -> tensor<10x10xf32> {
+// CHECK:           [[VAR_0_:%.+]] = shape.const_shape [10, 10] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[PARAM_0_]], [[VAR_0_]], dims = [0, 1] : (tensor<10x10xf32>, tensor<2xindex>) -> tensor<10x10xf32>
+// CHECK-DAG:       [[VAR_2_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[PARAM_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<10x10xf32>, tensor<2xindex>) -> tensor<10x10xf32>
+// CHECK:           [[VAR_3_:%.+]] = stablehlo.minimum [[VAR_1_]], [[VAR_2_]] : tensor<10x10xf32>
+// CHECK:           return [[VAR_3_]] : tensor<10x10xf32>
+// CHECK:         }
+}
+
+// -----
+
 func.func @test_leakyrelu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
   %0 = "onnx.LeakyRelu"(%arg0) {alpha=0.5:f32} : (tensor<?x10xf32>) -> tensor<?x10xf32>
   "func.return"(%0) : (tensor<?x10xf32>) -> ()
@@ -403,6 +418,29 @@ func.func @test_leakyrelu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32>

 // -----

+func.func @test_prelu_dynamic(%arg0 : tensor<?x10x12x12xf32>, %arg1: tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32> {
+  %0 = "onnx.PRelu"(%arg0, %arg1) : (tensor<?x10x12x12xf32>, tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32>
+  "func.return"(%0) : (tensor<?x10x12x12xf32>) -> ()
+// CHECK-LABEL:  func.func @test_prelu_dynamic
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<?x10x12x12xf32>, [[PARAM_1_:%.+]]: tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
+// CHECK-DAG:       [[VAR_1_:%.+]] = shape.const_shape [10, 1, 1] : tensor<3xindex>
+// CHECK-DAG:       [[VAR_2_:%.+]] = shape.shape_of [[PARAM_0_]] : tensor<?x10x12x12xf32> -> tensor<4xindex>
+// CHECK:           [[VAR_3_:%.+]] = shape.broadcast [[VAR_2_]], [[VAR_1_]] : tensor<4xindex>, tensor<3xindex> -> tensor<4xindex>
+// CHECK-DAG:       [[VAR_4_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[PARAM_0_]], [[VAR_3_]], dims = [0, 1, 2, 3] : (tensor<?x10x12x12xf32>, tensor<4xindex>) -> tensor<?x10x12x12xf32>
+// CHECK-DAG:       [[VAR_5_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[PARAM_1_]], [[VAR_3_]], dims = [1, 2, 3] : (tensor<10x1x1xf32>, tensor<4xindex>) -> tensor<?x10x12x12xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_6_:%.+]] = stablehlo.multiply [[VAR_4_]], [[VAR_5_]] : tensor<?x10x12x12xf32>
+// CHECK-DAG:       [[VAR_7_:%.+]] = shape.shape_of [[VAR_4_]] : tensor<?x10x12x12xf32> -> tensor<4xindex>
+// CHECK:           [[VAR_8_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_0_]], [[VAR_7_]], dims = [] : (tensor<f32>, tensor<4xindex>) -> tensor<?x10x12x12xf32>
+// CHECK:           [[VAR_9_:%.+]] = stablehlo.compare  GT, [[VAR_4_]], [[VAR_8_]],  NOTYPE : (tensor<?x10x12x12xf32>, tensor<?x10x12x12xf32>) -> tensor<?x10x12x12xi1>
+// CHECK:           [[VAR_10_:%.+]] = stablehlo.select [[VAR_9_]], [[VAR_4_]], [[VAR_6_]] : tensor<?x10x12x12xi1>, tensor<?x10x12x12xf32>
+// CHECK:           return [[VAR_10_]] : tensor<?x10x12x12xf32>
+// CHECK:         }
+}
+
+// -----
+
 func.func @test_neg(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
   %0 = "onnx.Neg"(%arg0) : (tensor<10x10xf32>) -> tensor<10x10xf32>
   "func.return"(%0) : (tensor<10x10xf32>) -> ()
@@ -426,3 +464,17 @@ func.func @test_sin(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
 // CHECK:           [[VAR_0_:%.+]] = stablehlo.sine [[PARAM_0_]] : tensor<10x10xf32>
 // CHECK:           return [[VAR_0_]] : tensor<10x10xf32>
 // CHECK:         }
+
+func.func @test_where(%arg0 : tensor<16x24x36xi1>, %arg1 : tensor<16x24x36xi64>, %arg2 : tensor<16x24x36xi64>) -> tensor<16x24x36xi64> {
+  %0 = "onnx.Where"(%arg0, %arg1, %arg2) : (tensor<16x24x36xi1>, tensor<16x24x36xi64>, tensor<16x24x36xi64>) -> tensor<16x24x36xi64>
+  "func.return"(%0) : (tensor<16x24x36xi64>) -> ()
+// CHECK-LABEL:  func.func @test_where
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<16x24x36xi1>, [[PARAM_1_:%.+]]: tensor<16x24x36xi64>, [[PARAM_2_:%.+]]: tensor<16x24x36xi64>) -> tensor<16x24x36xi64> {
+// CHECK:           [[VAR_0_:%.+]] = shape.const_shape [16, 24, 36] : tensor<3xindex>
+// CHECK-DAG:       [[VAR_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[PARAM_0_]], [[VAR_0_]], dims = [0, 1, 2] : (tensor<16x24x36xi1>, tensor<3xindex>) -> tensor<16x24x36xi1>
+// CHECK-DAG:       [[VAR_2_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[PARAM_1_]], [[VAR_0_]], dims = [0, 1, 2] : (tensor<16x24x36xi64>, tensor<3xindex>) -> tensor<16x24x36xi64>
+// CHECK-DAG:       [[VAR_3_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[PARAM_2_]], [[VAR_0_]], dims = [0, 1, 2] : (tensor<16x24x36xi64>, tensor<3xindex>) -> tensor<16x24x36xi64>
+// CHECK:           [[VAR_4_:%.+]] = stablehlo.select [[VAR_1_]], [[VAR_2_]], [[VAR_3_]] : tensor<16x24x36xi1>, tensor<16x24x36xi64>
+// CHECK:           return [[VAR_4_]] : tensor<16x24x36xi64>
+// CHECK:         }
+}
diff --git a/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM-loop.mlir b/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM-loop.mlir
new file mode 100644
index 0000000000..c6414342bb
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM-loop.mlir
@@ -0,0 +1,621 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-stablehlo="enable-unroll=false" --canonicalize -split-input-file %s | FileCheck %s
+func.func @test_lstm_loop(%arg0 : tensor<128x16x512xf32>, %arg1 : tensor<2x2048xf32>, %arg2 : tensor<2x1024x512xf32>, %arg3 : tensor<2x1024x256xf32>) -> tensor<128x2x16x256xf32> {
+  %0 = onnx.Constant dense<0.000000e+00> : tensor<2x16x256xf32>
+  %1 = "onnx.NoValue"() {value} : () -> none
+  %Y, %Y_h, %Y_c = "onnx.LSTM"(%arg0, %arg2, %arg3, %arg1, %1, %0, %0, %1) {direction = "bidirectional", hidden_size = 256 : si64, input_forget = 0 : si64, layout = 0 : si64} : (tensor<128x16x512xf32>, tensor<2x1024x512xf32>, tensor<2x1024x256xf32>, tensor<2x2048xf32>, none, tensor<2x16x256xf32>, tensor<2x16x256xf32>, none) -> (tensor<128x2x16x256xf32>, tensor<2x16x256xf32>, tensor<2x16x256xf32>)
+  return %Y : tensor<128x2x16x256xf32>
+// CHECK-LABEL:  func.func @test_lstm_loop
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<128x16x512xf32>, [[PARAM_1_:%.+]]: tensor<2x2048xf32>, [[PARAM_2_:%.+]]: tensor<2x1024x512xf32>, [[PARAM_3_:%.+]]: tensor<2x1024x256xf32>) -> tensor<128x2x16x256xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = shape.const_shape [16, 256] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_1_:%.+]] = shape.const_shape [1, 1] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_2_:%.+]] = shape.const_shape [1, 1, 16, 256] : tensor<4xindex>
+// CHECK-DAG:       [[VAR_3_:%.+]] = shape.const_shape [16, 512] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_4_:%.+]] = shape.const_shape [128, 16, 512] : tensor<3xindex>
+// CHECK-DAG:       [[VAR_5_:%.+]] = shape.const_shape [2048] : tensor<1xindex>
+// CHECK-DAG:       [[VAR_6_:%.+]] = shape.const_shape [1024, 256] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_7_:%.+]] = shape.const_shape [1024, 512] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_8_:%.+]] = shape.const_shape [2, 16, 256] : tensor<3xindex>
+// CHECK-DAG:       [[VAR_9_:%.+]] = stablehlo.constant dense<127> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_10_:%.+]] = stablehlo.constant dense<1024> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_11_:%.+]] = stablehlo.constant dense<768> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_12_:%.+]] = shape.const_shape [16, 1024] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_13_:%.+]] = stablehlo.constant dense<512> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_14_:%.+]] = shape.const_shape [1] : tensor<1xindex>
+// CHECK-DAG:       [[VAR_15_:%.+]] = stablehlo.constant dense<128> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_16_:%.+]] = stablehlo.constant dense<256> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_17_:%.+]] = stablehlo.constant dense<16> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_18_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<2x16x256xf32>
+// CHECK-DAG:       [[VAR_19_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<128x1x16x256xf32>
+// CHECK-DAG:       [[VAR_20_:%.+]] = stablehlo.constant dense<0> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_21_:%.+]] = stablehlo.constant dense<1> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_22_:%.+]] = stablehlo.constant dense<2> : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_23_:%.+]] = stablehlo.slice [[VAR_20_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_24_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_25_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_26_:%.+]] = stablehlo.compare  LT, [[VAR_24_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_27_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_26_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
+// CHECK-DAG:       [[VAR_28_:%.+]] = stablehlo.negate [[VAR_24_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_29_:%.+]] = stablehlo.add [[VAR_25_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_30_:%.+]] = stablehlo.add [[VAR_23_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_31_:%.+]] = stablehlo.reverse [[VAR_18_]], dims = [0] : tensor<2x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_32_:%.+]] = stablehlo.select [[VAR_26_]], [[VAR_29_]], [[VAR_23_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_33_:%.+]] = stablehlo.select [[VAR_26_]], [[VAR_30_]], [[VAR_25_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_34_:%.+]] = stablehlo.select [[VAR_26_]], [[VAR_28_]], [[VAR_24_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_35_:%.+]] = stablehlo.select [[VAR_27_]], [[VAR_31_]], [[VAR_18_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
+// CHECK:           [[VAR_36_:%.+]] = stablehlo.compare  GT, [[VAR_33_]], [[VAR_22_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_37_:%.+]] = stablehlo.select [[VAR_36_]], [[VAR_22_]], [[VAR_33_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_38_:%.+]] = stablehlo.compare  LT, [[VAR_37_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_39_:%.+]] = stablehlo.add [[VAR_37_]], [[VAR_22_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_40_:%.+]] = stablehlo.select [[VAR_38_]], [[VAR_39_]], [[VAR_37_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_41_:%.+]] = stablehlo.compare  LT, [[VAR_32_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_42_:%.+]] = stablehlo.add [[VAR_32_]], [[VAR_22_]] : tensor<1xi64>
+// CHECK:           [[VAR_43_:%.+]] = stablehlo.select [[VAR_41_]], [[VAR_42_]], [[VAR_32_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_44_:%.+]] = stablehlo.concatenate [[VAR_43_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_45_:%.+]] = stablehlo.concatenate [[VAR_40_]], [[VAR_17_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_46_:%.+]] = stablehlo.concatenate [[VAR_34_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_47_:%.+]] = stablehlo.real_dynamic_slice [[VAR_35_]], [[VAR_44_]], [[VAR_45_]], [[VAR_46_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_48_:%.+]] = stablehlo.dynamic_reshape [[VAR_47_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_49_:%.+]] = stablehlo.slice [[VAR_20_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_50_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_51_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_52_:%.+]] = stablehlo.compare  LT, [[VAR_50_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_53_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_52_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
+// CHECK-DAG:       [[VAR_54_:%.+]] = stablehlo.negate [[VAR_50_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_55_:%.+]] = stablehlo.add [[VAR_51_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_56_:%.+]] = stablehlo.add [[VAR_49_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_57_:%.+]] = stablehlo.reverse [[VAR_18_]], dims = [0] : tensor<2x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_58_:%.+]] = stablehlo.select [[VAR_52_]], [[VAR_55_]], [[VAR_49_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_59_:%.+]] = stablehlo.select [[VAR_52_]], [[VAR_56_]], [[VAR_51_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_60_:%.+]] = stablehlo.select [[VAR_52_]], [[VAR_54_]], [[VAR_50_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_61_:%.+]] = stablehlo.select [[VAR_53_]], [[VAR_57_]], [[VAR_18_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
+// CHECK:           [[VAR_62_:%.+]] = stablehlo.compare  GT, [[VAR_59_]], [[VAR_22_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_63_:%.+]] = stablehlo.select [[VAR_62_]], [[VAR_22_]], [[VAR_59_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_64_:%.+]] = stablehlo.compare  LT, [[VAR_63_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_65_:%.+]] = stablehlo.add [[VAR_63_]], [[VAR_22_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_66_:%.+]] = stablehlo.select [[VAR_64_]], [[VAR_65_]], [[VAR_63_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_67_:%.+]] = stablehlo.compare  LT, [[VAR_58_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_68_:%.+]] = stablehlo.add [[VAR_58_]], [[VAR_22_]] : tensor<1xi64>
+// CHECK:           [[VAR_69_:%.+]] = stablehlo.select [[VAR_67_]], [[VAR_68_]], [[VAR_58_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_70_:%.+]] = stablehlo.concatenate [[VAR_69_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_71_:%.+]] = stablehlo.concatenate [[VAR_66_]], [[VAR_17_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_72_:%.+]] = stablehlo.concatenate [[VAR_60_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_73_:%.+]] = stablehlo.real_dynamic_slice [[VAR_61_]], [[VAR_70_]], [[VAR_71_]], [[VAR_72_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_74_:%.+]] = stablehlo.dynamic_reshape [[VAR_73_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_75_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_76_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_77_:%.+]] = stablehlo.slice [[VAR_22_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_78_:%.+]] = stablehlo.compare  LT, [[VAR_76_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_79_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_78_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
+// CHECK-DAG:       [[VAR_80_:%.+]] = stablehlo.negate [[VAR_76_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_81_:%.+]] = stablehlo.add [[VAR_77_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_82_:%.+]] = stablehlo.add [[VAR_75_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_83_:%.+]] = stablehlo.reverse [[VAR_18_]], dims = [0] : tensor<2x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_84_:%.+]] = stablehlo.select [[VAR_78_]], [[VAR_81_]], [[VAR_75_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_85_:%.+]] = stablehlo.select [[VAR_78_]], [[VAR_82_]], [[VAR_77_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_86_:%.+]] = stablehlo.select [[VAR_78_]], [[VAR_80_]], [[VAR_76_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_87_:%.+]] = stablehlo.select [[VAR_79_]], [[VAR_83_]], [[VAR_18_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
+// CHECK:           [[VAR_88_:%.+]] = stablehlo.compare  GT, [[VAR_85_]], [[VAR_22_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_89_:%.+]] = stablehlo.select [[VAR_88_]], [[VAR_22_]], [[VAR_85_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_90_:%.+]] = stablehlo.compare  LT, [[VAR_89_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_91_:%.+]] = stablehlo.add [[VAR_89_]], [[VAR_22_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_92_:%.+]] = stablehlo.select [[VAR_90_]], [[VAR_91_]], [[VAR_89_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_93_:%.+]] = stablehlo.compare  LT, [[VAR_84_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_94_:%.+]] = stablehlo.add [[VAR_84_]], [[VAR_22_]] : tensor<1xi64>
+// CHECK:           [[VAR_95_:%.+]] = stablehlo.select [[VAR_93_]], [[VAR_94_]], [[VAR_84_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_96_:%.+]] = stablehlo.concatenate [[VAR_95_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_97_:%.+]] = stablehlo.concatenate [[VAR_92_]], [[VAR_17_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_98_:%.+]] = stablehlo.concatenate [[VAR_86_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_99_:%.+]] = stablehlo.real_dynamic_slice [[VAR_87_]], [[VAR_96_]], [[VAR_97_]], [[VAR_98_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_100_:%.+]] = stablehlo.dynamic_reshape [[VAR_99_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_101_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_102_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_103_:%.+]] = stablehlo.slice [[VAR_22_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_104_:%.+]] = stablehlo.compare  LT, [[VAR_102_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_105_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_104_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
+// CHECK-DAG:       [[VAR_106_:%.+]] = stablehlo.negate [[VAR_102_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_107_:%.+]] = stablehlo.add [[VAR_103_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_108_:%.+]] = stablehlo.add [[VAR_101_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_109_:%.+]] = stablehlo.reverse [[VAR_18_]], dims = [0] : tensor<2x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_110_:%.+]] = stablehlo.select [[VAR_104_]], [[VAR_107_]], [[VAR_101_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_111_:%.+]] = stablehlo.select [[VAR_104_]], [[VAR_108_]], [[VAR_103_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_112_:%.+]] = stablehlo.select [[VAR_104_]], [[VAR_106_]], [[VAR_102_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_113_:%.+]] = stablehlo.select [[VAR_105_]], [[VAR_109_]], [[VAR_18_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
+// CHECK:           [[VAR_114_:%.+]] = stablehlo.compare  GT, [[VAR_111_]], [[VAR_22_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_115_:%.+]] = stablehlo.select [[VAR_114_]], [[VAR_22_]], [[VAR_111_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_116_:%.+]] = stablehlo.compare  LT, [[VAR_115_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_117_:%.+]] = stablehlo.add [[VAR_115_]], [[VAR_22_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_118_:%.+]] = stablehlo.select [[VAR_116_]], [[VAR_117_]], [[VAR_115_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_119_:%.+]] = stablehlo.compare  LT, [[VAR_110_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_120_:%.+]] = stablehlo.add [[VAR_110_]], [[VAR_22_]] : tensor<1xi64>
+// CHECK:           [[VAR_121_:%.+]] = stablehlo.select [[VAR_119_]], [[VAR_120_]], [[VAR_110_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_122_:%.+]] = stablehlo.concatenate [[VAR_121_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_123_:%.+]] = stablehlo.concatenate [[VAR_118_]], [[VAR_17_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_124_:%.+]] = stablehlo.concatenate [[VAR_112_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_125_:%.+]] = stablehlo.real_dynamic_slice [[VAR_113_]], [[VAR_122_]], [[VAR_123_]], [[VAR_124_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_126_:%.+]] = stablehlo.dynamic_reshape [[VAR_125_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_127_:%.+]] = stablehlo.slice [[PARAM_2_]] [0:1, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-DAG:       [[VAR_128_:%.+]] = stablehlo.slice [[PARAM_2_]] [1:2, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_129_:%.+]] = stablehlo.dynamic_reshape [[VAR_127_]], [[VAR_7_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_130_:%.+]] = stablehlo.dynamic_reshape [[VAR_128_]], [[VAR_7_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_131_:%.+]] = stablehlo.slice [[PARAM_3_]] [0:1, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-DAG:       [[VAR_132_:%.+]] = stablehlo.slice [[PARAM_3_]] [1:2, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_133_:%.+]] = stablehlo.dynamic_reshape [[VAR_131_]], [[VAR_6_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_134_:%.+]] = stablehlo.dynamic_reshape [[VAR_132_]], [[VAR_6_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_135_:%.+]] = stablehlo.transpose [[VAR_129_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_136_:%.+]] = stablehlo.transpose [[VAR_133_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_137_:%.+]] = stablehlo.transpose [[VAR_130_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-DAG:       [[VAR_138_:%.+]] = stablehlo.transpose [[VAR_134_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_139_:%.+]] = stablehlo.slice [[PARAM_1_]] [0:1, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-DAG:       [[VAR_140_:%.+]] = stablehlo.slice [[PARAM_1_]] [1:2, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_141_:%.+]] = stablehlo.dynamic_reshape [[VAR_139_]], [[VAR_5_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
+// CHECK-DAG:       [[VAR_142_:%.+]] = stablehlo.dynamic_reshape [[VAR_140_]], [[VAR_5_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_143_:%.+]] = stablehlo.slice [[VAR_141_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_144_:%.+]] = stablehlo.slice [[VAR_141_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_145_:%.+]] = stablehlo.slice [[VAR_141_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_146_:%.+]] = stablehlo.slice [[VAR_141_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_147_:%.+]] = stablehlo.slice [[VAR_141_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_148_:%.+]] = stablehlo.slice [[VAR_141_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_149_:%.+]] = stablehlo.slice [[VAR_141_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_150_:%.+]] = stablehlo.slice [[VAR_141_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_151_:%.+]] = stablehlo.slice [[VAR_142_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_152_:%.+]] = stablehlo.slice [[VAR_142_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_153_:%.+]] = stablehlo.slice [[VAR_142_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_154_:%.+]] = stablehlo.slice [[VAR_142_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_155_:%.+]] = stablehlo.slice [[VAR_142_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_156_:%.+]] = stablehlo.slice [[VAR_142_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_157_:%.+]] = stablehlo.slice [[VAR_142_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_158_:%.+]] = stablehlo.slice [[VAR_142_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_159_:%.+]]:4 = stablehlo.while([[VAR_iterArg_:%.+]] = [[VAR_20_]], [[VAR_iterArg_0_:%.+]] = [[VAR_19_]], [[VAR_iterArg_1_:%.+]] = [[VAR_48_]], [[VAR_iterArg_2_:%.+]] = [[VAR_74_]]) : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK:            cond {
+// CHECK:             [[VAR_162_:%.+]] = stablehlo.compare  LT, [[VAR_iterArg_]], [[VAR_15_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_163_:%.+]] = stablehlo.reshape [[VAR_162_]] : (tensor<1xi1>) -> tensor<i1>
+// CHECK:             stablehlo.return [[VAR_163_]] : tensor<i1>
+// CHECK:           } do {
+// CHECK-DAG:         [[VAR_162_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_163_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_21_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_164_:%.+]] = stablehlo.add [[VAR_162_1_]], [[VAR_163_1_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_165_:%.+]] = stablehlo.slice [[VAR_iterArg_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_166_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_167_:%.+]] = stablehlo.slice [[VAR_164_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_168_:%.+]] = stablehlo.compare  LT, [[VAR_166_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_169_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_168_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<128x16x512xi1>
+// CHECK-DAG:         [[VAR_170_:%.+]] = stablehlo.negate [[VAR_166_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_171_:%.+]] = stablehlo.add [[VAR_167_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_172_:%.+]] = stablehlo.add [[VAR_165_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_173_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<128x16x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_174_:%.+]] = stablehlo.select [[VAR_168_]], [[VAR_171_]], [[VAR_165_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_175_:%.+]] = stablehlo.select [[VAR_168_]], [[VAR_172_]], [[VAR_167_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_176_:%.+]] = stablehlo.select [[VAR_168_]], [[VAR_170_]], [[VAR_166_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_177_:%.+]] = stablehlo.select [[VAR_169_]], [[VAR_173_]], [[PARAM_0_]] : tensor<128x16x512xi1>, tensor<128x16x512xf32>
+// CHECK:             [[VAR_178_:%.+]] = stablehlo.compare  GT, [[VAR_175_]], [[VAR_15_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_179_:%.+]] = stablehlo.select [[VAR_178_]], [[VAR_15_]], [[VAR_175_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_180_:%.+]] = stablehlo.compare  LT, [[VAR_179_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_181_:%.+]] = stablehlo.add [[VAR_179_]], [[VAR_15_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_182_:%.+]] = stablehlo.select [[VAR_180_]], [[VAR_181_]], [[VAR_179_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_183_:%.+]] = stablehlo.compare  LT, [[VAR_174_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_184_:%.+]] = stablehlo.add [[VAR_174_]], [[VAR_15_]] : tensor<1xi64>
+// CHECK:             [[VAR_185_:%.+]] = stablehlo.select [[VAR_183_]], [[VAR_184_]], [[VAR_174_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_186_:%.+]] = stablehlo.concatenate [[VAR_185_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:         [[VAR_187_:%.+]] = stablehlo.concatenate [[VAR_182_]], [[VAR_17_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:         [[VAR_188_:%.+]] = stablehlo.concatenate [[VAR_176_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:             [[VAR_189_:%.+]] = stablehlo.real_dynamic_slice [[VAR_177_]], [[VAR_186_]], [[VAR_187_]], [[VAR_188_]] : (tensor<128x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
+// CHECK:             [[VAR_190_:%.+]] = stablehlo.dynamic_reshape [[VAR_189_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_191_:%.+]] = stablehlo.broadcast_in_dim [[VAR_190_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_192_:%.+]] = stablehlo.broadcast_in_dim [[VAR_135_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_193_:%.+]] = stablehlo.dot [[VAR_191_]], [[VAR_192_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_194_:%.+]] = stablehlo.broadcast_in_dim [[VAR_iterArg_1_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_195_:%.+]] = stablehlo.broadcast_in_dim [[VAR_136_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_196_:%.+]] = stablehlo.dot [[VAR_194_]], [[VAR_195_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_197_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_193_]], [[VAR_12_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:             [[VAR_198_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_196_]], [[VAR_12_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_199_:%.+]] = stablehlo.add [[VAR_197_]], [[VAR_198_]] : tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_200_:%.+]] = stablehlo.slice [[VAR_20_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_201_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_202_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:             [[VAR_203_:%.+]] = stablehlo.compare  LT, [[VAR_201_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_204_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_203_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:         [[VAR_205_:%.+]] = stablehlo.negate [[VAR_201_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_206_:%.+]] = stablehlo.add [[VAR_202_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_207_:%.+]] = stablehlo.add [[VAR_200_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_208_:%.+]] = stablehlo.reverse [[VAR_199_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_209_:%.+]] = stablehlo.select [[VAR_203_]], [[VAR_206_]], [[VAR_200_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_210_:%.+]] = stablehlo.select [[VAR_203_]], [[VAR_207_]], [[VAR_202_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_211_:%.+]] = stablehlo.select [[VAR_203_]], [[VAR_205_]], [[VAR_201_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_212_:%.+]] = stablehlo.select [[VAR_204_]], [[VAR_208_]], [[VAR_199_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:             [[VAR_213_:%.+]] = stablehlo.compare  GT, [[VAR_210_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_214_:%.+]] = stablehlo.select [[VAR_213_]], [[VAR_10_]], [[VAR_210_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_215_:%.+]] = stablehlo.compare  LT, [[VAR_214_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_216_:%.+]] = stablehlo.add [[VAR_214_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_217_:%.+]] = stablehlo.select [[VAR_215_]], [[VAR_216_]], [[VAR_214_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_218_:%.+]] = stablehlo.compare  LT, [[VAR_209_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_219_:%.+]] = stablehlo.add [[VAR_209_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK:             [[VAR_220_:%.+]] = stablehlo.select [[VAR_218_]], [[VAR_219_]], [[VAR_209_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_221_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_220_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_222_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_217_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_223_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_21_]]1, dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_224_:%.+]] = stablehlo.real_dynamic_slice [[VAR_212_]], [[VAR_221_]], [[VAR_222_]], [[VAR_223_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_225_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_226_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_227_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:             [[VAR_228_:%.+]] = stablehlo.compare  LT, [[VAR_226_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_229_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_228_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:         [[VAR_230_:%.+]] = stablehlo.negate [[VAR_226_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_231_:%.+]] = stablehlo.add [[VAR_227_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_232_:%.+]] = stablehlo.add [[VAR_225_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_233_:%.+]] = stablehlo.reverse [[VAR_199_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_234_:%.+]] = stablehlo.select [[VAR_228_]], [[VAR_231_]], [[VAR_225_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_235_:%.+]] = stablehlo.select [[VAR_228_]], [[VAR_232_]], [[VAR_227_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_236_:%.+]] = stablehlo.select [[VAR_228_]], [[VAR_230_]], [[VAR_226_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_237_:%.+]] = stablehlo.select [[VAR_229_]], [[VAR_233_]], [[VAR_199_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:             [[VAR_238_:%.+]] = stablehlo.compare  GT, [[VAR_235_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_239_:%.+]] = stablehlo.select [[VAR_238_]], [[VAR_10_]], [[VAR_235_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_240_:%.+]] = stablehlo.compare  LT, [[VAR_239_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_241_:%.+]] = stablehlo.add [[VAR_239_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_242_:%.+]] = stablehlo.select [[VAR_240_]], [[VAR_241_]], [[VAR_239_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_243_:%.+]] = stablehlo.compare  LT, [[VAR_234_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_244_:%.+]] = stablehlo.add [[VAR_234_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK:             [[VAR_245_:%.+]] = stablehlo.select [[VAR_243_]], [[VAR_244_]], [[VAR_234_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_246_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_245_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_247_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_242_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_248_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_236_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_249_:%.+]] = stablehlo.real_dynamic_slice [[VAR_237_]], [[VAR_246_]], [[VAR_247_]], [[VAR_248_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_250_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_251_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_252_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:             [[VAR_253_:%.+]] = stablehlo.compare  LT, [[VAR_251_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_254_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_253_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:         [[VAR_255_:%.+]] = stablehlo.negate [[VAR_251_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_256_:%.+]] = stablehlo.add [[VAR_252_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_257_:%.+]] = stablehlo.add [[VAR_250_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_258_:%.+]] = stablehlo.reverse [[VAR_199_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_259_:%.+]] = stablehlo.select [[VAR_253_]], [[VAR_256_]], [[VAR_250_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_260_:%.+]] = stablehlo.select [[VAR_253_]], [[VAR_257_]], [[VAR_252_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_261_:%.+]] = stablehlo.select [[VAR_253_]], [[VAR_255_]], [[VAR_251_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_262_:%.+]] = stablehlo.select [[VAR_254_]], [[VAR_258_]], [[VAR_199_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:             [[VAR_263_:%.+]] = stablehlo.compare  GT, [[VAR_260_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_264_:%.+]] = stablehlo.select [[VAR_263_]], [[VAR_10_]], [[VAR_260_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_265_:%.+]] = stablehlo.compare  LT, [[VAR_264_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_266_:%.+]] = stablehlo.add [[VAR_264_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_267_:%.+]] = stablehlo.select [[VAR_265_]], [[VAR_266_]], [[VAR_264_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_268_:%.+]] = stablehlo.compare  LT, [[VAR_259_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_269_:%.+]] = stablehlo.add [[VAR_259_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK:             [[VAR_270_:%.+]] = stablehlo.select [[VAR_268_]], [[VAR_269_]], [[VAR_259_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_271_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_270_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_272_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_267_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_273_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_261_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_274_:%.+]] = stablehlo.real_dynamic_slice [[VAR_262_]], [[VAR_271_]], [[VAR_272_]], [[VAR_273_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_275_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_276_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_277_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:             [[VAR_278_:%.+]] = stablehlo.compare  LT, [[VAR_276_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_279_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_278_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:         [[VAR_280_:%.+]] = stablehlo.negate [[VAR_276_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_281_:%.+]] = stablehlo.add [[VAR_277_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_282_:%.+]] = stablehlo.add [[VAR_275_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_283_:%.+]] = stablehlo.reverse [[VAR_199_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_284_:%.+]] = stablehlo.select [[VAR_278_]], [[VAR_281_]], [[VAR_275_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_285_:%.+]] = stablehlo.select [[VAR_278_]], [[VAR_282_]], [[VAR_277_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_286_:%.+]] = stablehlo.select [[VAR_278_]], [[VAR_280_]], [[VAR_276_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_287_:%.+]] = stablehlo.select [[VAR_279_]], [[VAR_283_]], [[VAR_199_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:             [[VAR_288_:%.+]] = stablehlo.compare  GT, [[VAR_285_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_289_:%.+]] = stablehlo.select [[VAR_288_]], [[VAR_10_]], [[VAR_285_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_290_:%.+]] = stablehlo.compare  LT, [[VAR_289_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_291_:%.+]] = stablehlo.add [[VAR_289_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_292_:%.+]] = stablehlo.select [[VAR_290_]], [[VAR_291_]], [[VAR_289_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_293_:%.+]] = stablehlo.compare  LT, [[VAR_284_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_294_:%.+]] = stablehlo.add [[VAR_284_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK:             [[VAR_295_:%.+]] = stablehlo.select [[VAR_293_]], [[VAR_294_]], [[VAR_284_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_296_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_295_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_297_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_292_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_298_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_286_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_299_:%.+]] = stablehlo.real_dynamic_slice [[VAR_287_]], [[VAR_296_]], [[VAR_297_]], [[VAR_298_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_300_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_224_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_301_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_143_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_302_:%.+]] = stablehlo.add [[VAR_300_]], [[VAR_301_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_303_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_302_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_304_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_147_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_305_:%.+]] = stablehlo.add [[VAR_303_]], [[VAR_304_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_306_:%.+]] = stablehlo.logistic [[VAR_305_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_307_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_274_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_308_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_145_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_309_:%.+]] = stablehlo.add [[VAR_307_]], [[VAR_308_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_310_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_309_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_311_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_149_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_312_:%.+]] = stablehlo.add [[VAR_310_]], [[VAR_311_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_313_:%.+]] = stablehlo.logistic [[VAR_312_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_314_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_299_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_315_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_146_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_316_:%.+]] = stablehlo.add [[VAR_314_]], [[VAR_315_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_317_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_316_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_318_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_150_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_319_:%.+]] = stablehlo.add [[VAR_317_]], [[VAR_318_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_320_:%.+]] = stablehlo.tanh [[VAR_319_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_321_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_313_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_322_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_2_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_323_:%.+]] = stablehlo.multiply [[VAR_321_]], [[VAR_322_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_324_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_306_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_325_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_320_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_326_:%.+]] = stablehlo.multiply [[VAR_324_]], [[VAR_325_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_327_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_323_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_328_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_326_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_329_:%.+]] = stablehlo.add [[VAR_327_]], [[VAR_328_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_330_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_249_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_331_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_144_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_332_:%.+]] = stablehlo.add [[VAR_330_]], [[VAR_331_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_333_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_332_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_334_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_148_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_335_:%.+]] = stablehlo.add [[VAR_333_]], [[VAR_334_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_336_:%.+]] = stablehlo.logistic [[VAR_335_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_337_:%.+]] = stablehlo.tanh [[VAR_329_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_338_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_336_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_339_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_337_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_340_:%.+]] = stablehlo.multiply [[VAR_338_]], [[VAR_339_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_341_:%.+]] = stablehlo.dynamic_reshape [[VAR_340_]], [[VAR_2_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:         [[VAR_342_:%.+]] = stablehlo.dynamic_reshape [[VAR_iterArg_]], [[VAR_1_]] : (tensor<1xi64>, tensor<2xindex>) -> tensor<1x1xi64>
+// CHECK:             [[VAR_343_:%.+]] = "stablehlo.scatter"([[VAR_iterArg_0_]], [[VAR_342_]], [[VAR_341_]]) ({
+// CHECK:             ^bb0([[arg4_:%.+]]: tensor<f32>, [[arg5_:%.+]]: tensor<f32>):
+// CHECK:               stablehlo.return [[arg5_]] : tensor<f32>
+// CHECK:             }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2, 3], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<128x1x16x256xf32>, tensor<1x1xi64>, tensor<1x1x16x256xf32>) -> tensor<128x1x16x256xf32>
+// CHECK-DAG:         [[VAR_344_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_345_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_21_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK:             [[VAR_346_:%.+]] = stablehlo.add [[VAR_344_]], [[VAR_345_]] : tensor<1xi64>
+// CHECK:             stablehlo.return [[VAR_346_]], [[VAR_343_]], [[VAR_340_]], [[VAR_329_]] : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK:           }
+// CHECK:           [[VAR_160_:%.+]]:4 = stablehlo.while([[VAR_iterArg_1_:%.+]] = [[VAR_9_]], [[VAR_iterArg_0_1_:%.+]] = [[VAR_19_]], [[VAR_iterArg_1_1_:%.+]] = [[VAR_100_]], [[VAR_iterArg_2_1_:%.+]] = [[VAR_126_]]) : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK:            cond {
+// CHECK:             [[VAR_162_2_:%.+]] = stablehlo.compare  GE, [[VAR_iterArg_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_163_2_:%.+]] = stablehlo.reshape [[VAR_162_2_]] : (tensor<1xi1>) -> tensor<i1>
+// CHECK:             stablehlo.return [[VAR_163_2_]] : tensor<i1>
+// CHECK:           } do {
+// CHECK-DAG:         [[VAR_162_3_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_1_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_163_3_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_21_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_164_1_:%.+]] = stablehlo.add [[VAR_162_3_]], [[VAR_163_3_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_165_1_:%.+]] = stablehlo.slice [[VAR_iterArg_1_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_166_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_167_1_:%.+]] = stablehlo.slice [[VAR_164_1_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_168_1_:%.+]] = stablehlo.compare  LT, [[VAR_166_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_169_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_168_1_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<128x16x512xi1>
+// CHECK-DAG:         [[VAR_170_1_:%.+]] = stablehlo.negate [[VAR_166_1_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_171_1_:%.+]] = stablehlo.add [[VAR_167_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_172_1_:%.+]] = stablehlo.add [[VAR_165_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_173_1_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<128x16x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_174_1_:%.+]] = stablehlo.select [[VAR_168_1_]], [[VAR_171_1_]], [[VAR_165_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_175_1_:%.+]] = stablehlo.select [[VAR_168_1_]], [[VAR_172_1_]], [[VAR_167_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_176_1_:%.+]] = stablehlo.select [[VAR_168_1_]], [[VAR_170_1_]], [[VAR_166_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_177_1_:%.+]] = stablehlo.select [[VAR_169_1_]], [[VAR_173_1_]], [[PARAM_0_]] : tensor<128x16x512xi1>, tensor<128x16x512xf32>
+// CHECK:             [[VAR_178_1_:%.+]] = stablehlo.compare  GT, [[VAR_175_1_]], [[VAR_15_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_179_1_:%.+]] = stablehlo.select [[VAR_178_1_]], [[VAR_15_]], [[VAR_175_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_180_1_:%.+]] = stablehlo.compare  LT, [[VAR_179_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_181_1_:%.+]] = stablehlo.add [[VAR_179_1_]], [[VAR_15_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_182_1_:%.+]] = stablehlo.select [[VAR_180_1_]], [[VAR_181_1_]], [[VAR_179_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_183_1_:%.+]] = stablehlo.compare  LT, [[VAR_174_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_184_1_:%.+]] = stablehlo.add [[VAR_174_1_]], [[VAR_15_]] : tensor<1xi64>
+// CHECK:             [[VAR_185_1_:%.+]] = stablehlo.select [[VAR_183_1_]], [[VAR_184_1_]], [[VAR_174_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_186_1_:%.+]] = stablehlo.concatenate [[VAR_185_1_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:         [[VAR_187_1_:%.+]] = stablehlo.concatenate [[VAR_182_1_]], [[VAR_17_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:         [[VAR_188_1_:%.+]] = stablehlo.concatenate [[VAR_176_1_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:             [[VAR_189_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_177_1_]], [[VAR_186_1_]], [[VAR_187_1_]], [[VAR_188_1_]] : (tensor<128x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
+// CHECK:             [[VAR_190_1_:%.+]] = stablehlo.dynamic_reshape [[VAR_189_1_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_191_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_190_1_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_192_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_137_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_193_1_:%.+]] = stablehlo.dot [[VAR_191_1_]], [[VAR_192_1_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_194_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_iterArg_1_1_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_195_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_138_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_196_1_:%.+]] = stablehlo.dot [[VAR_194_1_]], [[VAR_195_1_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_197_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_193_1_]], [[VAR_12_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:             [[VAR_198_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_196_1_]], [[VAR_12_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_199_1_:%.+]] = stablehlo.add [[VAR_197_1_]], [[VAR_198_1_]] : tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_200_1_:%.+]] = stablehlo.slice [[VAR_20_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_201_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_202_1_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:             [[VAR_203_1_:%.+]] = stablehlo.compare  LT, [[VAR_201_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_204_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_203_1_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:         [[VAR_205_1_:%.+]] = stablehlo.negate [[VAR_201_1_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_206_1_:%.+]] = stablehlo.add [[VAR_202_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_207_1_:%.+]] = stablehlo.add [[VAR_200_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_208_1_:%.+]] = stablehlo.reverse [[VAR_199_1_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_209_1_:%.+]] = stablehlo.select [[VAR_203_1_]], [[VAR_206_1_]], [[VAR_200_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_210_1_:%.+]] = stablehlo.select [[VAR_203_1_]], [[VAR_207_1_]], [[VAR_202_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_211_1_:%.+]] = stablehlo.select [[VAR_203_1_]], [[VAR_205_1_]], [[VAR_201_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_212_1_:%.+]] = stablehlo.select [[VAR_204_1_]], [[VAR_208_1_]], [[VAR_199_1_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:             [[VAR_213_1_:%.+]] = stablehlo.compare  GT, [[VAR_210_1_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_214_1_:%.+]] = stablehlo.select [[VAR_213_1_]], [[VAR_10_]], [[VAR_210_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_215_1_:%.+]] = stablehlo.compare  LT, [[VAR_214_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_216_1_:%.+]] = stablehlo.add [[VAR_214_1_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_217_1_:%.+]] = stablehlo.select [[VAR_215_1_]], [[VAR_216_1_]], [[VAR_214_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_218_1_:%.+]] = stablehlo.compare  LT, [[VAR_209_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_219_1_:%.+]] = stablehlo.add [[VAR_209_1_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK:             [[VAR_220_1_:%.+]] = stablehlo.select [[VAR_218_1_]], [[VAR_219_1_]], [[VAR_209_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_221_1_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_220_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_222_1_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_217_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_223_1_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_21_]]1, dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_224_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_212_1_]], [[VAR_221_1_]], [[VAR_222_1_]], [[VAR_223_1_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_225_1_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_226_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_227_1_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:             [[VAR_228_1_:%.+]] = stablehlo.compare  LT, [[VAR_226_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_229_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_228_1_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:         [[VAR_230_1_:%.+]] = stablehlo.negate [[VAR_226_1_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_231_1_:%.+]] = stablehlo.add [[VAR_227_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_232_1_:%.+]] = stablehlo.add [[VAR_225_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_233_1_:%.+]] = stablehlo.reverse [[VAR_199_1_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_234_1_:%.+]] = stablehlo.select [[VAR_228_1_]], [[VAR_231_1_]], [[VAR_225_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_235_1_:%.+]] = stablehlo.select [[VAR_228_1_]], [[VAR_232_1_]], [[VAR_227_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_236_1_:%.+]] = stablehlo.select [[VAR_228_1_]], [[VAR_230_1_]], [[VAR_226_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_237_1_:%.+]] = stablehlo.select [[VAR_229_1_]], [[VAR_233_1_]], [[VAR_199_1_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:             [[VAR_238_1_:%.+]] = stablehlo.compare  GT, [[VAR_235_1_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_239_1_:%.+]] = stablehlo.select [[VAR_238_1_]], [[VAR_10_]], [[VAR_235_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_240_1_:%.+]] = stablehlo.compare  LT, [[VAR_239_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_241_1_:%.+]] = stablehlo.add [[VAR_239_1_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_242_1_:%.+]] = stablehlo.select [[VAR_240_1_]], [[VAR_241_1_]], [[VAR_239_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_243_1_:%.+]] = stablehlo.compare  LT, [[VAR_234_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_244_1_:%.+]] = stablehlo.add [[VAR_234_1_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK:             [[VAR_245_1_:%.+]] = stablehlo.select [[VAR_243_1_]], [[VAR_244_1_]], [[VAR_234_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_246_1_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_245_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_247_1_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_242_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_248_1_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_236_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_249_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_237_1_]], [[VAR_246_1_]], [[VAR_247_1_]], [[VAR_248_1_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_250_1_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_251_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_252_1_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:             [[VAR_253_1_:%.+]] = stablehlo.compare  LT, [[VAR_251_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_254_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_253_1_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:         [[VAR_255_1_:%.+]] = stablehlo.negate [[VAR_251_1_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_256_1_:%.+]] = stablehlo.add [[VAR_252_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_257_1_:%.+]] = stablehlo.add [[VAR_250_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_258_1_:%.+]] = stablehlo.reverse [[VAR_199_1_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_259_1_:%.+]] = stablehlo.select [[VAR_253_1_]], [[VAR_256_1_]], [[VAR_250_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_260_1_:%.+]] = stablehlo.select [[VAR_253_1_]], [[VAR_257_1_]], [[VAR_252_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_261_1_:%.+]] = stablehlo.select [[VAR_253_1_]], [[VAR_255_1_]], [[VAR_251_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_262_1_:%.+]] = stablehlo.select [[VAR_254_1_]], [[VAR_258_1_]], [[VAR_199_1_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:             [[VAR_263_1_:%.+]] = stablehlo.compare  GT, [[VAR_260_1_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_264_1_:%.+]] = stablehlo.select [[VAR_263_1_]], [[VAR_10_]], [[VAR_260_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_265_1_:%.+]] = stablehlo.compare  LT, [[VAR_264_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_266_1_:%.+]] = stablehlo.add [[VAR_264_1_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_267_1_:%.+]] = stablehlo.select [[VAR_265_1_]], [[VAR_266_1_]], [[VAR_264_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_268_1_:%.+]] = stablehlo.compare  LT, [[VAR_259_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_269_1_:%.+]] = stablehlo.add [[VAR_259_1_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK:             [[VAR_270_1_:%.+]] = stablehlo.select [[VAR_268_1_]], [[VAR_269_1_]], [[VAR_259_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_271_1_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_270_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_272_1_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_267_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_273_1_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_261_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_274_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_262_1_]], [[VAR_271_1_]], [[VAR_272_1_]], [[VAR_273_1_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_275_1_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_276_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_277_1_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:             [[VAR_278_1_:%.+]] = stablehlo.compare  LT, [[VAR_276_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_279_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_278_1_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:         [[VAR_280_1_:%.+]] = stablehlo.negate [[VAR_276_1_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_281_1_:%.+]] = stablehlo.add [[VAR_277_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_282_1_:%.+]] = stablehlo.add [[VAR_275_1_]], [[VAR_21_]] : tensor<1xi64>
+// CHECK-DAG:         [[VAR_283_1_:%.+]] = stablehlo.reverse [[VAR_199_1_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_284_1_:%.+]] = stablehlo.select [[VAR_278_1_]], [[VAR_281_1_]], [[VAR_275_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_285_1_:%.+]] = stablehlo.select [[VAR_278_1_]], [[VAR_282_1_]], [[VAR_277_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_286_1_:%.+]] = stablehlo.select [[VAR_278_1_]], [[VAR_280_1_]], [[VAR_276_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_287_1_:%.+]] = stablehlo.select [[VAR_279_1_]], [[VAR_283_1_]], [[VAR_199_1_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:             [[VAR_288_1_:%.+]] = stablehlo.compare  GT, [[VAR_285_1_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_289_1_:%.+]] = stablehlo.select [[VAR_288_1_]], [[VAR_10_]], [[VAR_285_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_290_1_:%.+]] = stablehlo.compare  LT, [[VAR_289_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_291_1_:%.+]] = stablehlo.add [[VAR_289_1_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_292_1_:%.+]] = stablehlo.select [[VAR_290_1_]], [[VAR_291_1_]], [[VAR_289_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_293_1_:%.+]] = stablehlo.compare  LT, [[VAR_284_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:         [[VAR_294_1_:%.+]] = stablehlo.add [[VAR_284_1_]], [[VAR_10_]] : tensor<1xi64>
+// CHECK:             [[VAR_295_1_:%.+]] = stablehlo.select [[VAR_293_1_]], [[VAR_294_1_]], [[VAR_284_1_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:         [[VAR_296_1_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_295_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_297_1_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_292_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:         [[VAR_298_1_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_286_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_299_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_287_1_]], [[VAR_296_1_]], [[VAR_297_1_]], [[VAR_298_1_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_300_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_224_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_301_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_151_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_302_1_:%.+]] = stablehlo.add [[VAR_300_1_]], [[VAR_301_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_303_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_302_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_304_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_155_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_305_1_:%.+]] = stablehlo.add [[VAR_303_1_]], [[VAR_304_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_306_1_:%.+]] = stablehlo.logistic [[VAR_305_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_307_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_274_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_308_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_153_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_309_1_:%.+]] = stablehlo.add [[VAR_307_1_]], [[VAR_308_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_310_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_309_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_311_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_157_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_312_1_:%.+]] = stablehlo.add [[VAR_310_1_]], [[VAR_311_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_313_1_:%.+]] = stablehlo.logistic [[VAR_312_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_314_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_299_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_315_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_154_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_316_1_:%.+]] = stablehlo.add [[VAR_314_1_]], [[VAR_315_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_317_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_316_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_318_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_158_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_319_1_:%.+]] = stablehlo.add [[VAR_317_1_]], [[VAR_318_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_320_1_:%.+]] = stablehlo.tanh [[VAR_319_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_321_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_313_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_322_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_2_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_323_1_:%.+]] = stablehlo.multiply [[VAR_321_1_]], [[VAR_322_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_324_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_306_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_325_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_320_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_326_1_:%.+]] = stablehlo.multiply [[VAR_324_1_]], [[VAR_325_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_327_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_323_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_328_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_326_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_329_1_:%.+]] = stablehlo.add [[VAR_327_1_]], [[VAR_328_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_330_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_249_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_331_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_152_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_332_1_:%.+]] = stablehlo.add [[VAR_330_1_]], [[VAR_331_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_333_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_332_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_334_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_156_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_335_1_:%.+]] = stablehlo.add [[VAR_333_1_]], [[VAR_334_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_336_1_:%.+]] = stablehlo.logistic [[VAR_335_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_337_1_:%.+]] = stablehlo.tanh [[VAR_329_1_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_338_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_336_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_339_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_337_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_340_1_:%.+]] = stablehlo.multiply [[VAR_338_1_]], [[VAR_339_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_341_1_:%.+]] = stablehlo.dynamic_reshape [[VAR_340_1_]], [[VAR_2_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:         [[VAR_342_1_:%.+]] = stablehlo.dynamic_reshape [[VAR_iterArg_1_]], [[VAR_1_]] : (tensor<1xi64>, tensor<2xindex>) -> tensor<1x1xi64>
+// CHECK:             [[VAR_343_1_:%.+]] = "stablehlo.scatter"([[VAR_iterArg_0_1_]], [[VAR_342_1_]], [[VAR_341_1_]]) ({
+// CHECK:             ^bb0([[arg4_1_:%.+]]: tensor<f32>, [[arg5_1_:%.+]]: tensor<f32>):
+// CHECK:               stablehlo.return [[arg5_1_]] : tensor<f32>
+// CHECK:             }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2, 3], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<128x1x16x256xf32>, tensor<1x1xi64>, tensor<1x1x16x256xf32>) -> tensor<128x1x16x256xf32>
+// CHECK-DAG:         [[VAR_344_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_1_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_345_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_21_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK:             [[VAR_346_1_:%.+]] = stablehlo.subtract [[VAR_344_1_]], [[VAR_345_1_]] : tensor<1xi64>
+// CHECK:             stablehlo.return [[VAR_346_1_]], [[VAR_343_1_]], [[VAR_340_1_]], [[VAR_329_1_]] : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK:           }
+// CHECK:           [[VAR_161_:%.+]] = stablehlo.concatenate [[VAR_159_]]#1, [[VAR_160_]]#1, dim = 1 : (tensor<128x1x16x256xf32>, tensor<128x1x16x256xf32>) -> tensor<128x2x16x256xf32>
+// CHECK:           return [[VAR_161_]] : tensor<128x2x16x256xf32>
+// CHECK:         }
+}
\ No newline at end of file
diff --git a/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM.mlir b/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM.mlir
new file mode 100644
index 0000000000..4001df8765
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM.mlir
@@ -0,0 +1,991 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-stablehlo --canonicalize -split-input-file %s | FileCheck %s
+func.func @test_lstm(%arg0 : tensor<2x16x512xf32>, %arg1 : tensor<2x2048xf32>, %arg2 : tensor<2x1024x512xf32>, %arg3 : tensor<2x1024x256xf32>) -> tensor<2x2x16x256xf32> {
+  %0 = onnx.Constant dense<0.000000e+00> : tensor<2x16x256xf32>
+  %1 = "onnx.NoValue"() {value} : () -> none
+  %Y, %Y_h, %Y_c = "onnx.LSTM"(%arg0, %arg2, %arg3, %arg1, %1, %0, %0, %1) {direction = "bidirectional", hidden_size = 256 : si64, input_forget = 0 : si64, layout = 0 : si64} : (tensor<2x16x512xf32>, tensor<2x1024x512xf32>, tensor<2x1024x256xf32>, tensor<2x2048xf32>, none, tensor<2x16x256xf32>, tensor<2x16x256xf32>, none) -> (tensor<2x2x16x256xf32>, tensor<2x16x256xf32>, tensor<2x16x256xf32>)
+  return %Y : tensor<2x2x16x256xf32>
+// CHECK-LABEL:  func.func @test_lstm
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<2x16x512xf32>, [[PARAM_1_:%.+]]: tensor<2x2048xf32>, [[PARAM_2_:%.+]]: tensor<2x1024x512xf32>, [[PARAM_3_:%.+]]: tensor<2x1024x256xf32>) -> tensor<2x2x16x256xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = shape.const_shape [16, 256] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_1_:%.+]] = shape.const_shape [1, 1, 16, 256] : tensor<4xindex>
+// CHECK-DAG:       [[VAR_2_:%.+]] = shape.const_shape [16, 1024] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_3_:%.+]] = shape.const_shape [16, 512] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_4_:%.+]] = shape.const_shape [2, 16, 512] : tensor<3xindex>
+// CHECK-DAG:       [[VAR_5_:%.+]] = shape.const_shape [2048] : tensor<1xindex>
+// CHECK-DAG:       [[VAR_6_:%.+]] = shape.const_shape [1024, 256] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_7_:%.+]] = shape.const_shape [1024, 512] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_8_:%.+]] = shape.const_shape [2, 16, 256] : tensor<3xindex>
+// CHECK-DAG:       [[VAR_9_:%.+]] = stablehlo.constant dense<1024> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_10_:%.+]] = stablehlo.constant dense<768> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_11_:%.+]] = stablehlo.constant dense<512> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_12_:%.+]] = shape.const_shape [1] : tensor<1xindex>
+// CHECK-DAG:       [[VAR_13_:%.+]] = stablehlo.constant dense<256> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_14_:%.+]] = stablehlo.constant dense<16> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_15_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<2x16x256xf32>
+// CHECK-DAG:       [[VAR_16_:%.+]] = stablehlo.constant dense<0> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_17_:%.+]] = stablehlo.constant dense<1> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_18_:%.+]] = stablehlo.constant dense<2> : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_19_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_20_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_21_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_22_:%.+]] = stablehlo.compare  LT, [[VAR_20_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_23_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_22_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
+// CHECK-DAG:       [[VAR_24_:%.+]] = stablehlo.negate [[VAR_20_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_25_:%.+]] = stablehlo.add [[VAR_21_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_26_:%.+]] = stablehlo.add [[VAR_19_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_27_:%.+]] = stablehlo.reverse [[VAR_15_]], dims = [0] : tensor<2x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_28_:%.+]] = stablehlo.select [[VAR_22_]], [[VAR_25_]], [[VAR_19_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_29_:%.+]] = stablehlo.select [[VAR_22_]], [[VAR_26_]], [[VAR_21_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_30_:%.+]] = stablehlo.select [[VAR_22_]], [[VAR_24_]], [[VAR_20_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_31_:%.+]] = stablehlo.select [[VAR_23_]], [[VAR_27_]], [[VAR_15_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
+// CHECK:           [[VAR_32_:%.+]] = stablehlo.compare  GT, [[VAR_29_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_33_:%.+]] = stablehlo.select [[VAR_32_]], [[VAR_18_]], [[VAR_29_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_34_:%.+]] = stablehlo.compare  LT, [[VAR_33_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_35_:%.+]] = stablehlo.add [[VAR_33_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_36_:%.+]] = stablehlo.select [[VAR_34_]], [[VAR_35_]], [[VAR_33_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_37_:%.+]] = stablehlo.compare  LT, [[VAR_28_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_38_:%.+]] = stablehlo.add [[VAR_28_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK:           [[VAR_39_:%.+]] = stablehlo.select [[VAR_37_]], [[VAR_38_]], [[VAR_28_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_40_:%.+]] = stablehlo.concatenate [[VAR_39_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_41_:%.+]] = stablehlo.concatenate [[VAR_36_]], [[VAR_14_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_42_:%.+]] = stablehlo.concatenate [[VAR_30_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_43_:%.+]] = stablehlo.real_dynamic_slice [[VAR_31_]], [[VAR_40_]], [[VAR_41_]], [[VAR_42_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_44_:%.+]] = stablehlo.dynamic_reshape [[VAR_43_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_45_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_46_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_47_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_48_:%.+]] = stablehlo.compare  LT, [[VAR_46_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_49_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_48_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
+// CHECK-DAG:       [[VAR_50_:%.+]] = stablehlo.negate [[VAR_46_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_51_:%.+]] = stablehlo.add [[VAR_47_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_52_:%.+]] = stablehlo.add [[VAR_45_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_53_:%.+]] = stablehlo.reverse [[VAR_15_]], dims = [0] : tensor<2x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_54_:%.+]] = stablehlo.select [[VAR_48_]], [[VAR_51_]], [[VAR_45_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_55_:%.+]] = stablehlo.select [[VAR_48_]], [[VAR_52_]], [[VAR_47_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_56_:%.+]] = stablehlo.select [[VAR_48_]], [[VAR_50_]], [[VAR_46_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_57_:%.+]] = stablehlo.select [[VAR_49_]], [[VAR_53_]], [[VAR_15_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
+// CHECK:           [[VAR_58_:%.+]] = stablehlo.compare  GT, [[VAR_55_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_59_:%.+]] = stablehlo.select [[VAR_58_]], [[VAR_18_]], [[VAR_55_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_60_:%.+]] = stablehlo.compare  LT, [[VAR_59_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_61_:%.+]] = stablehlo.add [[VAR_59_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_62_:%.+]] = stablehlo.select [[VAR_60_]], [[VAR_61_]], [[VAR_59_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_63_:%.+]] = stablehlo.compare  LT, [[VAR_54_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_64_:%.+]] = stablehlo.add [[VAR_54_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK:           [[VAR_65_:%.+]] = stablehlo.select [[VAR_63_]], [[VAR_64_]], [[VAR_54_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_66_:%.+]] = stablehlo.concatenate [[VAR_65_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_67_:%.+]] = stablehlo.concatenate [[VAR_62_]], [[VAR_14_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_68_:%.+]] = stablehlo.concatenate [[VAR_56_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_69_:%.+]] = stablehlo.real_dynamic_slice [[VAR_57_]], [[VAR_66_]], [[VAR_67_]], [[VAR_68_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_70_:%.+]] = stablehlo.dynamic_reshape [[VAR_69_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_71_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_72_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_73_:%.+]] = stablehlo.slice [[VAR_18_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_74_:%.+]] = stablehlo.compare  LT, [[VAR_72_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_75_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_74_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
+// CHECK-DAG:       [[VAR_76_:%.+]] = stablehlo.negate [[VAR_72_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_77_:%.+]] = stablehlo.add [[VAR_73_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_78_:%.+]] = stablehlo.add [[VAR_71_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_79_:%.+]] = stablehlo.reverse [[VAR_15_]], dims = [0] : tensor<2x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_80_:%.+]] = stablehlo.select [[VAR_74_]], [[VAR_77_]], [[VAR_71_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_81_:%.+]] = stablehlo.select [[VAR_74_]], [[VAR_78_]], [[VAR_73_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_82_:%.+]] = stablehlo.select [[VAR_74_]], [[VAR_76_]], [[VAR_72_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_83_:%.+]] = stablehlo.select [[VAR_75_]], [[VAR_79_]], [[VAR_15_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
+// CHECK:           [[VAR_84_:%.+]] = stablehlo.compare  GT, [[VAR_81_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_85_:%.+]] = stablehlo.select [[VAR_84_]], [[VAR_18_]], [[VAR_81_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_86_:%.+]] = stablehlo.compare  LT, [[VAR_85_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_87_:%.+]] = stablehlo.add [[VAR_85_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_88_:%.+]] = stablehlo.select [[VAR_86_]], [[VAR_87_]], [[VAR_85_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_89_:%.+]] = stablehlo.compare  LT, [[VAR_80_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_90_:%.+]] = stablehlo.add [[VAR_80_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK:           [[VAR_91_:%.+]] = stablehlo.select [[VAR_89_]], [[VAR_90_]], [[VAR_80_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_92_:%.+]] = stablehlo.concatenate [[VAR_91_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_93_:%.+]] = stablehlo.concatenate [[VAR_88_]], [[VAR_14_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_94_:%.+]] = stablehlo.concatenate [[VAR_82_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_95_:%.+]] = stablehlo.real_dynamic_slice [[VAR_83_]], [[VAR_92_]], [[VAR_93_]], [[VAR_94_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_96_:%.+]] = stablehlo.dynamic_reshape [[VAR_95_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_97_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_98_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_99_:%.+]] = stablehlo.slice [[VAR_18_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_100_:%.+]] = stablehlo.compare  LT, [[VAR_98_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_101_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_100_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
+// CHECK-DAG:       [[VAR_102_:%.+]] = stablehlo.negate [[VAR_98_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_103_:%.+]] = stablehlo.add [[VAR_99_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_104_:%.+]] = stablehlo.add [[VAR_97_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_105_:%.+]] = stablehlo.reverse [[VAR_15_]], dims = [0] : tensor<2x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_106_:%.+]] = stablehlo.select [[VAR_100_]], [[VAR_103_]], [[VAR_97_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_107_:%.+]] = stablehlo.select [[VAR_100_]], [[VAR_104_]], [[VAR_99_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_108_:%.+]] = stablehlo.select [[VAR_100_]], [[VAR_102_]], [[VAR_98_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_109_:%.+]] = stablehlo.select [[VAR_101_]], [[VAR_105_]], [[VAR_15_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
+// CHECK:           [[VAR_110_:%.+]] = stablehlo.compare  GT, [[VAR_107_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_111_:%.+]] = stablehlo.select [[VAR_110_]], [[VAR_18_]], [[VAR_107_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_112_:%.+]] = stablehlo.compare  LT, [[VAR_111_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_113_:%.+]] = stablehlo.add [[VAR_111_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_114_:%.+]] = stablehlo.select [[VAR_112_]], [[VAR_113_]], [[VAR_111_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_115_:%.+]] = stablehlo.compare  LT, [[VAR_106_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_116_:%.+]] = stablehlo.add [[VAR_106_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK:           [[VAR_117_:%.+]] = stablehlo.select [[VAR_115_]], [[VAR_116_]], [[VAR_106_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_118_:%.+]] = stablehlo.concatenate [[VAR_117_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_119_:%.+]] = stablehlo.concatenate [[VAR_114_]], [[VAR_14_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_120_:%.+]] = stablehlo.concatenate [[VAR_108_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_121_:%.+]] = stablehlo.real_dynamic_slice [[VAR_109_]], [[VAR_118_]], [[VAR_119_]], [[VAR_120_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_122_:%.+]] = stablehlo.dynamic_reshape [[VAR_121_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_123_:%.+]] = stablehlo.slice [[PARAM_2_]] [0:1, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-DAG:       [[VAR_124_:%.+]] = stablehlo.slice [[PARAM_2_]] [1:2, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_125_:%.+]] = stablehlo.dynamic_reshape [[VAR_123_]], [[VAR_7_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_126_:%.+]] = stablehlo.dynamic_reshape [[VAR_124_]], [[VAR_7_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_127_:%.+]] = stablehlo.slice [[PARAM_3_]] [0:1, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-DAG:       [[VAR_128_:%.+]] = stablehlo.slice [[PARAM_3_]] [1:2, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_129_:%.+]] = stablehlo.dynamic_reshape [[VAR_127_]], [[VAR_6_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_130_:%.+]] = stablehlo.dynamic_reshape [[VAR_128_]], [[VAR_6_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_131_:%.+]] = stablehlo.transpose [[VAR_125_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_132_:%.+]] = stablehlo.transpose [[VAR_129_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_133_:%.+]] = stablehlo.transpose [[VAR_126_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-DAG:       [[VAR_134_:%.+]] = stablehlo.transpose [[VAR_130_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_135_:%.+]] = stablehlo.slice [[PARAM_1_]] [0:1, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-DAG:       [[VAR_136_:%.+]] = stablehlo.slice [[PARAM_1_]] [1:2, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_137_:%.+]] = stablehlo.dynamic_reshape [[VAR_135_]], [[VAR_5_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
+// CHECK-DAG:       [[VAR_138_:%.+]] = stablehlo.dynamic_reshape [[VAR_136_]], [[VAR_5_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_139_:%.+]] = stablehlo.slice [[VAR_137_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_140_:%.+]] = stablehlo.slice [[VAR_137_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_141_:%.+]] = stablehlo.slice [[VAR_137_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_142_:%.+]] = stablehlo.slice [[VAR_137_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_143_:%.+]] = stablehlo.slice [[VAR_137_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_144_:%.+]] = stablehlo.slice [[VAR_137_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_145_:%.+]] = stablehlo.slice [[VAR_137_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_146_:%.+]] = stablehlo.slice [[VAR_137_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_147_:%.+]] = stablehlo.slice [[VAR_138_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_148_:%.+]] = stablehlo.slice [[VAR_138_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_149_:%.+]] = stablehlo.slice [[VAR_138_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_150_:%.+]] = stablehlo.slice [[VAR_138_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_151_:%.+]] = stablehlo.slice [[VAR_138_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_152_:%.+]] = stablehlo.slice [[VAR_138_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_153_:%.+]] = stablehlo.slice [[VAR_138_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_154_:%.+]] = stablehlo.slice [[VAR_138_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_155_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_16_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_156_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_157_:%.+]] = stablehlo.add [[VAR_155_]], [[VAR_156_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_158_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_159_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_160_:%.+]] = stablehlo.slice [[VAR_157_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_161_:%.+]] = stablehlo.compare  LT, [[VAR_159_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_162_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_161_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x512xi1>
+// CHECK-DAG:       [[VAR_163_:%.+]] = stablehlo.negate [[VAR_159_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_164_:%.+]] = stablehlo.add [[VAR_160_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_165_:%.+]] = stablehlo.add [[VAR_158_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_166_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<2x16x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_167_:%.+]] = stablehlo.select [[VAR_161_]], [[VAR_164_]], [[VAR_158_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_168_:%.+]] = stablehlo.select [[VAR_161_]], [[VAR_165_]], [[VAR_160_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_169_:%.+]] = stablehlo.select [[VAR_161_]], [[VAR_163_]], [[VAR_159_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_170_:%.+]] = stablehlo.select [[VAR_162_]], [[VAR_166_]], [[PARAM_0_]] : tensor<2x16x512xi1>, tensor<2x16x512xf32>
+// CHECK:           [[VAR_171_:%.+]] = stablehlo.compare  GT, [[VAR_168_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_172_:%.+]] = stablehlo.select [[VAR_171_]], [[VAR_18_]], [[VAR_168_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_173_:%.+]] = stablehlo.compare  LT, [[VAR_172_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_174_:%.+]] = stablehlo.add [[VAR_172_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_175_:%.+]] = stablehlo.select [[VAR_173_]], [[VAR_174_]], [[VAR_172_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_176_:%.+]] = stablehlo.compare  LT, [[VAR_167_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_177_:%.+]] = stablehlo.add [[VAR_167_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK:           [[VAR_178_:%.+]] = stablehlo.select [[VAR_176_]], [[VAR_177_]], [[VAR_167_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_179_:%.+]] = stablehlo.concatenate [[VAR_178_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_180_:%.+]] = stablehlo.concatenate [[VAR_175_]], [[VAR_14_]], [[VAR_11_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_181_:%.+]] = stablehlo.concatenate [[VAR_169_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_182_:%.+]] = stablehlo.real_dynamic_slice [[VAR_170_]], [[VAR_179_]], [[VAR_180_]], [[VAR_181_]] : (tensor<2x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_183_:%.+]] = stablehlo.dynamic_reshape [[VAR_182_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_184_:%.+]] = stablehlo.broadcast_in_dim [[VAR_183_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_185_:%.+]] = stablehlo.broadcast_in_dim [[VAR_131_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_186_:%.+]] = stablehlo.dot [[VAR_184_]], [[VAR_185_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_187_:%.+]] = stablehlo.broadcast_in_dim [[VAR_44_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_188_:%.+]] = stablehlo.broadcast_in_dim [[VAR_132_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_189_:%.+]] = stablehlo.dot [[VAR_187_]], [[VAR_188_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_190_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_186_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_191_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_189_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_192_:%.+]] = stablehlo.add [[VAR_190_]], [[VAR_191_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_193_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_194_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_195_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_196_:%.+]] = stablehlo.compare  LT, [[VAR_194_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_197_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_196_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_198_:%.+]] = stablehlo.negate [[VAR_194_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_199_:%.+]] = stablehlo.add [[VAR_195_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_200_:%.+]] = stablehlo.add [[VAR_193_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_201_:%.+]] = stablehlo.reverse [[VAR_192_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_202_:%.+]] = stablehlo.select [[VAR_196_]], [[VAR_199_]], [[VAR_193_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_203_:%.+]] = stablehlo.select [[VAR_196_]], [[VAR_200_]], [[VAR_195_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_204_:%.+]] = stablehlo.select [[VAR_196_]], [[VAR_198_]], [[VAR_194_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_205_:%.+]] = stablehlo.select [[VAR_197_]], [[VAR_201_]], [[VAR_192_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_206_:%.+]] = stablehlo.compare  GT, [[VAR_203_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_207_:%.+]] = stablehlo.select [[VAR_206_]], [[VAR_9_]], [[VAR_203_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_208_:%.+]] = stablehlo.compare  LT, [[VAR_207_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_209_:%.+]] = stablehlo.add [[VAR_207_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_210_:%.+]] = stablehlo.select [[VAR_208_]], [[VAR_209_]], [[VAR_207_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_211_:%.+]] = stablehlo.compare  LT, [[VAR_202_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_212_:%.+]] = stablehlo.add [[VAR_202_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_213_:%.+]] = stablehlo.select [[VAR_211_]], [[VAR_212_]], [[VAR_202_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_214_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_213_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_215_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_210_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_216_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_204_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_217_:%.+]] = stablehlo.real_dynamic_slice [[VAR_205_]], [[VAR_214_]], [[VAR_215_]], [[VAR_216_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_218_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_219_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_220_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_221_:%.+]] = stablehlo.compare  LT, [[VAR_219_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_222_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_221_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_223_:%.+]] = stablehlo.negate [[VAR_219_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_224_:%.+]] = stablehlo.add [[VAR_220_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_225_:%.+]] = stablehlo.add [[VAR_218_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_226_:%.+]] = stablehlo.reverse [[VAR_192_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_227_:%.+]] = stablehlo.select [[VAR_221_]], [[VAR_224_]], [[VAR_218_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_228_:%.+]] = stablehlo.select [[VAR_221_]], [[VAR_225_]], [[VAR_220_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_229_:%.+]] = stablehlo.select [[VAR_221_]], [[VAR_223_]], [[VAR_219_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_230_:%.+]] = stablehlo.select [[VAR_222_]], [[VAR_226_]], [[VAR_192_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_231_:%.+]] = stablehlo.compare  GT, [[VAR_228_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_232_:%.+]] = stablehlo.select [[VAR_231_]], [[VAR_9_]], [[VAR_228_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_233_:%.+]] = stablehlo.compare  LT, [[VAR_232_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_234_:%.+]] = stablehlo.add [[VAR_232_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_235_:%.+]] = stablehlo.select [[VAR_233_]], [[VAR_234_]], [[VAR_232_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_236_:%.+]] = stablehlo.compare  LT, [[VAR_227_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_237_:%.+]] = stablehlo.add [[VAR_227_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_238_:%.+]] = stablehlo.select [[VAR_236_]], [[VAR_237_]], [[VAR_227_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_239_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_238_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_240_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_235_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_241_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_229_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_242_:%.+]] = stablehlo.real_dynamic_slice [[VAR_230_]], [[VAR_239_]], [[VAR_240_]], [[VAR_241_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_243_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_244_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_245_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_246_:%.+]] = stablehlo.compare  LT, [[VAR_244_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_247_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_246_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_248_:%.+]] = stablehlo.negate [[VAR_244_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_249_:%.+]] = stablehlo.add [[VAR_245_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_250_:%.+]] = stablehlo.add [[VAR_243_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_251_:%.+]] = stablehlo.reverse [[VAR_192_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_252_:%.+]] = stablehlo.select [[VAR_246_]], [[VAR_249_]], [[VAR_243_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_253_:%.+]] = stablehlo.select [[VAR_246_]], [[VAR_250_]], [[VAR_245_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_254_:%.+]] = stablehlo.select [[VAR_246_]], [[VAR_248_]], [[VAR_244_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_255_:%.+]] = stablehlo.select [[VAR_247_]], [[VAR_251_]], [[VAR_192_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_256_:%.+]] = stablehlo.compare  GT, [[VAR_253_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_257_:%.+]] = stablehlo.select [[VAR_256_]], [[VAR_9_]], [[VAR_253_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_258_:%.+]] = stablehlo.compare  LT, [[VAR_257_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_259_:%.+]] = stablehlo.add [[VAR_257_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_260_:%.+]] = stablehlo.select [[VAR_258_]], [[VAR_259_]], [[VAR_257_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_261_:%.+]] = stablehlo.compare  LT, [[VAR_252_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_262_:%.+]] = stablehlo.add [[VAR_252_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_263_:%.+]] = stablehlo.select [[VAR_261_]], [[VAR_262_]], [[VAR_252_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_264_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_263_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_265_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_260_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_266_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_254_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_267_:%.+]] = stablehlo.real_dynamic_slice [[VAR_255_]], [[VAR_264_]], [[VAR_265_]], [[VAR_266_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_268_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_269_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_270_:%.+]] = stablehlo.slice [[VAR_9_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_271_:%.+]] = stablehlo.compare  LT, [[VAR_269_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_272_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_271_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_273_:%.+]] = stablehlo.negate [[VAR_269_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_274_:%.+]] = stablehlo.add [[VAR_270_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_275_:%.+]] = stablehlo.add [[VAR_268_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_276_:%.+]] = stablehlo.reverse [[VAR_192_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_277_:%.+]] = stablehlo.select [[VAR_271_]], [[VAR_274_]], [[VAR_268_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_278_:%.+]] = stablehlo.select [[VAR_271_]], [[VAR_275_]], [[VAR_270_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_279_:%.+]] = stablehlo.select [[VAR_271_]], [[VAR_273_]], [[VAR_269_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_280_:%.+]] = stablehlo.select [[VAR_272_]], [[VAR_276_]], [[VAR_192_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_281_:%.+]] = stablehlo.compare  GT, [[VAR_278_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_282_:%.+]] = stablehlo.select [[VAR_281_]], [[VAR_9_]], [[VAR_278_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_283_:%.+]] = stablehlo.compare  LT, [[VAR_282_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_284_:%.+]] = stablehlo.add [[VAR_282_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_285_:%.+]] = stablehlo.select [[VAR_283_]], [[VAR_284_]], [[VAR_282_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_286_:%.+]] = stablehlo.compare  LT, [[VAR_277_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_287_:%.+]] = stablehlo.add [[VAR_277_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_288_:%.+]] = stablehlo.select [[VAR_286_]], [[VAR_287_]], [[VAR_277_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_289_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_288_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_290_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_285_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_291_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_279_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_292_:%.+]] = stablehlo.real_dynamic_slice [[VAR_280_]], [[VAR_289_]], [[VAR_290_]], [[VAR_291_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_293_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_217_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_294_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_139_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_295_:%.+]] = stablehlo.add [[VAR_293_]], [[VAR_294_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_296_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_295_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_297_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_143_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_298_:%.+]] = stablehlo.add [[VAR_296_]], [[VAR_297_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_299_:%.+]] = stablehlo.logistic [[VAR_298_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_300_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_267_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_301_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_141_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_302_:%.+]] = stablehlo.add [[VAR_300_]], [[VAR_301_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_303_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_302_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_304_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_145_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_305_:%.+]] = stablehlo.add [[VAR_303_]], [[VAR_304_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_306_:%.+]] = stablehlo.logistic [[VAR_305_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_307_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_292_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_308_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_142_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_309_:%.+]] = stablehlo.add [[VAR_307_]], [[VAR_308_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_310_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_309_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_311_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_146_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_312_:%.+]] = stablehlo.add [[VAR_310_]], [[VAR_311_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_313_:%.+]] = stablehlo.tanh [[VAR_312_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_314_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_306_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_315_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_70_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_316_:%.+]] = stablehlo.multiply [[VAR_314_]], [[VAR_315_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_317_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_299_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_318_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_313_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_319_:%.+]] = stablehlo.multiply [[VAR_317_]], [[VAR_318_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_320_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_316_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_321_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_319_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_322_:%.+]] = stablehlo.add [[VAR_320_]], [[VAR_321_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_323_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_242_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_324_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_140_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_325_:%.+]] = stablehlo.add [[VAR_323_]], [[VAR_324_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_326_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_325_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_327_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_144_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_328_:%.+]] = stablehlo.add [[VAR_326_]], [[VAR_327_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_329_:%.+]] = stablehlo.logistic [[VAR_328_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_330_:%.+]] = stablehlo.tanh [[VAR_322_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_331_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_329_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_332_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_330_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_333_:%.+]] = stablehlo.multiply [[VAR_331_]], [[VAR_332_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_334_:%.+]] = stablehlo.dynamic_reshape [[VAR_333_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_335_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_336_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_337_:%.+]] = stablehlo.add [[VAR_335_]], [[VAR_336_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_338_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_339_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_340_:%.+]] = stablehlo.slice [[VAR_337_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_341_:%.+]] = stablehlo.compare  LT, [[VAR_339_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_342_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_341_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x512xi1>
+// CHECK-DAG:       [[VAR_343_:%.+]] = stablehlo.negate [[VAR_339_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_344_:%.+]] = stablehlo.add [[VAR_340_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_345_:%.+]] = stablehlo.add [[VAR_338_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_346_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<2x16x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_347_:%.+]] = stablehlo.select [[VAR_341_]], [[VAR_344_]], [[VAR_338_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_348_:%.+]] = stablehlo.select [[VAR_341_]], [[VAR_345_]], [[VAR_340_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_349_:%.+]] = stablehlo.select [[VAR_341_]], [[VAR_343_]], [[VAR_339_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_350_:%.+]] = stablehlo.select [[VAR_342_]], [[VAR_346_]], [[PARAM_0_]] : tensor<2x16x512xi1>, tensor<2x16x512xf32>
+// CHECK:           [[VAR_351_:%.+]] = stablehlo.compare  GT, [[VAR_348_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_352_:%.+]] = stablehlo.select [[VAR_351_]], [[VAR_18_]], [[VAR_348_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_353_:%.+]] = stablehlo.compare  LT, [[VAR_352_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_354_:%.+]] = stablehlo.add [[VAR_352_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_355_:%.+]] = stablehlo.select [[VAR_353_]], [[VAR_354_]], [[VAR_352_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_356_:%.+]] = stablehlo.compare  LT, [[VAR_347_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_357_:%.+]] = stablehlo.add [[VAR_347_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK:           [[VAR_358_:%.+]] = stablehlo.select [[VAR_356_]], [[VAR_357_]], [[VAR_347_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_359_:%.+]] = stablehlo.concatenate [[VAR_358_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_360_:%.+]] = stablehlo.concatenate [[VAR_355_]], [[VAR_14_]], [[VAR_11_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_361_:%.+]] = stablehlo.concatenate [[VAR_349_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_362_:%.+]] = stablehlo.real_dynamic_slice [[VAR_350_]], [[VAR_359_]], [[VAR_360_]], [[VAR_361_]] : (tensor<2x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_363_:%.+]] = stablehlo.dynamic_reshape [[VAR_362_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_364_:%.+]] = stablehlo.broadcast_in_dim [[VAR_363_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_365_:%.+]] = stablehlo.broadcast_in_dim [[VAR_131_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_366_:%.+]] = stablehlo.dot [[VAR_364_]], [[VAR_365_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_367_:%.+]] = stablehlo.broadcast_in_dim [[VAR_333_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_368_:%.+]] = stablehlo.broadcast_in_dim [[VAR_132_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_369_:%.+]] = stablehlo.dot [[VAR_367_]], [[VAR_368_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_370_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_366_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_371_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_369_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_372_:%.+]] = stablehlo.add [[VAR_370_]], [[VAR_371_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_373_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_374_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_375_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_376_:%.+]] = stablehlo.compare  LT, [[VAR_374_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_377_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_376_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_378_:%.+]] = stablehlo.negate [[VAR_374_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_379_:%.+]] = stablehlo.add [[VAR_375_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_380_:%.+]] = stablehlo.add [[VAR_373_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_381_:%.+]] = stablehlo.reverse [[VAR_372_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_382_:%.+]] = stablehlo.select [[VAR_376_]], [[VAR_379_]], [[VAR_373_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_383_:%.+]] = stablehlo.select [[VAR_376_]], [[VAR_380_]], [[VAR_375_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_384_:%.+]] = stablehlo.select [[VAR_376_]], [[VAR_378_]], [[VAR_374_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_385_:%.+]] = stablehlo.select [[VAR_377_]], [[VAR_381_]], [[VAR_372_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_386_:%.+]] = stablehlo.compare  GT, [[VAR_383_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_387_:%.+]] = stablehlo.select [[VAR_386_]], [[VAR_9_]], [[VAR_383_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_388_:%.+]] = stablehlo.compare  LT, [[VAR_387_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_389_:%.+]] = stablehlo.add [[VAR_387_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_390_:%.+]] = stablehlo.select [[VAR_388_]], [[VAR_389_]], [[VAR_387_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_391_:%.+]] = stablehlo.compare  LT, [[VAR_382_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_392_:%.+]] = stablehlo.add [[VAR_382_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_393_:%.+]] = stablehlo.select [[VAR_391_]], [[VAR_392_]], [[VAR_382_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_394_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_393_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_395_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_390_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_396_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_384_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_397_:%.+]] = stablehlo.real_dynamic_slice [[VAR_385_]], [[VAR_394_]], [[VAR_395_]], [[VAR_396_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_398_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_399_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_400_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_401_:%.+]] = stablehlo.compare  LT, [[VAR_399_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_402_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_401_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_403_:%.+]] = stablehlo.negate [[VAR_399_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_404_:%.+]] = stablehlo.add [[VAR_400_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_405_:%.+]] = stablehlo.add [[VAR_398_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_406_:%.+]] = stablehlo.reverse [[VAR_372_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_407_:%.+]] = stablehlo.select [[VAR_401_]], [[VAR_404_]], [[VAR_398_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_408_:%.+]] = stablehlo.select [[VAR_401_]], [[VAR_405_]], [[VAR_400_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_409_:%.+]] = stablehlo.select [[VAR_401_]], [[VAR_403_]], [[VAR_399_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_410_:%.+]] = stablehlo.select [[VAR_402_]], [[VAR_406_]], [[VAR_372_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_411_:%.+]] = stablehlo.compare  GT, [[VAR_408_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_412_:%.+]] = stablehlo.select [[VAR_411_]], [[VAR_9_]], [[VAR_408_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_413_:%.+]] = stablehlo.compare  LT, [[VAR_412_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_414_:%.+]] = stablehlo.add [[VAR_412_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_415_:%.+]] = stablehlo.select [[VAR_413_]], [[VAR_414_]], [[VAR_412_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_416_:%.+]] = stablehlo.compare  LT, [[VAR_407_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_417_:%.+]] = stablehlo.add [[VAR_407_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_418_:%.+]] = stablehlo.select [[VAR_416_]], [[VAR_417_]], [[VAR_407_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_419_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_418_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_420_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_415_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_421_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_409_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_422_:%.+]] = stablehlo.real_dynamic_slice [[VAR_410_]], [[VAR_419_]], [[VAR_420_]], [[VAR_421_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_423_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_424_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_425_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_426_:%.+]] = stablehlo.compare  LT, [[VAR_424_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_427_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_426_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_428_:%.+]] = stablehlo.negate [[VAR_424_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_429_:%.+]] = stablehlo.add [[VAR_425_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_430_:%.+]] = stablehlo.add [[VAR_423_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_431_:%.+]] = stablehlo.reverse [[VAR_372_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_432_:%.+]] = stablehlo.select [[VAR_426_]], [[VAR_429_]], [[VAR_423_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_433_:%.+]] = stablehlo.select [[VAR_426_]], [[VAR_430_]], [[VAR_425_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_434_:%.+]] = stablehlo.select [[VAR_426_]], [[VAR_428_]], [[VAR_424_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_435_:%.+]] = stablehlo.select [[VAR_427_]], [[VAR_431_]], [[VAR_372_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_436_:%.+]] = stablehlo.compare  GT, [[VAR_433_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_437_:%.+]] = stablehlo.select [[VAR_436_]], [[VAR_9_]], [[VAR_433_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_438_:%.+]] = stablehlo.compare  LT, [[VAR_437_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_439_:%.+]] = stablehlo.add [[VAR_437_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_440_:%.+]] = stablehlo.select [[VAR_438_]], [[VAR_439_]], [[VAR_437_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_441_:%.+]] = stablehlo.compare  LT, [[VAR_432_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_442_:%.+]] = stablehlo.add [[VAR_432_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_443_:%.+]] = stablehlo.select [[VAR_441_]], [[VAR_442_]], [[VAR_432_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_444_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_443_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_445_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_440_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_446_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_434_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_447_:%.+]] = stablehlo.real_dynamic_slice [[VAR_435_]], [[VAR_444_]], [[VAR_445_]], [[VAR_446_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_448_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_449_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_450_:%.+]] = stablehlo.slice [[VAR_9_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_451_:%.+]] = stablehlo.compare  LT, [[VAR_449_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_452_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_451_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_453_:%.+]] = stablehlo.negate [[VAR_449_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_454_:%.+]] = stablehlo.add [[VAR_450_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_455_:%.+]] = stablehlo.add [[VAR_448_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_456_:%.+]] = stablehlo.reverse [[VAR_372_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_457_:%.+]] = stablehlo.select [[VAR_451_]], [[VAR_454_]], [[VAR_448_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_458_:%.+]] = stablehlo.select [[VAR_451_]], [[VAR_455_]], [[VAR_450_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_459_:%.+]] = stablehlo.select [[VAR_451_]], [[VAR_453_]], [[VAR_449_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_460_:%.+]] = stablehlo.select [[VAR_452_]], [[VAR_456_]], [[VAR_372_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_461_:%.+]] = stablehlo.compare  GT, [[VAR_458_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_462_:%.+]] = stablehlo.select [[VAR_461_]], [[VAR_9_]], [[VAR_458_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_463_:%.+]] = stablehlo.compare  LT, [[VAR_462_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_464_:%.+]] = stablehlo.add [[VAR_462_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_465_:%.+]] = stablehlo.select [[VAR_463_]], [[VAR_464_]], [[VAR_462_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_466_:%.+]] = stablehlo.compare  LT, [[VAR_457_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_467_:%.+]] = stablehlo.add [[VAR_457_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_468_:%.+]] = stablehlo.select [[VAR_466_]], [[VAR_467_]], [[VAR_457_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_469_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_468_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_470_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_465_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_471_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_459_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_472_:%.+]] = stablehlo.real_dynamic_slice [[VAR_460_]], [[VAR_469_]], [[VAR_470_]], [[VAR_471_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_473_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_397_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_474_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_139_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_475_:%.+]] = stablehlo.add [[VAR_473_]], [[VAR_474_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_476_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_475_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_477_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_143_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_478_:%.+]] = stablehlo.add [[VAR_476_]], [[VAR_477_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_479_:%.+]] = stablehlo.logistic [[VAR_478_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_480_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_447_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_481_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_141_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_482_:%.+]] = stablehlo.add [[VAR_480_]], [[VAR_481_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_483_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_482_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_484_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_145_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_485_:%.+]] = stablehlo.add [[VAR_483_]], [[VAR_484_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_486_:%.+]] = stablehlo.logistic [[VAR_485_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_487_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_472_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_488_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_142_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_489_:%.+]] = stablehlo.add [[VAR_487_]], [[VAR_488_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_490_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_489_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_491_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_146_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_492_:%.+]] = stablehlo.add [[VAR_490_]], [[VAR_491_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_493_:%.+]] = stablehlo.tanh [[VAR_492_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_494_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_486_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_495_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_322_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_496_:%.+]] = stablehlo.multiply [[VAR_494_]], [[VAR_495_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_497_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_479_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_498_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_493_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_499_:%.+]] = stablehlo.multiply [[VAR_497_]], [[VAR_498_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_500_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_496_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_501_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_499_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_502_:%.+]] = stablehlo.add [[VAR_500_]], [[VAR_501_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_503_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_422_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_504_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_140_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_505_:%.+]] = stablehlo.add [[VAR_503_]], [[VAR_504_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_506_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_505_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_507_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_144_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_508_:%.+]] = stablehlo.add [[VAR_506_]], [[VAR_507_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_509_:%.+]] = stablehlo.logistic [[VAR_508_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_510_:%.+]] = stablehlo.tanh [[VAR_502_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_511_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_509_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_512_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_510_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_513_:%.+]] = stablehlo.multiply [[VAR_511_]], [[VAR_512_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_514_:%.+]] = stablehlo.dynamic_reshape [[VAR_513_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_515_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_516_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_517_:%.+]] = stablehlo.add [[VAR_515_]], [[VAR_516_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_518_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_519_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_520_:%.+]] = stablehlo.slice [[VAR_517_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_521_:%.+]] = stablehlo.compare  LT, [[VAR_519_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_522_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_521_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x512xi1>
+// CHECK-DAG:       [[VAR_523_:%.+]] = stablehlo.negate [[VAR_519_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_524_:%.+]] = stablehlo.add [[VAR_520_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_525_:%.+]] = stablehlo.add [[VAR_518_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_526_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<2x16x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_527_:%.+]] = stablehlo.select [[VAR_521_]], [[VAR_524_]], [[VAR_518_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_528_:%.+]] = stablehlo.select [[VAR_521_]], [[VAR_525_]], [[VAR_520_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_529_:%.+]] = stablehlo.select [[VAR_521_]], [[VAR_523_]], [[VAR_519_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_530_:%.+]] = stablehlo.select [[VAR_522_]], [[VAR_526_]], [[PARAM_0_]] : tensor<2x16x512xi1>, tensor<2x16x512xf32>
+// CHECK:           [[VAR_531_:%.+]] = stablehlo.compare  GT, [[VAR_528_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_532_:%.+]] = stablehlo.select [[VAR_531_]], [[VAR_18_]], [[VAR_528_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_533_:%.+]] = stablehlo.compare  LT, [[VAR_532_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_534_:%.+]] = stablehlo.add [[VAR_532_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_535_:%.+]] = stablehlo.select [[VAR_533_]], [[VAR_534_]], [[VAR_532_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_536_:%.+]] = stablehlo.compare  LT, [[VAR_527_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_537_:%.+]] = stablehlo.add [[VAR_527_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK:           [[VAR_538_:%.+]] = stablehlo.select [[VAR_536_]], [[VAR_537_]], [[VAR_527_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_539_:%.+]] = stablehlo.concatenate [[VAR_538_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_540_:%.+]] = stablehlo.concatenate [[VAR_535_]], [[VAR_14_]], [[VAR_11_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_541_:%.+]] = stablehlo.concatenate [[VAR_529_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_542_:%.+]] = stablehlo.real_dynamic_slice [[VAR_530_]], [[VAR_539_]], [[VAR_540_]], [[VAR_541_]] : (tensor<2x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_543_:%.+]] = stablehlo.dynamic_reshape [[VAR_542_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_544_:%.+]] = stablehlo.broadcast_in_dim [[VAR_543_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_545_:%.+]] = stablehlo.broadcast_in_dim [[VAR_133_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_546_:%.+]] = stablehlo.dot [[VAR_544_]], [[VAR_545_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_547_:%.+]] = stablehlo.broadcast_in_dim [[VAR_96_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_548_:%.+]] = stablehlo.broadcast_in_dim [[VAR_134_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_549_:%.+]] = stablehlo.dot [[VAR_547_]], [[VAR_548_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_550_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_546_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_551_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_549_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_552_:%.+]] = stablehlo.add [[VAR_550_]], [[VAR_551_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_553_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_554_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_555_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_556_:%.+]] = stablehlo.compare  LT, [[VAR_554_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_557_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_556_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_558_:%.+]] = stablehlo.negate [[VAR_554_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_559_:%.+]] = stablehlo.add [[VAR_555_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_560_:%.+]] = stablehlo.add [[VAR_553_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_561_:%.+]] = stablehlo.reverse [[VAR_552_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_562_:%.+]] = stablehlo.select [[VAR_556_]], [[VAR_559_]], [[VAR_553_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_563_:%.+]] = stablehlo.select [[VAR_556_]], [[VAR_560_]], [[VAR_555_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_564_:%.+]] = stablehlo.select [[VAR_556_]], [[VAR_558_]], [[VAR_554_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_565_:%.+]] = stablehlo.select [[VAR_557_]], [[VAR_561_]], [[VAR_552_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_566_:%.+]] = stablehlo.compare  GT, [[VAR_563_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_567_:%.+]] = stablehlo.select [[VAR_566_]], [[VAR_9_]], [[VAR_563_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_568_:%.+]] = stablehlo.compare  LT, [[VAR_567_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_569_:%.+]] = stablehlo.add [[VAR_567_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_570_:%.+]] = stablehlo.select [[VAR_568_]], [[VAR_569_]], [[VAR_567_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_571_:%.+]] = stablehlo.compare  LT, [[VAR_562_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_572_:%.+]] = stablehlo.add [[VAR_562_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_573_:%.+]] = stablehlo.select [[VAR_571_]], [[VAR_572_]], [[VAR_562_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_574_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_573_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_575_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_570_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_576_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_564_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_577_:%.+]] = stablehlo.real_dynamic_slice [[VAR_565_]], [[VAR_574_]], [[VAR_575_]], [[VAR_576_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_578_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_579_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_580_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_581_:%.+]] = stablehlo.compare  LT, [[VAR_579_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_582_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_581_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_583_:%.+]] = stablehlo.negate [[VAR_579_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_584_:%.+]] = stablehlo.add [[VAR_580_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_585_:%.+]] = stablehlo.add [[VAR_578_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_586_:%.+]] = stablehlo.reverse [[VAR_552_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_587_:%.+]] = stablehlo.select [[VAR_581_]], [[VAR_584_]], [[VAR_578_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_588_:%.+]] = stablehlo.select [[VAR_581_]], [[VAR_585_]], [[VAR_580_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_589_:%.+]] = stablehlo.select [[VAR_581_]], [[VAR_583_]], [[VAR_579_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_590_:%.+]] = stablehlo.select [[VAR_582_]], [[VAR_586_]], [[VAR_552_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_591_:%.+]] = stablehlo.compare  GT, [[VAR_588_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_592_:%.+]] = stablehlo.select [[VAR_591_]], [[VAR_9_]], [[VAR_588_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_593_:%.+]] = stablehlo.compare  LT, [[VAR_592_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_594_:%.+]] = stablehlo.add [[VAR_592_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_595_:%.+]] = stablehlo.select [[VAR_593_]], [[VAR_594_]], [[VAR_592_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_596_:%.+]] = stablehlo.compare  LT, [[VAR_587_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_597_:%.+]] = stablehlo.add [[VAR_587_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_598_:%.+]] = stablehlo.select [[VAR_596_]], [[VAR_597_]], [[VAR_587_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_599_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_598_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_600_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_595_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_601_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_589_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_602_:%.+]] = stablehlo.real_dynamic_slice [[VAR_590_]], [[VAR_599_]], [[VAR_600_]], [[VAR_601_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_603_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_604_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_605_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_606_:%.+]] = stablehlo.compare  LT, [[VAR_604_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_607_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_606_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_608_:%.+]] = stablehlo.negate [[VAR_604_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_609_:%.+]] = stablehlo.add [[VAR_605_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_610_:%.+]] = stablehlo.add [[VAR_603_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_611_:%.+]] = stablehlo.reverse [[VAR_552_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_612_:%.+]] = stablehlo.select [[VAR_606_]], [[VAR_609_]], [[VAR_603_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_613_:%.+]] = stablehlo.select [[VAR_606_]], [[VAR_610_]], [[VAR_605_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_614_:%.+]] = stablehlo.select [[VAR_606_]], [[VAR_608_]], [[VAR_604_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_615_:%.+]] = stablehlo.select [[VAR_607_]], [[VAR_611_]], [[VAR_552_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_616_:%.+]] = stablehlo.compare  GT, [[VAR_613_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_617_:%.+]] = stablehlo.select [[VAR_616_]], [[VAR_9_]], [[VAR_613_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_618_:%.+]] = stablehlo.compare  LT, [[VAR_617_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_619_:%.+]] = stablehlo.add [[VAR_617_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_620_:%.+]] = stablehlo.select [[VAR_618_]], [[VAR_619_]], [[VAR_617_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_621_:%.+]] = stablehlo.compare  LT, [[VAR_612_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_622_:%.+]] = stablehlo.add [[VAR_612_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_623_:%.+]] = stablehlo.select [[VAR_621_]], [[VAR_622_]], [[VAR_612_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_624_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_623_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_625_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_620_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_626_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_614_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_627_:%.+]] = stablehlo.real_dynamic_slice [[VAR_615_]], [[VAR_624_]], [[VAR_625_]], [[VAR_626_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_628_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_629_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_630_:%.+]] = stablehlo.slice [[VAR_9_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_631_:%.+]] = stablehlo.compare  LT, [[VAR_629_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_632_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_631_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_633_:%.+]] = stablehlo.negate [[VAR_629_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_634_:%.+]] = stablehlo.add [[VAR_630_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_635_:%.+]] = stablehlo.add [[VAR_628_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_636_:%.+]] = stablehlo.reverse [[VAR_552_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_637_:%.+]] = stablehlo.select [[VAR_631_]], [[VAR_634_]], [[VAR_628_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_638_:%.+]] = stablehlo.select [[VAR_631_]], [[VAR_635_]], [[VAR_630_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_639_:%.+]] = stablehlo.select [[VAR_631_]], [[VAR_633_]], [[VAR_629_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_640_:%.+]] = stablehlo.select [[VAR_632_]], [[VAR_636_]], [[VAR_552_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_641_:%.+]] = stablehlo.compare  GT, [[VAR_638_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_642_:%.+]] = stablehlo.select [[VAR_641_]], [[VAR_9_]], [[VAR_638_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_643_:%.+]] = stablehlo.compare  LT, [[VAR_642_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_644_:%.+]] = stablehlo.add [[VAR_642_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_645_:%.+]] = stablehlo.select [[VAR_643_]], [[VAR_644_]], [[VAR_642_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_646_:%.+]] = stablehlo.compare  LT, [[VAR_637_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_647_:%.+]] = stablehlo.add [[VAR_637_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_648_:%.+]] = stablehlo.select [[VAR_646_]], [[VAR_647_]], [[VAR_637_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_649_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_648_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_650_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_645_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_651_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_639_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_652_:%.+]] = stablehlo.real_dynamic_slice [[VAR_640_]], [[VAR_649_]], [[VAR_650_]], [[VAR_651_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_653_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_577_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_654_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_147_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_655_:%.+]] = stablehlo.add [[VAR_653_]], [[VAR_654_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_656_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_655_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_657_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_151_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_658_:%.+]] = stablehlo.add [[VAR_656_]], [[VAR_657_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_659_:%.+]] = stablehlo.logistic [[VAR_658_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_660_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_627_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_661_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_149_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_662_:%.+]] = stablehlo.add [[VAR_660_]], [[VAR_661_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_663_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_662_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_664_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_153_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_665_:%.+]] = stablehlo.add [[VAR_663_]], [[VAR_664_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_666_:%.+]] = stablehlo.logistic [[VAR_665_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_667_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_652_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_668_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_150_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_669_:%.+]] = stablehlo.add [[VAR_667_]], [[VAR_668_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_670_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_669_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_671_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_154_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_672_:%.+]] = stablehlo.add [[VAR_670_]], [[VAR_671_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_673_:%.+]] = stablehlo.tanh [[VAR_672_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_674_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_666_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_675_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_122_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_676_:%.+]] = stablehlo.multiply [[VAR_674_]], [[VAR_675_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_677_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_659_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_678_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_673_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_679_:%.+]] = stablehlo.multiply [[VAR_677_]], [[VAR_678_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_680_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_676_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_681_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_679_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_682_:%.+]] = stablehlo.add [[VAR_680_]], [[VAR_681_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_683_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_602_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_684_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_148_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_685_:%.+]] = stablehlo.add [[VAR_683_]], [[VAR_684_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_686_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_685_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_687_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_152_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_688_:%.+]] = stablehlo.add [[VAR_686_]], [[VAR_687_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_689_:%.+]] = stablehlo.logistic [[VAR_688_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_690_:%.+]] = stablehlo.tanh [[VAR_682_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_691_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_689_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_692_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_690_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_693_:%.+]] = stablehlo.multiply [[VAR_691_]], [[VAR_692_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_694_:%.+]] = stablehlo.dynamic_reshape [[VAR_693_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_695_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_16_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_696_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_697_:%.+]] = stablehlo.add [[VAR_695_]], [[VAR_696_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_698_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_699_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_700_:%.+]] = stablehlo.slice [[VAR_697_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_701_:%.+]] = stablehlo.compare  LT, [[VAR_699_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_702_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_701_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x512xi1>
+// CHECK-DAG:       [[VAR_703_:%.+]] = stablehlo.negate [[VAR_699_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_704_:%.+]] = stablehlo.add [[VAR_700_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_705_:%.+]] = stablehlo.add [[VAR_698_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_706_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<2x16x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_707_:%.+]] = stablehlo.select [[VAR_701_]], [[VAR_704_]], [[VAR_698_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_708_:%.+]] = stablehlo.select [[VAR_701_]], [[VAR_705_]], [[VAR_700_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_709_:%.+]] = stablehlo.select [[VAR_701_]], [[VAR_703_]], [[VAR_699_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_710_:%.+]] = stablehlo.select [[VAR_702_]], [[VAR_706_]], [[PARAM_0_]] : tensor<2x16x512xi1>, tensor<2x16x512xf32>
+// CHECK:           [[VAR_711_:%.+]] = stablehlo.compare  GT, [[VAR_708_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_712_:%.+]] = stablehlo.select [[VAR_711_]], [[VAR_18_]], [[VAR_708_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_713_:%.+]] = stablehlo.compare  LT, [[VAR_712_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_714_:%.+]] = stablehlo.add [[VAR_712_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_715_:%.+]] = stablehlo.select [[VAR_713_]], [[VAR_714_]], [[VAR_712_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_716_:%.+]] = stablehlo.compare  LT, [[VAR_707_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_717_:%.+]] = stablehlo.add [[VAR_707_]], [[VAR_18_]] : tensor<1xi64>
+// CHECK:           [[VAR_718_:%.+]] = stablehlo.select [[VAR_716_]], [[VAR_717_]], [[VAR_707_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_719_:%.+]] = stablehlo.concatenate [[VAR_718_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_720_:%.+]] = stablehlo.concatenate [[VAR_715_]], [[VAR_14_]], [[VAR_11_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK-DAG:       [[VAR_721_:%.+]] = stablehlo.concatenate [[VAR_709_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
+// CHECK:           [[VAR_722_:%.+]] = stablehlo.real_dynamic_slice [[VAR_710_]], [[VAR_719_]], [[VAR_720_]], [[VAR_721_]] : (tensor<2x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_723_:%.+]] = stablehlo.dynamic_reshape [[VAR_722_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_724_:%.+]] = stablehlo.broadcast_in_dim [[VAR_723_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_725_:%.+]] = stablehlo.broadcast_in_dim [[VAR_133_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_726_:%.+]] = stablehlo.dot [[VAR_724_]], [[VAR_725_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_727_:%.+]] = stablehlo.broadcast_in_dim [[VAR_693_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_728_:%.+]] = stablehlo.broadcast_in_dim [[VAR_134_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_729_:%.+]] = stablehlo.dot [[VAR_727_]], [[VAR_728_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_730_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_726_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_731_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_729_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_732_:%.+]] = stablehlo.add [[VAR_730_]], [[VAR_731_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_733_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_734_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_735_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_736_:%.+]] = stablehlo.compare  LT, [[VAR_734_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_737_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_736_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_738_:%.+]] = stablehlo.negate [[VAR_734_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_739_:%.+]] = stablehlo.add [[VAR_735_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_740_:%.+]] = stablehlo.add [[VAR_733_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_741_:%.+]] = stablehlo.reverse [[VAR_732_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_742_:%.+]] = stablehlo.select [[VAR_736_]], [[VAR_739_]], [[VAR_733_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_743_:%.+]] = stablehlo.select [[VAR_736_]], [[VAR_740_]], [[VAR_735_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_744_:%.+]] = stablehlo.select [[VAR_736_]], [[VAR_738_]], [[VAR_734_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_745_:%.+]] = stablehlo.select [[VAR_737_]], [[VAR_741_]], [[VAR_732_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_746_:%.+]] = stablehlo.compare  GT, [[VAR_743_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_747_:%.+]] = stablehlo.select [[VAR_746_]], [[VAR_9_]], [[VAR_743_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_748_:%.+]] = stablehlo.compare  LT, [[VAR_747_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_749_:%.+]] = stablehlo.add [[VAR_747_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_750_:%.+]] = stablehlo.select [[VAR_748_]], [[VAR_749_]], [[VAR_747_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_751_:%.+]] = stablehlo.compare  LT, [[VAR_742_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_752_:%.+]] = stablehlo.add [[VAR_742_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_753_:%.+]] = stablehlo.select [[VAR_751_]], [[VAR_752_]], [[VAR_742_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_754_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_753_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_755_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_750_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_756_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_744_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_757_:%.+]] = stablehlo.real_dynamic_slice [[VAR_745_]], [[VAR_754_]], [[VAR_755_]], [[VAR_756_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_758_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_759_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_760_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_761_:%.+]] = stablehlo.compare  LT, [[VAR_759_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_762_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_761_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_763_:%.+]] = stablehlo.negate [[VAR_759_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_764_:%.+]] = stablehlo.add [[VAR_760_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_765_:%.+]] = stablehlo.add [[VAR_758_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_766_:%.+]] = stablehlo.reverse [[VAR_732_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_767_:%.+]] = stablehlo.select [[VAR_761_]], [[VAR_764_]], [[VAR_758_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_768_:%.+]] = stablehlo.select [[VAR_761_]], [[VAR_765_]], [[VAR_760_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_769_:%.+]] = stablehlo.select [[VAR_761_]], [[VAR_763_]], [[VAR_759_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_770_:%.+]] = stablehlo.select [[VAR_762_]], [[VAR_766_]], [[VAR_732_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_771_:%.+]] = stablehlo.compare  GT, [[VAR_768_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_772_:%.+]] = stablehlo.select [[VAR_771_]], [[VAR_9_]], [[VAR_768_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_773_:%.+]] = stablehlo.compare  LT, [[VAR_772_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_774_:%.+]] = stablehlo.add [[VAR_772_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_775_:%.+]] = stablehlo.select [[VAR_773_]], [[VAR_774_]], [[VAR_772_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_776_:%.+]] = stablehlo.compare  LT, [[VAR_767_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_777_:%.+]] = stablehlo.add [[VAR_767_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_778_:%.+]] = stablehlo.select [[VAR_776_]], [[VAR_777_]], [[VAR_767_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_779_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_778_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_780_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_775_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_781_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_769_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_782_:%.+]] = stablehlo.real_dynamic_slice [[VAR_770_]], [[VAR_779_]], [[VAR_780_]], [[VAR_781_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_783_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_784_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_785_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_786_:%.+]] = stablehlo.compare  LT, [[VAR_784_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_787_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_786_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_788_:%.+]] = stablehlo.negate [[VAR_784_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_789_:%.+]] = stablehlo.add [[VAR_785_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_790_:%.+]] = stablehlo.add [[VAR_783_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_791_:%.+]] = stablehlo.reverse [[VAR_732_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_792_:%.+]] = stablehlo.select [[VAR_786_]], [[VAR_789_]], [[VAR_783_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_793_:%.+]] = stablehlo.select [[VAR_786_]], [[VAR_790_]], [[VAR_785_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_794_:%.+]] = stablehlo.select [[VAR_786_]], [[VAR_788_]], [[VAR_784_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_795_:%.+]] = stablehlo.select [[VAR_787_]], [[VAR_791_]], [[VAR_732_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_796_:%.+]] = stablehlo.compare  GT, [[VAR_793_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_797_:%.+]] = stablehlo.select [[VAR_796_]], [[VAR_9_]], [[VAR_793_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_798_:%.+]] = stablehlo.compare  LT, [[VAR_797_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_799_:%.+]] = stablehlo.add [[VAR_797_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_800_:%.+]] = stablehlo.select [[VAR_798_]], [[VAR_799_]], [[VAR_797_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_801_:%.+]] = stablehlo.compare  LT, [[VAR_792_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_802_:%.+]] = stablehlo.add [[VAR_792_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_803_:%.+]] = stablehlo.select [[VAR_801_]], [[VAR_802_]], [[VAR_792_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_804_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_803_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_805_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_800_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_806_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_794_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_807_:%.+]] = stablehlo.real_dynamic_slice [[VAR_795_]], [[VAR_804_]], [[VAR_805_]], [[VAR_806_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_808_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_809_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_810_:%.+]] = stablehlo.slice [[VAR_9_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
+// CHECK:           [[VAR_811_:%.+]] = stablehlo.compare  LT, [[VAR_809_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_812_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_811_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
+// CHECK-DAG:       [[VAR_813_:%.+]] = stablehlo.negate [[VAR_809_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_814_:%.+]] = stablehlo.add [[VAR_810_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_815_:%.+]] = stablehlo.add [[VAR_808_]], [[VAR_17_]] : tensor<1xi64>
+// CHECK-DAG:       [[VAR_816_:%.+]] = stablehlo.reverse [[VAR_732_]], dims = [1] : tensor<16x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_817_:%.+]] = stablehlo.select [[VAR_811_]], [[VAR_814_]], [[VAR_808_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_818_:%.+]] = stablehlo.select [[VAR_811_]], [[VAR_815_]], [[VAR_810_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_819_:%.+]] = stablehlo.select [[VAR_811_]], [[VAR_813_]], [[VAR_809_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_820_:%.+]] = stablehlo.select [[VAR_812_]], [[VAR_816_]], [[VAR_732_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
+// CHECK:           [[VAR_821_:%.+]] = stablehlo.compare  GT, [[VAR_818_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:           [[VAR_822_:%.+]] = stablehlo.select [[VAR_821_]], [[VAR_9_]], [[VAR_818_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_823_:%.+]] = stablehlo.compare  LT, [[VAR_822_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_824_:%.+]] = stablehlo.add [[VAR_822_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_825_:%.+]] = stablehlo.select [[VAR_823_]], [[VAR_824_]], [[VAR_822_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_826_:%.+]] = stablehlo.compare  LT, [[VAR_817_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK-DAG:       [[VAR_827_:%.+]] = stablehlo.add [[VAR_817_]], [[VAR_9_]] : tensor<1xi64>
+// CHECK:           [[VAR_828_:%.+]] = stablehlo.select [[VAR_826_]], [[VAR_827_]], [[VAR_817_]] : tensor<1xi1>, tensor<1xi64>
+// CHECK-DAG:       [[VAR_829_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_828_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_830_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_825_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-DAG:       [[VAR_831_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_819_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_832_:%.+]] = stablehlo.real_dynamic_slice [[VAR_820_]], [[VAR_829_]], [[VAR_830_]], [[VAR_831_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_833_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_757_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_834_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_147_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_835_:%.+]] = stablehlo.add [[VAR_833_]], [[VAR_834_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_836_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_835_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_837_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_151_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_838_:%.+]] = stablehlo.add [[VAR_836_]], [[VAR_837_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_839_:%.+]] = stablehlo.logistic [[VAR_838_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_840_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_807_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_841_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_149_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_842_:%.+]] = stablehlo.add [[VAR_840_]], [[VAR_841_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_843_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_842_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_844_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_153_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_845_:%.+]] = stablehlo.add [[VAR_843_]], [[VAR_844_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_846_:%.+]] = stablehlo.logistic [[VAR_845_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_847_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_832_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_848_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_150_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_849_:%.+]] = stablehlo.add [[VAR_847_]], [[VAR_848_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_850_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_849_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_851_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_154_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_852_:%.+]] = stablehlo.add [[VAR_850_]], [[VAR_851_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_853_:%.+]] = stablehlo.tanh [[VAR_852_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_854_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_846_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_855_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_682_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_856_:%.+]] = stablehlo.multiply [[VAR_854_]], [[VAR_855_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_857_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_839_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_858_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_853_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_859_:%.+]] = stablehlo.multiply [[VAR_857_]], [[VAR_858_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_860_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_856_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_861_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_859_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_862_:%.+]] = stablehlo.add [[VAR_860_]], [[VAR_861_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_863_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_782_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_864_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_148_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_865_:%.+]] = stablehlo.add [[VAR_863_]], [[VAR_864_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_866_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_865_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_867_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_152_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_868_:%.+]] = stablehlo.add [[VAR_866_]], [[VAR_867_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_869_:%.+]] = stablehlo.logistic [[VAR_868_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_870_:%.+]] = stablehlo.tanh [[VAR_862_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_871_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_869_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_872_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_870_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_873_:%.+]] = stablehlo.multiply [[VAR_871_]], [[VAR_872_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_874_:%.+]] = stablehlo.dynamic_reshape [[VAR_873_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_875_:%.+]] = stablehlo.concatenate [[VAR_334_]], [[VAR_514_]], dim = 0 : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
+// CHECK:           [[VAR_876_:%.+]] = stablehlo.concatenate [[VAR_874_]], [[VAR_694_]], dim = 0 : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
+// CHECK:           [[VAR_877_:%.+]] = stablehlo.concatenate [[VAR_875_]], [[VAR_876_]], dim = 1 : (tensor<2x1x16x256xf32>, tensor<2x1x16x256xf32>) -> tensor<2x2x16x256xf32>
+// CHECK:           return [[VAR_877_]] : tensor<2x2x16x256xf32>
+// CHECK:         }
+}
diff --git a/test/mlir/conversion/onnx_to_stablehlo/Tensor/OneHot.mlir b/test/mlir/conversion/onnx_to_stablehlo/Tensor/OneHot.mlir
new file mode 100644
index 0000000000..b818e5e70a
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_stablehlo/Tensor/OneHot.mlir
@@ -0,0 +1,31 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-stablehlo %s --canonicalize -split-input-file | FileCheck %s
+
+func.func @test_onehot(%arg0 : tensor<2x3x4xi64>) -> tensor<*xi64> {
+  %0 = onnx.Constant dense<64> : tensor<1xi64>
+  %1 = onnx.Constant dense<[0, 1]> : tensor<2xi64>
+  %2 = "onnx.OneHot"(%arg0, %0, %1) {axis = -1 : si64} : (tensor<2x3x4xi64>, tensor<1xi64>, tensor<2xi64>) -> tensor<*xi64>
+  "func.return"(%2) : (tensor<*xi64>) -> ()
+// CHECK-LABEL:  func.func @test_onehot
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<2x3x4xi64>) -> tensor<2x3x4x64xi64> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = stablehlo.constant dense<0> : tensor<i64>
+// CHECK-DAG:       [[VAR_1_:%.+]] = stablehlo.constant dense<64> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_2_:%.+]] = stablehlo.constant dense<[0, 1]> : tensor<2xi64>
+// CHECK-DAG:       [[VAR_3_:%.+]] = stablehlo.iota dim = 3 : tensor<2x3x4x64xi64>
+// CHECK-DAG:       [[VAR_4_:%.+]] = stablehlo.broadcast_in_dim [[PARAM_0_]], dims = [0, 1, 2] : (tensor<2x3x4xi64>) -> tensor<2x3x4x64xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_5_:%.+]] = stablehlo.broadcast_in_dim [[VAR_0_]], dims = [] : (tensor<i64>) -> tensor<2x3x4x64xi64>
+// CHECK-DAG:       [[VAR_6_:%.+]] = stablehlo.broadcast_in_dim [[VAR_1_]], dims = [0] : (tensor<1xi64>) -> tensor<2x3x4x64xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_7_:%.+]] = stablehlo.compare  GE, [[VAR_4_]], [[VAR_5_]],  NOTYPE : (tensor<2x3x4x64xi64>, tensor<2x3x4x64xi64>) -> tensor<2x3x4x64xi1>
+// CHECK-DAG:       [[VAR_8_:%.+]] = stablehlo.add [[VAR_4_]], [[VAR_6_]] : tensor<2x3x4x64xi64>
+// CHECK:           [[VAR_9_:%.+]] = stablehlo.select [[VAR_7_]], [[VAR_4_]], [[VAR_8_]] : tensor<2x3x4x64xi1>, tensor<2x3x4x64xi64>
+// CHECK-DAG:       [[VAR_10_:%.+]] = stablehlo.compare  EQ, [[VAR_9_]], [[VAR_3_]],  NOTYPE : (tensor<2x3x4x64xi64>, tensor<2x3x4x64xi64>) -> tensor<2x3x4x64xi1>
+// CHECK-DAG:       [[VAR_11_:%.+]] = stablehlo.slice [[VAR_2_]] [0:1] : (tensor<2xi64>) -> tensor<1xi64>
+// CHECK-DAG:       [[VAR_12_:%.+]] = stablehlo.slice [[VAR_2_]] [1:2] : (tensor<2xi64>) -> tensor<1xi64>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_13_:%.+]] = stablehlo.broadcast_in_dim [[VAR_11_]], dims = [0] : (tensor<1xi64>) -> tensor<2x3x4x64xi64>
+// CHECK-DAG:       [[VAR_14_:%.+]] = stablehlo.broadcast_in_dim [[VAR_12_]], dims = [0] : (tensor<1xi64>) -> tensor<2x3x4x64xi64>
+// CHECK:           [[VAR_15_:%.+]] = stablehlo.select [[VAR_10_]], [[VAR_14_]], [[VAR_13_]] : tensor<2x3x4x64xi1>, tensor<2x3x4x64xi64>
+// CHECK:           return [[VAR_15_]] : tensor<2x3x4x64xi64>
+// CHECK:         }
+}
diff --git a/test/mlir/conversion/onnx_to_stablehlo/Tensor/ScatterND.mlir b/test/mlir/conversion/onnx_to_stablehlo/Tensor/ScatterND.mlir
new file mode 100644
index 0000000000..ae2036964e
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_stablehlo/Tensor/ScatterND.mlir
@@ -0,0 +1,29 @@
+// RUN: onnx-mlir-opt --canonicalize --convert-onnx-to-stablehlo %s -split-input-file | FileCheck %s
+
+func.func @test_scatternd_1(%arg0 : tensor<8xf32>, %arg1 : tensor<4x1xi64>, %arg2 : tensor<4xf32>) -> tensor<8xf32> {
+  %0 = "onnx.ScatterND"(%arg0, %arg1, %arg2) : (tensor<8xf32>, tensor<4x1xi64>, tensor<4xf32>) -> tensor<8xf32>
+  return %0 : tensor<8xf32>
+// CHECK-LABEL:  func.func @test_scatternd_1
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<8xf32>, [[PARAM_1_:%.+]]: tensor<4x1xi64>, [[PARAM_2_:%.+]]: tensor<4xf32>) -> tensor<8xf32> {
+// CHECK:           [[VAR_0_:%.+]] = "stablehlo.scatter"([[PARAM_0_]], [[PARAM_1_]], [[PARAM_2_]]) ({
+// CHECK:           ^bb0([[arg3_:%.+]]: tensor<f32>, [[arg4_:%.+]]: tensor<f32>):
+// CHECK:             stablehlo.return [[arg4_]] : tensor<f32>
+// CHECK:           }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<8xf32>, tensor<4x1xi64>, tensor<4xf32>) -> tensor<8xf32>
+// CHECK:           return [[VAR_0_]] : tensor<8xf32>
+// CHECK:         }
+}
+
+// -----
+
+func.func @test_scatternd_2(%arg0 : tensor<4x4x4xi32>, %arg1 : tensor<2x1xi64>, %arg2 : tensor<2x4x4xi32>) -> tensor<4x4x4xi32> {
+  %0 = "onnx.ScatterND"(%arg0, %arg1, %arg2) : (tensor<4x4x4xi32>, tensor<2x1xi64>, tensor<2x4x4xi32>) -> tensor<4x4x4xi32>
+  return %0 : tensor<4x4x4xi32>
+// CHECK-LABEL:  func.func @test_scatternd_2
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<4x4x4xi32>, [[PARAM_1_:%.+]]: tensor<2x1xi64>, [[PARAM_2_:%.+]]: tensor<2x4x4xi32>) -> tensor<4x4x4xi32> {
+// CHECK:           [[VAR_0_:%.+]] = "stablehlo.scatter"([[PARAM_0_]], [[PARAM_1_]], [[PARAM_2_]]) ({
+// CHECK:           ^bb0([[arg3_:%.+]]: tensor<i32>, [[arg4_:%.+]]: tensor<i32>):
+// CHECK:             stablehlo.return [[arg4_]] : tensor<i32>
+// CHECK:           }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<4x4x4xi32>, tensor<2x1xi64>, tensor<2x4x4xi32>) -> tensor<4x4x4xi32>
+// CHECK:           return [[VAR_0_]] : tensor<4x4x4xi32>
+// CHECK:         }
+}

From 89d530ec24ef1a3ec2856beb34f21a0c170da846 Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Wed, 20 Dec 2023 03:28:24 -0500
Subject: [PATCH 02/11] fix clang-format

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 .../ONNXToStableHlo/DialectBuilder.cpp           | 16 +++++++++-------
 1 file changed, 9 insertions(+), 7 deletions(-)

diff --git a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp b/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
index 10f959097e..296ebd1891 100644
--- a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
+++ b/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
@@ -14,8 +14,8 @@
 //===----------------------------------------------------------------------===//

 #include "mlir/Dialect/Arith/IR/Arith.h"
-#include "llvm/ADT/TypeSwitch.h"
 #include "stablehlo/dialect/StablehloOps.h"
+#include "llvm/ADT/TypeSwitch.h"

 #include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
 #include "src/Dialect/ONNX/ONNXOps.hpp"
@@ -50,15 +50,17 @@ Value StablehloBuilder::constant(mlir::Type type, double val) const {
         unsigned width = elementType.getWidth();

         if (width == 1)
-          constant =
-              b().create<stablehlo::ConstantOp>(loc(), b().getBoolAttr(val != 0));
+          constant = b().create<stablehlo::ConstantOp>(
+              loc(), b().getBoolAttr(val != 0));
         else {
           if (elementType.isUnsignedInteger()) {
-            constant = b().create<stablehlo::ConstantOp>(loc(),
-                b().getIntegerAttr(elementType, APInt(width, (uint64_t)val, false)));
+            constant = b().create<stablehlo::ConstantOp>(
+                loc(), b().getIntegerAttr(
+                           elementType, APInt(width, (uint64_t)val, false)));
           } else {
-            constant = b().create<stablehlo::ConstantOp>(loc(),
-                b().getIntegerAttr(elementType, APInt(width, (int64_t)val, true)));
+            constant = b().create<stablehlo::ConstantOp>(
+                loc(), b().getIntegerAttr(
+                           elementType, APInt(width, (int64_t)val, true)));
           }
         }
       })

From 606355bc378915d1db55e7856ad3a86224ed0794 Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Wed, 20 Dec 2023 05:08:47 -0500
Subject: [PATCH 03/11] add pass to pass.hpp

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 src/Pass/Passes.hpp | 1 +
 1 file changed, 1 insertion(+)

diff --git a/src/Pass/Passes.hpp b/src/Pass/Passes.hpp
index a708e67176..3f8906fa82 100644
--- a/src/Pass/Passes.hpp
+++ b/src/Pass/Passes.hpp
@@ -92,6 +92,7 @@ std::unique_ptr<mlir::Pass> createProcessAffineParallelPrivatePass();
 #ifdef ONNX_MLIR_ENABLE_STABLEHLO
 /// Add pass for lowering to StableHlo IR.
 std::unique_ptr<mlir::Pass> createLowerToStableHloPass();
+std::unique_ptr<mlir::Pass> createLowerToStableHloPass(bool enableUnroll);
 #endif

 /// Pass for lowering krnl.dim operations to standard dialect.

From 2a04fdddf543de7c73905e4cc1a8439f01854018 Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Mon, 25 Dec 2023 03:57:04 -0500
Subject: [PATCH 04/11] update DenseI64ArrayAttr, lower to
 stablehlo.dynamic_slice if possible

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 .../ONNXToStableHlo/DialectBuilder.cpp        |   41 +-
 .../ONNXToStableHlo/DialectBuilder.hpp        |   21 +-
 src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp   |   35 +-
 .../ONNXToStableHlo/RNN/RNNBase.cpp           |   72 +-
 .../ONNXToStableHlo/Tensor/OneHot.cpp         |   20 +-
 .../onnx_to_stablehlo/RNN/LSTM-loop.mlir      |  817 +++--------
 .../onnx_to_stablehlo/RNN/LSTM.mlir           | 1296 ++++-------------
 7 files changed, 654 insertions(+), 1648 deletions(-)

diff --git a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp b/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
index 296ebd1891..8ea4148c53 100644
--- a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
+++ b/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
@@ -74,8 +74,8 @@ Value StablehloBuilder::constant(mlir::Type type, double val) const {
   return constant;
 }

-Value StablehloBuilder::constantIndex(int64_t val) const {
-  IntegerAttr constantAttr = b().getIntegerAttr(b().getIndexType(), val);
+Value StablehloBuilder::constantI64(int64_t val) const {
+  IntegerAttr constantAttr = b().getIntegerAttr(b().getI64Type(), val);
   return b().create<stablehlo::ConstantOp>(loc(), constantAttr);
 }

@@ -83,6 +83,43 @@ Value StablehloBuilder::shaped_zero(mlir::Type type) const {
   return b().create<stablehlo::ConstantOp>(loc(), b().getZeroAttr(type));
 }

+Value StablehloBuilder::reshape(Type resultType, Value operand) const {
+  return b().create<stablehlo::ReshapeOp>(loc(), resultType, operand);
+}
+
+mlir::Value StablehloBuilder::real_dynamic_slice(mlir::Type type,
+    mlir::Value operand, mlir::Value startIndices, mlir::Value limitIndices,
+    mlir::Value strides) const {
+  return b().create<stablehlo::RealDynamicSliceOp>(
+      loc(), type, operand, startIndices, limitIndices, strides);
+}
+
+mlir::Value StablehloBuilder::dynamic_slice(mlir::Value operand,
+    SmallVector<Value> startIndices, SmallVector<int64_t> sliceSizes) const {
+  return b().create<stablehlo::DynamicSliceOp>(
+      loc(), operand, startIndices, sliceSizes);
+}
+
+mlir::Value StablehloBuilder::dynamic_slice(mlir::Value operand,
+    SmallVector<Value> startIndices, DenseI64ArrayAttr sliceSizes) const {
+  return b().create<stablehlo::DynamicSliceOp>(
+      loc(), operand, startIndices, sliceSizes);
+}
+
+mlir::Value StablehloBuilder::slice(mlir::Value operand,
+    SmallVector<int64_t> startIndices, SmallVector<int64_t> limitIndices,
+    SmallVector<int64_t> strides) const {
+  return b().create<stablehlo::SliceOp>(
+      loc(), operand, startIndices, limitIndices, strides);
+}
+
+mlir::Value StablehloBuilder::slice(mlir::Value operand,
+    DenseI64ArrayAttr startIndices, DenseI64ArrayAttr limitIndices,
+    DenseI64ArrayAttr strides) const {
+  return b().create<stablehlo::SliceOp>(
+      loc(), operand, startIndices, limitIndices, strides);
+}
+
 // =============================================================================
 // IndexExpr Builder for Lowering using Shape/StableHlo Dialect.
 // =============================================================================
diff --git a/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp b/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
index 820c94d58a..2125d884bd 100644
--- a/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
+++ b/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
@@ -40,8 +40,27 @@ struct StablehloBuilder : DialectBuilder {

   // ConstantOp
   mlir::Value constant(mlir::Type type, double val) const;
-  mlir::Value constantIndex(int64_t val) const;
+  mlir::Value constantI64(int64_t val) const;
   mlir::Value shaped_zero(mlir::Type type) const;
+  // ReshapeOp
+  mlir::Value reshape(mlir::Type resultType, mlir::Value operand) const;
+  // SliceOp
+  mlir::Value real_dynamic_slice(mlir::Type type, mlir::Value operand,
+      mlir::Value startIndices, mlir::Value limitIndices,
+      mlir::Value strides) const;
+  mlir::Value dynamic_slice(mlir::Value operand,
+      mlir::SmallVector<mlir::Value> startIndices,
+      mlir::SmallVector<int64_t> sliceSizes) const;
+  mlir::Value dynamic_slice(mlir::Value operand,
+      mlir::SmallVector<mlir::Value> startIndices,
+      mlir::DenseI64ArrayAttr sliceSizes) const;
+  mlir::Value slice(mlir::Value operand,
+      mlir::SmallVector<int64_t> startIndices,
+      mlir::SmallVector<int64_t> limitIndices,
+      mlir::SmallVector<int64_t> strides) const;
+  mlir::Value slice(mlir::Value operand, mlir::DenseI64ArrayAttr startIndices,
+      mlir::DenseI64ArrayAttr limitIndices,
+      mlir::DenseI64ArrayAttr strides) const;

 protected:
   // Private getters of builder (concise version).
diff --git a/src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp b/src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp
index cdeafcee7e..17c9e5dae8 100644
--- a/src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp
+++ b/src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp
@@ -476,7 +476,7 @@ void calculateState<LstmState, LstmActivationPack, LstmWeightPack,
   // ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)
   // Ht = ot (.) h(Ct)

-  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  MultiDialectBuilder<OnnxBuilder, StablehloBuilder> create(rewriter, loc);

   ArrayRef<int64_t> xtShape = Xt.getType().cast<ShapedType>().getShape();
   int64_t batchSize = xtShape[0];
@@ -502,22 +502,23 @@ void calculateState<LstmState, LstmActivationPack, LstmWeightPack,
   Value XtWT = create.onnx.matmul(matrixAllGatesType, Xt, weightPack.WT);
   Value HtRT = create.onnx.matmul(matrixAllGatesType, Ht, weightPack.RT);
   Value commonSum = create.onnx.add(XtWT, HtRT);
-  RankedTensorType matrixSingleGateType =
-      RankedTensorType::get({batchSize, hiddenSize}, elementType);
-  Value zeroIndex = create.onnx.constantInt64({0});
-  Value oneIndex = create.onnx.constantInt64({1});
-  Value oneHiddenIndex = create.onnx.constantInt64({hiddenSize});
-  Value twoHiddenIndex = create.onnx.constantInt64({2 * hiddenSize});
-  Value threeHiddenIndex = create.onnx.constantInt64({3 * hiddenSize});
-  Value fourHiddenIndex = create.onnx.constantInt64({4 * hiddenSize});
-  Value it = create.onnx.slice(matrixSingleGateType, commonSum, zeroIndex,
-      oneHiddenIndex, oneIndex, oneIndex);
-  Value ot = create.onnx.slice(matrixSingleGateType, commonSum, oneHiddenIndex,
-      twoHiddenIndex, oneIndex, oneIndex);
-  Value ft = create.onnx.slice(matrixSingleGateType, commonSum, twoHiddenIndex,
-      threeHiddenIndex, oneIndex, oneIndex);
-  Value ct = create.onnx.slice(matrixSingleGateType, commonSum,
-      threeHiddenIndex, fourHiddenIndex, oneIndex, oneIndex);
+  Value zeroIndex = create.stablehlo.constantI64(0);
+  Value oneHiddenIndex = create.stablehlo.constantI64(hiddenSize);
+  Value twoHiddenIndex = create.stablehlo.constantI64(2 * hiddenSize);
+  Value threeHiddenIndex = create.stablehlo.constantI64(3 * hiddenSize);
+  SmallVector<int64_t> sliceSizes = {batchSize, hiddenSize};
+  SmallVector<Value> iStartIndices = {zeroIndex, zeroIndex};
+  SmallVector<Value> oStartIndices = {zeroIndex, oneHiddenIndex};
+  SmallVector<Value> fStartIndices = {zeroIndex, twoHiddenIndex};
+  SmallVector<Value> cStartIndices = {zeroIndex, threeHiddenIndex};
+  Value it =
+      create.stablehlo.dynamic_slice(commonSum, iStartIndices, sliceSizes);
+  Value ot =
+      create.stablehlo.dynamic_slice(commonSum, oStartIndices, sliceSizes);
+  Value ft =
+      create.stablehlo.dynamic_slice(commonSum, fStartIndices, sliceSizes);
+  Value ct =
+      create.stablehlo.dynamic_slice(commonSum, cStartIndices, sliceSizes);
   if (biasPack.hasBias) {
     it = create.onnx.add(it, biasPack.Wbi);
     it = create.onnx.add(it, biasPack.Rbi);
diff --git a/src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp b/src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp
index 6c771cfa0b..6fb0d7aeb9 100644
--- a/src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp
+++ b/src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp
@@ -15,10 +15,6 @@
 #include "src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp"
 #include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"

-#include "llvm/Support/Debug.h"
-
-#define DEBUG_TYPE "lstm"
-
 using namespace mlir;

 namespace onnx_mlir {
@@ -34,7 +30,6 @@ int64_t dimAt(Value val, int index) {
 /// Shape :: [seq_length, num_directions, batch_size, hidden_size]
 Value allocAllHidden(
     ConversionPatternRewriter &rewriter, Location loc, Value X, Value R) {
-  LLVM_DEBUG(llvm::dbgs() << "allocAllHidden\n");
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
   RankedTensorType zeroType =
       RankedTensorType::get({dimAt(X, 0), 1, dimAt(X, 1), dimAt(R, 2)},
@@ -46,7 +41,6 @@ Value allocAllHidden(
 /// Allocate the hidden or cell output.
 mlir::Value allocHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
     mlir::Location loc, mlir::Value X, mlir::Value W, mlir::Value R) {
-  LLVM_DEBUG(llvm::dbgs() << "allocHiddenOrCell\n");
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
   RankedTensorType zeroType = RankedTensorType::get(
       {/*num_directions=*/dimAt(W, 0), /*batch_size=*/dimAt(X, 1),
@@ -60,7 +54,6 @@ mlir::Value allocHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
 /// Shape :: [batch_size, hidden_size]
 Value allocIntermediateState(
     ConversionPatternRewriter &rewriter, Location loc, Value X, Value R) {
-  LLVM_DEBUG(llvm::dbgs() << "allocIntermediateState\n");
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
   RankedTensorType zeroType =
       RankedTensorType::get({/*batch_size=*/dimAt(X, 1),
@@ -76,54 +69,54 @@ void initializeIntermediateStates(ConversionPatternRewriter &rewriter,
     Location loc, Value &forwardHt, Value &reverseHt, Value &forwardCt,
     Value &reverseCt, Value initialH, Value initialC, Type elementType,
     StringRef direction, bool onlyHidden) {
-  LLVM_DEBUG(llvm::dbgs() << "initializeIntermediateStates\n");
-  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  MultiDialectBuilder<OnnxBuilder, StablehloBuilder> create(rewriter, loc);

-  Value zeroIndex = create.onnx.constantInt64({0});
-  Value oneIndex = create.onnx.constantInt64({1});
-  Value twoIndex = create.onnx.constantInt64({2});
+  Value zeroIndex = create.stablehlo.constantI64(0);
+  Value zeroIndex1D = create.onnx.constantInt64({0});
+  Value oneIndex = create.stablehlo.constantI64(1);

   Value boundVal = (direction == FORWARD || direction == BIDIRECTIONAL)
                        ? forwardHt
                        : reverseHt;
   auto valShape = boundVal.getType().cast<ShapedType>().getShape();
-  RankedTensorType sliceType =
-      RankedTensorType::get({1, valShape[0], valShape[1]},
-          boundVal.getType().cast<RankedTensorType>().getElementType());
+  SmallVector<int64_t> sliceSizes = {1, valShape[0], valShape[1]};
+  SmallVector<Value> firstStartIndices = {zeroIndex, zeroIndex, zeroIndex};
+  SmallVector<Value> secondStartIndices = {oneIndex, zeroIndex, zeroIndex};
+
   RankedTensorType valType = boundVal.getType().cast<RankedTensorType>();
   if (direction == FORWARD || direction == BIDIRECTIONAL) {
     if (!isNoneValue(initialH)) {
-      forwardHt = create.onnx.slice(
-          sliceType, initialH, zeroIndex, oneIndex, zeroIndex, oneIndex);
-      forwardHt = create.onnx.squeeze(valType, forwardHt, zeroIndex);
+      forwardHt = create.stablehlo.dynamic_slice(
+          initialH, firstStartIndices, sliceSizes);
+      forwardHt = create.onnx.squeeze(valType, forwardHt, zeroIndex1D);
     }
     if (!onlyHidden && !isNoneValue(initialC)) {
-      forwardCt = create.onnx.slice(
-          sliceType, initialC, zeroIndex, oneIndex, zeroIndex, oneIndex);
-      forwardCt = create.onnx.squeeze(valType, forwardCt, zeroIndex);
+      forwardCt = create.stablehlo.dynamic_slice(
+          initialC, firstStartIndices, sliceSizes);
+      forwardCt = create.onnx.squeeze(valType, forwardCt, zeroIndex1D);
     }
   }
   if (direction == REVERSE || direction == BIDIRECTIONAL) {
     if (!isNoneValue(initialH)) {
       if (direction == REVERSE) {
-        reverseHt = create.onnx.slice(
-            sliceType, initialH, zeroIndex, oneIndex, zeroIndex, oneIndex);
-        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex);
+        reverseHt = create.stablehlo.dynamic_slice(
+            initialH, firstStartIndices, sliceSizes);
+        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex1D);
       } else {
-        reverseHt = create.onnx.slice(
-            sliceType, initialH, oneIndex, twoIndex, zeroIndex, oneIndex);
-        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex);
+        reverseHt = create.stablehlo.dynamic_slice(
+            initialH, secondStartIndices, sliceSizes);
+        reverseHt = create.onnx.squeeze(valType, reverseHt, zeroIndex1D);
       }
     }
     if (!onlyHidden and !isNoneValue(initialC)) {
       if (direction == REVERSE) {
-        reverseCt = create.onnx.slice(
-            sliceType, initialC, zeroIndex, oneIndex, zeroIndex, oneIndex);
-        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex);
+        reverseCt = create.stablehlo.dynamic_slice(
+            initialC, firstStartIndices, sliceSizes);
+        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex1D);
       } else {
-        reverseCt = create.onnx.slice(
-            sliceType, initialC, oneIndex, twoIndex, zeroIndex, oneIndex);
-        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex);
+        reverseCt = create.stablehlo.dynamic_slice(
+            initialC, secondStartIndices, sliceSizes);
+        reverseCt = create.onnx.squeeze(valType, reverseCt, zeroIndex1D);
       }
     }
   }
@@ -213,17 +206,18 @@ Value applyActivation(OpBuilder &rewriter, Location loc,
 /// Create a copy of a slice of X at a specific timestep.
 Value emitXSliceAt(ConversionPatternRewriter &rewriter, Location loc, Value X,
     Value timestepIV) {
-  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  MultiDialectBuilder<OnnxBuilder, StablehloBuilder> create(rewriter, loc);
   int64_t batchSize = dimAt(X, 1);
   int64_t inputSize = dimAt(X, 2);
   Type elementType = X.getType().cast<ShapedType>().getElementType();
-  RankedTensorType sliceXType =
-      RankedTensorType::get({1, batchSize, inputSize}, elementType);
   RankedTensorType squeezedXType =
       RankedTensorType::get({batchSize, inputSize}, elementType);
-  Value sliceX = create.onnx.slice(sliceXType, X, timestepIV,
-      create.onnx.add(timestepIV, create.onnx.constantInt64({1})),
-      create.onnx.constantInt64({0}), create.onnx.constantInt64({1}));
+  SmallVector<int64_t> sliceSizes = {1, batchSize, inputSize};
+  Value zeroIndex = create.stablehlo.constantI64(0);
+  Value timestepIV0D = create.stablehlo.reshape(
+      RankedTensorType::get({}, rewriter.getI64Type()), timestepIV);
+  SmallVector<Value> startIndices = {timestepIV0D, zeroIndex, zeroIndex};
+  Value sliceX = create.stablehlo.dynamic_slice(X, startIndices, sliceSizes);
   sliceX = create.onnx.squeeze(
       squeezedXType, sliceX, create.onnx.constantInt64({0}));
   return sliceX;
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp b/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
index f69e0ac48f..d33ed591c6 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
+++ b/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
@@ -31,6 +31,7 @@ struct ONNXOneHotOpLoweringToStableHlo
       ONNXOneHotOpAdaptor adaptor,
       ConversionPatternRewriter &rewriter) const final {
     Operation *op = onehotOp.getOperation();
+    MLIRContext *context = op->getContext();
     Location loc = ONNXLoc<ONNXOneHotOp>(op);
     ValueRange operands = adaptor.getOperands();
     Value indices = adaptor.getIndices();
@@ -92,24 +93,17 @@ struct ONNXOneHotOpLoweringToStableHlo
         loc, indexType, compareGeZero, broadcastIndices, positiveIndices);
     Value compare = rewriter.create<stablehlo::CompareOp>(
         loc, normalizedIndices, iota, stablehlo::ComparisonDirection::EQ);
-    Type indexElementType = rewriter.getI64Type();
     Type valueType = values.getType().cast<ShapedType>().getElementType();
     Value offValue = rewriter.create<stablehlo::SliceOp>(loc,
         RankedTensorType::get({1}, valueType), values,
-        DenseIntElementsAttr::get(
-            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{0}),
-        DenseIntElementsAttr::get(
-            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
-        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
-            ArrayRef<int64_t>{1}));
+        DenseI64ArrayAttr::get(context, ArrayRef<int64_t>{0}),
+        DenseI64ArrayAttr::get(context, ArrayRef<int64_t>{1}),
+        DenseI64ArrayAttr::get(context, ArrayRef<int64_t>{1}));
     Value onValue = rewriter.create<stablehlo::SliceOp>(loc,
         RankedTensorType::get({1}, valueType), values,
-        DenseIntElementsAttr::get(
-            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
-        DenseIntElementsAttr::get(
-            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{2}),
-        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
-            ArrayRef<int64_t>{1}));
+        DenseI64ArrayAttr::get(context, ArrayRef<int64_t>{1}),
+        DenseI64ArrayAttr::get(context, ArrayRef<int64_t>{2}),
+        DenseI64ArrayAttr::get(context, ArrayRef<int64_t>{1}));
     Value offValueBroadcast = rewriter.create<stablehlo::BroadcastInDimOp>(
         loc, outputType, offValue, rewriter.getI64TensorAttr({0}));
     Value onValueBroadcast = rewriter.create<stablehlo::BroadcastInDimOp>(
diff --git a/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM-loop.mlir b/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM-loop.mlir
index c6414342bb..c1e63c87eb 100644
--- a/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM-loop.mlir
+++ b/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM-loop.mlir
@@ -7,615 +7,234 @@ func.func @test_lstm_loop(%arg0 : tensor<128x16x512xf32>, %arg1 : tensor<2x2048x
 // CHECK-LABEL:  func.func @test_lstm_loop
 // CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<128x16x512xf32>, [[PARAM_1_:%.+]]: tensor<2x2048xf32>, [[PARAM_2_:%.+]]: tensor<2x1024x512xf32>, [[PARAM_3_:%.+]]: tensor<2x1024x256xf32>) -> tensor<128x2x16x256xf32> {
 // CHECK-DAG:       [[VAR_0_:%.+]] = shape.const_shape [16, 256] : tensor<2xindex>
-// CHECK-DAG:       [[VAR_1_:%.+]] = shape.const_shape [1, 1] : tensor<2xindex>
-// CHECK-DAG:       [[VAR_2_:%.+]] = shape.const_shape [1, 1, 16, 256] : tensor<4xindex>
-// CHECK-DAG:       [[VAR_3_:%.+]] = shape.const_shape [16, 512] : tensor<2xindex>
-// CHECK-DAG:       [[VAR_4_:%.+]] = shape.const_shape [128, 16, 512] : tensor<3xindex>
-// CHECK-DAG:       [[VAR_5_:%.+]] = shape.const_shape [2048] : tensor<1xindex>
-// CHECK-DAG:       [[VAR_6_:%.+]] = shape.const_shape [1024, 256] : tensor<2xindex>
-// CHECK-DAG:       [[VAR_7_:%.+]] = shape.const_shape [1024, 512] : tensor<2xindex>
-// CHECK-DAG:       [[VAR_8_:%.+]] = shape.const_shape [2, 16, 256] : tensor<3xindex>
+// CHECK-DAG:       [[VAR_1_:%.+]] = shape.const_shape [1] : tensor<1xindex>
+// CHECK-DAG:       [[VAR_2_:%.+]] = shape.const_shape [1, 1] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_3_:%.+]] = shape.const_shape [1, 1, 16, 256] : tensor<4xindex>
+// CHECK-DAG:       [[VAR_4_:%.+]] = shape.const_shape [16, 1024] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_5_:%.+]] = shape.const_shape [16, 512] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_6_:%.+]] = shape.const_shape [2048] : tensor<1xindex>
+// CHECK-DAG:       [[VAR_7_:%.+]] = shape.const_shape [1024, 256] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_8_:%.+]] = shape.const_shape [1024, 512] : tensor<2xindex>
 // CHECK-DAG:       [[VAR_9_:%.+]] = stablehlo.constant dense<127> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_10_:%.+]] = stablehlo.constant dense<1024> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_11_:%.+]] = stablehlo.constant dense<768> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_12_:%.+]] = shape.const_shape [16, 1024] : tensor<2xindex>
-// CHECK-DAG:       [[VAR_13_:%.+]] = stablehlo.constant dense<512> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_14_:%.+]] = shape.const_shape [1] : tensor<1xindex>
-// CHECK-DAG:       [[VAR_15_:%.+]] = stablehlo.constant dense<128> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_16_:%.+]] = stablehlo.constant dense<256> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_17_:%.+]] = stablehlo.constant dense<16> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_18_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<2x16x256xf32>
-// CHECK-DAG:       [[VAR_19_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<128x1x16x256xf32>
-// CHECK-DAG:       [[VAR_20_:%.+]] = stablehlo.constant dense<0> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_21_:%.+]] = stablehlo.constant dense<1> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_22_:%.+]] = stablehlo.constant dense<2> : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_23_:%.+]] = stablehlo.slice [[VAR_20_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_24_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_25_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_26_:%.+]] = stablehlo.compare  LT, [[VAR_24_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_27_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_26_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
-// CHECK-DAG:       [[VAR_28_:%.+]] = stablehlo.negate [[VAR_24_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_29_:%.+]] = stablehlo.add [[VAR_25_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_30_:%.+]] = stablehlo.add [[VAR_23_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_31_:%.+]] = stablehlo.reverse [[VAR_18_]], dims = [0] : tensor<2x16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_32_:%.+]] = stablehlo.select [[VAR_26_]], [[VAR_29_]], [[VAR_23_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_33_:%.+]] = stablehlo.select [[VAR_26_]], [[VAR_30_]], [[VAR_25_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_34_:%.+]] = stablehlo.select [[VAR_26_]], [[VAR_28_]], [[VAR_24_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_35_:%.+]] = stablehlo.select [[VAR_27_]], [[VAR_31_]], [[VAR_18_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
-// CHECK:           [[VAR_36_:%.+]] = stablehlo.compare  GT, [[VAR_33_]], [[VAR_22_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_37_:%.+]] = stablehlo.select [[VAR_36_]], [[VAR_22_]], [[VAR_33_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_38_:%.+]] = stablehlo.compare  LT, [[VAR_37_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_39_:%.+]] = stablehlo.add [[VAR_37_]], [[VAR_22_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_40_:%.+]] = stablehlo.select [[VAR_38_]], [[VAR_39_]], [[VAR_37_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_41_:%.+]] = stablehlo.compare  LT, [[VAR_32_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_42_:%.+]] = stablehlo.add [[VAR_32_]], [[VAR_22_]] : tensor<1xi64>
-// CHECK:           [[VAR_43_:%.+]] = stablehlo.select [[VAR_41_]], [[VAR_42_]], [[VAR_32_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_44_:%.+]] = stablehlo.concatenate [[VAR_43_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_45_:%.+]] = stablehlo.concatenate [[VAR_40_]], [[VAR_17_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_46_:%.+]] = stablehlo.concatenate [[VAR_34_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_47_:%.+]] = stablehlo.real_dynamic_slice [[VAR_35_]], [[VAR_44_]], [[VAR_45_]], [[VAR_46_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
-// CHECK-DAG:       [[VAR_48_:%.+]] = stablehlo.dynamic_reshape [[VAR_47_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_49_:%.+]] = stablehlo.slice [[VAR_20_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_50_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_51_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_52_:%.+]] = stablehlo.compare  LT, [[VAR_50_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_53_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_52_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
-// CHECK-DAG:       [[VAR_54_:%.+]] = stablehlo.negate [[VAR_50_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_55_:%.+]] = stablehlo.add [[VAR_51_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_56_:%.+]] = stablehlo.add [[VAR_49_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_57_:%.+]] = stablehlo.reverse [[VAR_18_]], dims = [0] : tensor<2x16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_58_:%.+]] = stablehlo.select [[VAR_52_]], [[VAR_55_]], [[VAR_49_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_59_:%.+]] = stablehlo.select [[VAR_52_]], [[VAR_56_]], [[VAR_51_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_60_:%.+]] = stablehlo.select [[VAR_52_]], [[VAR_54_]], [[VAR_50_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_61_:%.+]] = stablehlo.select [[VAR_53_]], [[VAR_57_]], [[VAR_18_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
-// CHECK:           [[VAR_62_:%.+]] = stablehlo.compare  GT, [[VAR_59_]], [[VAR_22_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_63_:%.+]] = stablehlo.select [[VAR_62_]], [[VAR_22_]], [[VAR_59_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_64_:%.+]] = stablehlo.compare  LT, [[VAR_63_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_65_:%.+]] = stablehlo.add [[VAR_63_]], [[VAR_22_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_66_:%.+]] = stablehlo.select [[VAR_64_]], [[VAR_65_]], [[VAR_63_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_67_:%.+]] = stablehlo.compare  LT, [[VAR_58_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_68_:%.+]] = stablehlo.add [[VAR_58_]], [[VAR_22_]] : tensor<1xi64>
-// CHECK:           [[VAR_69_:%.+]] = stablehlo.select [[VAR_67_]], [[VAR_68_]], [[VAR_58_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_70_:%.+]] = stablehlo.concatenate [[VAR_69_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_71_:%.+]] = stablehlo.concatenate [[VAR_66_]], [[VAR_17_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_72_:%.+]] = stablehlo.concatenate [[VAR_60_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_73_:%.+]] = stablehlo.real_dynamic_slice [[VAR_61_]], [[VAR_70_]], [[VAR_71_]], [[VAR_72_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
-// CHECK-DAG:       [[VAR_74_:%.+]] = stablehlo.dynamic_reshape [[VAR_73_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_75_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_76_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_77_:%.+]] = stablehlo.slice [[VAR_22_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_78_:%.+]] = stablehlo.compare  LT, [[VAR_76_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_79_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_78_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
-// CHECK-DAG:       [[VAR_80_:%.+]] = stablehlo.negate [[VAR_76_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_81_:%.+]] = stablehlo.add [[VAR_77_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_82_:%.+]] = stablehlo.add [[VAR_75_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_83_:%.+]] = stablehlo.reverse [[VAR_18_]], dims = [0] : tensor<2x16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_84_:%.+]] = stablehlo.select [[VAR_78_]], [[VAR_81_]], [[VAR_75_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_85_:%.+]] = stablehlo.select [[VAR_78_]], [[VAR_82_]], [[VAR_77_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_86_:%.+]] = stablehlo.select [[VAR_78_]], [[VAR_80_]], [[VAR_76_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_87_:%.+]] = stablehlo.select [[VAR_79_]], [[VAR_83_]], [[VAR_18_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
-// CHECK:           [[VAR_88_:%.+]] = stablehlo.compare  GT, [[VAR_85_]], [[VAR_22_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_89_:%.+]] = stablehlo.select [[VAR_88_]], [[VAR_22_]], [[VAR_85_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_90_:%.+]] = stablehlo.compare  LT, [[VAR_89_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_91_:%.+]] = stablehlo.add [[VAR_89_]], [[VAR_22_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_92_:%.+]] = stablehlo.select [[VAR_90_]], [[VAR_91_]], [[VAR_89_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_93_:%.+]] = stablehlo.compare  LT, [[VAR_84_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_94_:%.+]] = stablehlo.add [[VAR_84_]], [[VAR_22_]] : tensor<1xi64>
-// CHECK:           [[VAR_95_:%.+]] = stablehlo.select [[VAR_93_]], [[VAR_94_]], [[VAR_84_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_96_:%.+]] = stablehlo.concatenate [[VAR_95_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_97_:%.+]] = stablehlo.concatenate [[VAR_92_]], [[VAR_17_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_98_:%.+]] = stablehlo.concatenate [[VAR_86_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_99_:%.+]] = stablehlo.real_dynamic_slice [[VAR_87_]], [[VAR_96_]], [[VAR_97_]], [[VAR_98_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
-// CHECK-DAG:       [[VAR_100_:%.+]] = stablehlo.dynamic_reshape [[VAR_99_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_101_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_102_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_103_:%.+]] = stablehlo.slice [[VAR_22_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_104_:%.+]] = stablehlo.compare  LT, [[VAR_102_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_105_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_104_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
-// CHECK-DAG:       [[VAR_106_:%.+]] = stablehlo.negate [[VAR_102_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_107_:%.+]] = stablehlo.add [[VAR_103_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_108_:%.+]] = stablehlo.add [[VAR_101_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_109_:%.+]] = stablehlo.reverse [[VAR_18_]], dims = [0] : tensor<2x16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_110_:%.+]] = stablehlo.select [[VAR_104_]], [[VAR_107_]], [[VAR_101_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_111_:%.+]] = stablehlo.select [[VAR_104_]], [[VAR_108_]], [[VAR_103_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_112_:%.+]] = stablehlo.select [[VAR_104_]], [[VAR_106_]], [[VAR_102_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_113_:%.+]] = stablehlo.select [[VAR_105_]], [[VAR_109_]], [[VAR_18_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
-// CHECK:           [[VAR_114_:%.+]] = stablehlo.compare  GT, [[VAR_111_]], [[VAR_22_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_115_:%.+]] = stablehlo.select [[VAR_114_]], [[VAR_22_]], [[VAR_111_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_116_:%.+]] = stablehlo.compare  LT, [[VAR_115_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_117_:%.+]] = stablehlo.add [[VAR_115_]], [[VAR_22_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_118_:%.+]] = stablehlo.select [[VAR_116_]], [[VAR_117_]], [[VAR_115_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_119_:%.+]] = stablehlo.compare  LT, [[VAR_110_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_120_:%.+]] = stablehlo.add [[VAR_110_]], [[VAR_22_]] : tensor<1xi64>
-// CHECK:           [[VAR_121_:%.+]] = stablehlo.select [[VAR_119_]], [[VAR_120_]], [[VAR_110_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_122_:%.+]] = stablehlo.concatenate [[VAR_121_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_123_:%.+]] = stablehlo.concatenate [[VAR_118_]], [[VAR_17_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_124_:%.+]] = stablehlo.concatenate [[VAR_112_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_125_:%.+]] = stablehlo.real_dynamic_slice [[VAR_113_]], [[VAR_122_]], [[VAR_123_]], [[VAR_124_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
-// CHECK-DAG:       [[VAR_126_:%.+]] = stablehlo.dynamic_reshape [[VAR_125_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_127_:%.+]] = stablehlo.slice [[PARAM_2_]] [0:1, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
-// CHECK-DAG:       [[VAR_128_:%.+]] = stablehlo.slice [[PARAM_2_]] [1:2, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_129_:%.+]] = stablehlo.dynamic_reshape [[VAR_127_]], [[VAR_7_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
-// CHECK-DAG:       [[VAR_130_:%.+]] = stablehlo.dynamic_reshape [[VAR_128_]], [[VAR_7_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
-// CHECK-DAG:       [[VAR_131_:%.+]] = stablehlo.slice [[PARAM_3_]] [0:1, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
-// CHECK-DAG:       [[VAR_132_:%.+]] = stablehlo.slice [[PARAM_3_]] [1:2, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_133_:%.+]] = stablehlo.dynamic_reshape [[VAR_131_]], [[VAR_6_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
-// CHECK-DAG:       [[VAR_134_:%.+]] = stablehlo.dynamic_reshape [[VAR_132_]], [[VAR_6_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
-// CHECK-DAG:       [[VAR_135_:%.+]] = stablehlo.transpose [[VAR_129_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_136_:%.+]] = stablehlo.transpose [[VAR_133_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
-// CHECK-DAG:       [[VAR_137_:%.+]] = stablehlo.transpose [[VAR_130_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
-// CHECK-DAG:       [[VAR_138_:%.+]] = stablehlo.transpose [[VAR_134_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
-// CHECK-DAG:       [[VAR_139_:%.+]] = stablehlo.slice [[PARAM_1_]] [0:1, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
-// CHECK-DAG:       [[VAR_140_:%.+]] = stablehlo.slice [[PARAM_1_]] [1:2, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_141_:%.+]] = stablehlo.dynamic_reshape [[VAR_139_]], [[VAR_5_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
-// CHECK-DAG:       [[VAR_142_:%.+]] = stablehlo.dynamic_reshape [[VAR_140_]], [[VAR_5_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_143_:%.+]] = stablehlo.slice [[VAR_141_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_144_:%.+]] = stablehlo.slice [[VAR_141_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_145_:%.+]] = stablehlo.slice [[VAR_141_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_146_:%.+]] = stablehlo.slice [[VAR_141_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_147_:%.+]] = stablehlo.slice [[VAR_141_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_148_:%.+]] = stablehlo.slice [[VAR_141_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_149_:%.+]] = stablehlo.slice [[VAR_141_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_150_:%.+]] = stablehlo.slice [[VAR_141_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_151_:%.+]] = stablehlo.slice [[VAR_142_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_152_:%.+]] = stablehlo.slice [[VAR_142_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_153_:%.+]] = stablehlo.slice [[VAR_142_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_154_:%.+]] = stablehlo.slice [[VAR_142_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_155_:%.+]] = stablehlo.slice [[VAR_142_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_156_:%.+]] = stablehlo.slice [[VAR_142_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_157_:%.+]] = stablehlo.slice [[VAR_142_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_158_:%.+]] = stablehlo.slice [[VAR_142_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_159_:%.+]]:4 = stablehlo.while([[VAR_iterArg_:%.+]] = [[VAR_20_]], [[VAR_iterArg_0_:%.+]] = [[VAR_19_]], [[VAR_iterArg_1_:%.+]] = [[VAR_48_]], [[VAR_iterArg_2_:%.+]] = [[VAR_74_]]) : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_10_:%.+]] = stablehlo.constant dense<1> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_11_:%.+]] = stablehlo.constant dense<768> : tensor<i64>
+// CHECK-DAG:       [[VAR_12_:%.+]] = stablehlo.constant dense<512> : tensor<i64>
+// CHECK-DAG:       [[VAR_13_:%.+]] = stablehlo.constant dense<256> : tensor<i64>
+// CHECK-DAG:       [[VAR_14_:%.+]] = stablehlo.constant dense<128> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_15_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<2x16x256xf32>
+// CHECK-DAG:       [[VAR_16_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<128x1x16x256xf32>
+// CHECK-DAG:       [[VAR_17_:%.+]] = stablehlo.constant dense<0> : tensor<i64>
+// CHECK-DAG:       [[VAR_18_:%.+]] = stablehlo.constant dense<0> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_19_:%.+]] = stablehlo.constant dense<1> : tensor<i64>
+// CHECK:           [[VAR_20_:%.+]] = stablehlo.dynamic_slice [[VAR_15_]], [[VAR_17_]], [[VAR_17_]], [[VAR_17_]], sizes = [1, 16, 256] : (tensor<2x16x256xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_21_:%.+]] = stablehlo.dynamic_reshape [[VAR_20_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_22_:%.+]] = stablehlo.dynamic_slice [[VAR_15_]], [[VAR_17_]], [[VAR_17_]], [[VAR_17_]], sizes = [1, 16, 256] : (tensor<2x16x256xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_23_:%.+]] = stablehlo.dynamic_reshape [[VAR_22_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_24_:%.+]] = stablehlo.dynamic_slice [[VAR_15_]], [[VAR_19_]], [[VAR_17_]], [[VAR_17_]], sizes = [1, 16, 256] : (tensor<2x16x256xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_25_:%.+]] = stablehlo.dynamic_reshape [[VAR_24_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_26_:%.+]] = stablehlo.dynamic_slice [[VAR_15_]], [[VAR_19_]], [[VAR_17_]], [[VAR_17_]], sizes = [1, 16, 256] : (tensor<2x16x256xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_27_:%.+]] = stablehlo.dynamic_reshape [[VAR_26_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_28_:%.+]] = stablehlo.slice [[PARAM_2_]] [0:1, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-DAG:       [[VAR_29_:%.+]] = stablehlo.slice [[PARAM_2_]] [1:2, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_30_:%.+]] = stablehlo.dynamic_reshape [[VAR_28_]], [[VAR_8_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_31_:%.+]] = stablehlo.dynamic_reshape [[VAR_29_]], [[VAR_8_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_32_:%.+]] = stablehlo.slice [[PARAM_3_]] [0:1, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-DAG:       [[VAR_33_:%.+]] = stablehlo.slice [[PARAM_3_]] [1:2, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_34_:%.+]] = stablehlo.dynamic_reshape [[VAR_32_]], [[VAR_7_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_35_:%.+]] = stablehlo.dynamic_reshape [[VAR_33_]], [[VAR_7_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_36_:%.+]] = stablehlo.transpose [[VAR_30_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_37_:%.+]] = stablehlo.transpose [[VAR_34_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_38_:%.+]] = stablehlo.transpose [[VAR_31_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-DAG:       [[VAR_39_:%.+]] = stablehlo.transpose [[VAR_35_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_40_:%.+]] = stablehlo.slice [[PARAM_1_]] [0:1, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-DAG:       [[VAR_41_:%.+]] = stablehlo.slice [[PARAM_1_]] [1:2, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_42_:%.+]] = stablehlo.dynamic_reshape [[VAR_40_]], [[VAR_6_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
+// CHECK-DAG:       [[VAR_43_:%.+]] = stablehlo.dynamic_reshape [[VAR_41_]], [[VAR_6_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_44_:%.+]] = stablehlo.slice [[VAR_42_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_45_:%.+]] = stablehlo.slice [[VAR_42_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_46_:%.+]] = stablehlo.slice [[VAR_42_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_47_:%.+]] = stablehlo.slice [[VAR_42_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_48_:%.+]] = stablehlo.slice [[VAR_42_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_49_:%.+]] = stablehlo.slice [[VAR_42_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_50_:%.+]] = stablehlo.slice [[VAR_42_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_51_:%.+]] = stablehlo.slice [[VAR_42_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_52_:%.+]] = stablehlo.slice [[VAR_43_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_53_:%.+]] = stablehlo.slice [[VAR_43_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_54_:%.+]] = stablehlo.slice [[VAR_43_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_55_:%.+]] = stablehlo.slice [[VAR_43_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_56_:%.+]] = stablehlo.slice [[VAR_43_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_57_:%.+]] = stablehlo.slice [[VAR_43_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_58_:%.+]] = stablehlo.slice [[VAR_43_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_59_:%.+]] = stablehlo.slice [[VAR_43_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_60_:%.+]]:4 = stablehlo.while([[VAR_iterArg_:%.+]] = [[VAR_18_]], [[VAR_iterArg_0_:%.+]] = [[VAR_16_]], [[VAR_iterArg_1_:%.+]] = [[VAR_21_]], [[VAR_iterArg_2_:%.+]] = [[VAR_23_]]) : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
 // CHECK:            cond {
-// CHECK:             [[VAR_162_:%.+]] = stablehlo.compare  LT, [[VAR_iterArg_]], [[VAR_15_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_163_:%.+]] = stablehlo.reshape [[VAR_162_]] : (tensor<1xi1>) -> tensor<i1>
-// CHECK:             stablehlo.return [[VAR_163_]] : tensor<i1>
+// CHECK:             [[VAR_63_:%.+]] = stablehlo.compare  LT, [[VAR_iterArg_]], [[VAR_14_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_64_:%.+]] = stablehlo.reshape [[VAR_63_]] : (tensor<1xi1>) -> tensor<i1>
+// CHECK:             stablehlo.return [[VAR_64_]] : tensor<i1>
 // CHECK:           } do {
-// CHECK-DAG:         [[VAR_162_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_163_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_21_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_164_:%.+]] = stablehlo.add [[VAR_162_1_]], [[VAR_163_1_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_165_:%.+]] = stablehlo.slice [[VAR_iterArg_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_166_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_167_:%.+]] = stablehlo.slice [[VAR_164_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_168_:%.+]] = stablehlo.compare  LT, [[VAR_166_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_169_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_168_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<128x16x512xi1>
-// CHECK-DAG:         [[VAR_170_:%.+]] = stablehlo.negate [[VAR_166_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_171_:%.+]] = stablehlo.add [[VAR_167_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_172_:%.+]] = stablehlo.add [[VAR_165_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_173_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<128x16x512xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_174_:%.+]] = stablehlo.select [[VAR_168_]], [[VAR_171_]], [[VAR_165_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_175_:%.+]] = stablehlo.select [[VAR_168_]], [[VAR_172_]], [[VAR_167_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_176_:%.+]] = stablehlo.select [[VAR_168_]], [[VAR_170_]], [[VAR_166_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_177_:%.+]] = stablehlo.select [[VAR_169_]], [[VAR_173_]], [[PARAM_0_]] : tensor<128x16x512xi1>, tensor<128x16x512xf32>
-// CHECK:             [[VAR_178_:%.+]] = stablehlo.compare  GT, [[VAR_175_]], [[VAR_15_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_179_:%.+]] = stablehlo.select [[VAR_178_]], [[VAR_15_]], [[VAR_175_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_180_:%.+]] = stablehlo.compare  LT, [[VAR_179_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_181_:%.+]] = stablehlo.add [[VAR_179_]], [[VAR_15_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_182_:%.+]] = stablehlo.select [[VAR_180_]], [[VAR_181_]], [[VAR_179_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_183_:%.+]] = stablehlo.compare  LT, [[VAR_174_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_184_:%.+]] = stablehlo.add [[VAR_174_]], [[VAR_15_]] : tensor<1xi64>
-// CHECK:             [[VAR_185_:%.+]] = stablehlo.select [[VAR_183_]], [[VAR_184_]], [[VAR_174_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_186_:%.+]] = stablehlo.concatenate [[VAR_185_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:         [[VAR_187_:%.+]] = stablehlo.concatenate [[VAR_182_]], [[VAR_17_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:         [[VAR_188_:%.+]] = stablehlo.concatenate [[VAR_176_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:             [[VAR_189_:%.+]] = stablehlo.real_dynamic_slice [[VAR_177_]], [[VAR_186_]], [[VAR_187_]], [[VAR_188_]] : (tensor<128x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
-// CHECK:             [[VAR_190_:%.+]] = stablehlo.dynamic_reshape [[VAR_189_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
-// CHECK-DAG:         [[VAR_191_:%.+]] = stablehlo.broadcast_in_dim [[VAR_190_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
-// CHECK-DAG:         [[VAR_192_:%.+]] = stablehlo.broadcast_in_dim [[VAR_135_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_193_:%.+]] = stablehlo.dot [[VAR_191_]], [[VAR_192_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:         [[VAR_194_:%.+]] = stablehlo.broadcast_in_dim [[VAR_iterArg_1_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_195_:%.+]] = stablehlo.broadcast_in_dim [[VAR_136_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_196_:%.+]] = stablehlo.dot [[VAR_194_]], [[VAR_195_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:         [[VAR_197_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_193_]], [[VAR_12_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK:             [[VAR_198_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_196_]], [[VAR_12_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK-DAG:         [[VAR_199_:%.+]] = stablehlo.add [[VAR_197_]], [[VAR_198_]] : tensor<16x1024xf32>
-// CHECK-DAG:         [[VAR_200_:%.+]] = stablehlo.slice [[VAR_20_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_201_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_202_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:             [[VAR_203_:%.+]] = stablehlo.compare  LT, [[VAR_201_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_204_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_203_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:         [[VAR_205_:%.+]] = stablehlo.negate [[VAR_201_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_206_:%.+]] = stablehlo.add [[VAR_202_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_207_:%.+]] = stablehlo.add [[VAR_200_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_208_:%.+]] = stablehlo.reverse [[VAR_199_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_209_:%.+]] = stablehlo.select [[VAR_203_]], [[VAR_206_]], [[VAR_200_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_210_:%.+]] = stablehlo.select [[VAR_203_]], [[VAR_207_]], [[VAR_202_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_211_:%.+]] = stablehlo.select [[VAR_203_]], [[VAR_205_]], [[VAR_201_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_212_:%.+]] = stablehlo.select [[VAR_204_]], [[VAR_208_]], [[VAR_199_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:             [[VAR_213_:%.+]] = stablehlo.compare  GT, [[VAR_210_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_214_:%.+]] = stablehlo.select [[VAR_213_]], [[VAR_10_]], [[VAR_210_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_215_:%.+]] = stablehlo.compare  LT, [[VAR_214_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_216_:%.+]] = stablehlo.add [[VAR_214_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_217_:%.+]] = stablehlo.select [[VAR_215_]], [[VAR_216_]], [[VAR_214_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_218_:%.+]] = stablehlo.compare  LT, [[VAR_209_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_219_:%.+]] = stablehlo.add [[VAR_209_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK:             [[VAR_220_:%.+]] = stablehlo.select [[VAR_218_]], [[VAR_219_]], [[VAR_209_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_221_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_220_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_222_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_217_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_223_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_21_]]1, dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_224_:%.+]] = stablehlo.real_dynamic_slice [[VAR_212_]], [[VAR_221_]], [[VAR_222_]], [[VAR_223_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_225_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_226_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_227_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:             [[VAR_228_:%.+]] = stablehlo.compare  LT, [[VAR_226_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_229_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_228_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:         [[VAR_230_:%.+]] = stablehlo.negate [[VAR_226_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_231_:%.+]] = stablehlo.add [[VAR_227_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_232_:%.+]] = stablehlo.add [[VAR_225_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_233_:%.+]] = stablehlo.reverse [[VAR_199_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_234_:%.+]] = stablehlo.select [[VAR_228_]], [[VAR_231_]], [[VAR_225_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_235_:%.+]] = stablehlo.select [[VAR_228_]], [[VAR_232_]], [[VAR_227_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_236_:%.+]] = stablehlo.select [[VAR_228_]], [[VAR_230_]], [[VAR_226_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_237_:%.+]] = stablehlo.select [[VAR_229_]], [[VAR_233_]], [[VAR_199_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:             [[VAR_238_:%.+]] = stablehlo.compare  GT, [[VAR_235_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_239_:%.+]] = stablehlo.select [[VAR_238_]], [[VAR_10_]], [[VAR_235_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_240_:%.+]] = stablehlo.compare  LT, [[VAR_239_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_241_:%.+]] = stablehlo.add [[VAR_239_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_242_:%.+]] = stablehlo.select [[VAR_240_]], [[VAR_241_]], [[VAR_239_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_243_:%.+]] = stablehlo.compare  LT, [[VAR_234_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_244_:%.+]] = stablehlo.add [[VAR_234_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK:             [[VAR_245_:%.+]] = stablehlo.select [[VAR_243_]], [[VAR_244_]], [[VAR_234_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_246_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_245_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_247_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_242_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_248_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_236_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_249_:%.+]] = stablehlo.real_dynamic_slice [[VAR_237_]], [[VAR_246_]], [[VAR_247_]], [[VAR_248_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_250_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_251_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_252_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:             [[VAR_253_:%.+]] = stablehlo.compare  LT, [[VAR_251_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_254_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_253_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:         [[VAR_255_:%.+]] = stablehlo.negate [[VAR_251_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_256_:%.+]] = stablehlo.add [[VAR_252_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_257_:%.+]] = stablehlo.add [[VAR_250_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_258_:%.+]] = stablehlo.reverse [[VAR_199_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_259_:%.+]] = stablehlo.select [[VAR_253_]], [[VAR_256_]], [[VAR_250_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_260_:%.+]] = stablehlo.select [[VAR_253_]], [[VAR_257_]], [[VAR_252_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_261_:%.+]] = stablehlo.select [[VAR_253_]], [[VAR_255_]], [[VAR_251_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_262_:%.+]] = stablehlo.select [[VAR_254_]], [[VAR_258_]], [[VAR_199_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:             [[VAR_263_:%.+]] = stablehlo.compare  GT, [[VAR_260_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_264_:%.+]] = stablehlo.select [[VAR_263_]], [[VAR_10_]], [[VAR_260_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_265_:%.+]] = stablehlo.compare  LT, [[VAR_264_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_266_:%.+]] = stablehlo.add [[VAR_264_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_267_:%.+]] = stablehlo.select [[VAR_265_]], [[VAR_266_]], [[VAR_264_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_268_:%.+]] = stablehlo.compare  LT, [[VAR_259_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_269_:%.+]] = stablehlo.add [[VAR_259_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK:             [[VAR_270_:%.+]] = stablehlo.select [[VAR_268_]], [[VAR_269_]], [[VAR_259_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_271_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_270_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_272_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_267_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_273_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_261_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_274_:%.+]] = stablehlo.real_dynamic_slice [[VAR_262_]], [[VAR_271_]], [[VAR_272_]], [[VAR_273_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_275_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_276_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_277_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:             [[VAR_278_:%.+]] = stablehlo.compare  LT, [[VAR_276_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_279_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_278_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:         [[VAR_280_:%.+]] = stablehlo.negate [[VAR_276_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_281_:%.+]] = stablehlo.add [[VAR_277_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_282_:%.+]] = stablehlo.add [[VAR_275_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_283_:%.+]] = stablehlo.reverse [[VAR_199_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_284_:%.+]] = stablehlo.select [[VAR_278_]], [[VAR_281_]], [[VAR_275_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_285_:%.+]] = stablehlo.select [[VAR_278_]], [[VAR_282_]], [[VAR_277_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_286_:%.+]] = stablehlo.select [[VAR_278_]], [[VAR_280_]], [[VAR_276_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_287_:%.+]] = stablehlo.select [[VAR_279_]], [[VAR_283_]], [[VAR_199_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:             [[VAR_288_:%.+]] = stablehlo.compare  GT, [[VAR_285_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_289_:%.+]] = stablehlo.select [[VAR_288_]], [[VAR_10_]], [[VAR_285_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_290_:%.+]] = stablehlo.compare  LT, [[VAR_289_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_291_:%.+]] = stablehlo.add [[VAR_289_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_292_:%.+]] = stablehlo.select [[VAR_290_]], [[VAR_291_]], [[VAR_289_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_293_:%.+]] = stablehlo.compare  LT, [[VAR_284_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_294_:%.+]] = stablehlo.add [[VAR_284_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK:             [[VAR_295_:%.+]] = stablehlo.select [[VAR_293_]], [[VAR_294_]], [[VAR_284_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_296_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_295_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_297_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_292_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_298_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_286_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_299_:%.+]] = stablehlo.real_dynamic_slice [[VAR_287_]], [[VAR_296_]], [[VAR_297_]], [[VAR_298_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_300_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_224_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_301_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_143_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_302_:%.+]] = stablehlo.add [[VAR_300_]], [[VAR_301_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_303_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_302_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_304_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_147_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_305_:%.+]] = stablehlo.add [[VAR_303_]], [[VAR_304_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_306_:%.+]] = stablehlo.logistic [[VAR_305_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_307_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_274_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_308_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_145_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_309_:%.+]] = stablehlo.add [[VAR_307_]], [[VAR_308_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_310_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_309_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_311_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_149_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_312_:%.+]] = stablehlo.add [[VAR_310_]], [[VAR_311_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_313_:%.+]] = stablehlo.logistic [[VAR_312_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_314_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_299_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_315_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_146_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_316_:%.+]] = stablehlo.add [[VAR_314_]], [[VAR_315_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_317_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_316_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_318_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_150_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_319_:%.+]] = stablehlo.add [[VAR_317_]], [[VAR_318_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_320_:%.+]] = stablehlo.tanh [[VAR_319_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_321_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_313_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_322_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_2_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_323_:%.+]] = stablehlo.multiply [[VAR_321_]], [[VAR_322_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_324_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_306_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_325_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_320_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_326_:%.+]] = stablehlo.multiply [[VAR_324_]], [[VAR_325_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_327_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_323_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_328_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_326_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_329_:%.+]] = stablehlo.add [[VAR_327_]], [[VAR_328_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_330_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_249_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_331_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_144_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_332_:%.+]] = stablehlo.add [[VAR_330_]], [[VAR_331_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_333_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_332_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_334_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_148_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_335_:%.+]] = stablehlo.add [[VAR_333_]], [[VAR_334_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_336_:%.+]] = stablehlo.logistic [[VAR_335_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_337_:%.+]] = stablehlo.tanh [[VAR_329_]] : tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_338_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_336_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_339_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_337_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_340_:%.+]] = stablehlo.multiply [[VAR_338_]], [[VAR_339_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_341_:%.+]] = stablehlo.dynamic_reshape [[VAR_340_]], [[VAR_2_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
-// CHECK-DAG:         [[VAR_342_:%.+]] = stablehlo.dynamic_reshape [[VAR_iterArg_]], [[VAR_1_]] : (tensor<1xi64>, tensor<2xindex>) -> tensor<1x1xi64>
-// CHECK:             [[VAR_343_:%.+]] = "stablehlo.scatter"([[VAR_iterArg_0_]], [[VAR_342_]], [[VAR_341_]]) ({
+// CHECK:             [[VAR_63_1_:%.+]] = stablehlo.reshape [[VAR_iterArg_]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:             [[VAR_64_1_:%.+]] = stablehlo.dynamic_slice [[PARAM_0_]], [[VAR_63_1_]], [[VAR_17_]], [[VAR_17_]], sizes = [1, 16, 512] : (tensor<128x16x512xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x512xf32>
+// CHECK:             [[VAR_65_:%.+]] = stablehlo.dynamic_reshape [[VAR_64_1_]], [[VAR_5_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_66_:%.+]] = stablehlo.broadcast_in_dim [[VAR_65_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_67_:%.+]] = stablehlo.broadcast_in_dim [[VAR_36_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_68_:%.+]] = stablehlo.dot [[VAR_66_]], [[VAR_67_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_69_:%.+]] = stablehlo.broadcast_in_dim [[VAR_iterArg_1_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_70_:%.+]] = stablehlo.broadcast_in_dim [[VAR_37_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_71_:%.+]] = stablehlo.dot [[VAR_69_]], [[VAR_70_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_72_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_68_]], [[VAR_4_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:             [[VAR_73_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_71_]], [[VAR_4_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:             [[VAR_74_:%.+]] = stablehlo.add [[VAR_72_]], [[VAR_73_]] : tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_75_:%.+]] = stablehlo.dynamic_slice [[VAR_74_]], [[VAR_17_]], [[VAR_17_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_76_:%.+]] = stablehlo.dynamic_slice [[VAR_74_]], [[VAR_17_]], [[VAR_13_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_77_:%.+]] = stablehlo.dynamic_slice [[VAR_74_]], [[VAR_17_]], [[VAR_12_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_78_:%.+]] = stablehlo.dynamic_slice [[VAR_74_]], [[VAR_17_]], [[VAR_11_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_79_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_75_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_80_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_44_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_81_:%.+]] = stablehlo.add [[VAR_79_]], [[VAR_80_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_82_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_81_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_83_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_48_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_84_:%.+]] = stablehlo.add [[VAR_82_]], [[VAR_83_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_85_:%.+]] = stablehlo.logistic [[VAR_84_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_86_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_77_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_87_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_46_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_88_:%.+]] = stablehlo.add [[VAR_86_]], [[VAR_87_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_89_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_88_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_90_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_50_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_91_:%.+]] = stablehlo.add [[VAR_89_]], [[VAR_90_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_92_:%.+]] = stablehlo.logistic [[VAR_91_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_93_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_78_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_94_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_47_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_95_:%.+]] = stablehlo.add [[VAR_93_]], [[VAR_94_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_96_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_95_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_97_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_51_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_98_:%.+]] = stablehlo.add [[VAR_96_]], [[VAR_97_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_99_:%.+]] = stablehlo.tanh [[VAR_98_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_100_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_92_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_101_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_2_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_102_:%.+]] = stablehlo.multiply [[VAR_100_]], [[VAR_101_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_103_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_85_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_104_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_99_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_105_:%.+]] = stablehlo.multiply [[VAR_103_]], [[VAR_104_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_106_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_102_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_107_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_105_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_108_:%.+]] = stablehlo.add [[VAR_106_]], [[VAR_107_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_109_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_76_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_110_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_45_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_111_:%.+]] = stablehlo.add [[VAR_109_]], [[VAR_110_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_112_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_111_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_113_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_49_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_114_:%.+]] = stablehlo.add [[VAR_112_]], [[VAR_113_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_115_:%.+]] = stablehlo.logistic [[VAR_114_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_116_:%.+]] = stablehlo.tanh [[VAR_108_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_117_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_115_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_118_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_116_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_119_:%.+]] = stablehlo.multiply [[VAR_117_]], [[VAR_118_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_120_:%.+]] = stablehlo.dynamic_reshape [[VAR_119_]], [[VAR_3_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:         [[VAR_121_:%.+]] = stablehlo.dynamic_reshape [[VAR_iterArg_]], [[VAR_2_]] : (tensor<1xi64>, tensor<2xindex>) -> tensor<1x1xi64>
+// CHECK:             [[VAR_122_:%.+]] = "stablehlo.scatter"([[VAR_iterArg_0_]], [[VAR_121_]], [[VAR_120_]]) ({
 // CHECK:             ^bb0([[arg4_:%.+]]: tensor<f32>, [[arg5_:%.+]]: tensor<f32>):
 // CHECK:               stablehlo.return [[arg5_]] : tensor<f32>
 // CHECK:             }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2, 3], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<128x1x16x256xf32>, tensor<1x1xi64>, tensor<1x1x16x256xf32>) -> tensor<128x1x16x256xf32>
-// CHECK-DAG:         [[VAR_344_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_345_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_21_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK:             [[VAR_346_:%.+]] = stablehlo.add [[VAR_344_]], [[VAR_345_]] : tensor<1xi64>
-// CHECK:             stablehlo.return [[VAR_346_]], [[VAR_343_]], [[VAR_340_]], [[VAR_329_]] : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_123_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_]], [[VAR_1_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_124_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_10_]], [[VAR_1_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK:             [[VAR_125_:%.+]] = stablehlo.add [[VAR_123_]], [[VAR_124_]] : tensor<1xi64>
+// CHECK:             stablehlo.return [[VAR_125_]], [[VAR_122_]], [[VAR_119_]], [[VAR_108_]] : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
 // CHECK:           }
-// CHECK:           [[VAR_160_:%.+]]:4 = stablehlo.while([[VAR_iterArg_1_:%.+]] = [[VAR_9_]], [[VAR_iterArg_0_1_:%.+]] = [[VAR_19_]], [[VAR_iterArg_1_1_:%.+]] = [[VAR_100_]], [[VAR_iterArg_2_1_:%.+]] = [[VAR_126_]]) : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK:           [[VAR_61_:%.+]]:4 = stablehlo.while([[VAR_iterArg_1_:%.+]] = [[VAR_9_]], [[VAR_iterArg_0_1_:%.+]] = [[VAR_16_]], [[VAR_iterArg_1_1_:%.+]] = [[VAR_25_]], [[VAR_iterArg_2_1_:%.+]] = [[VAR_27_]]) : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
 // CHECK:            cond {
-// CHECK:             [[VAR_162_2_:%.+]] = stablehlo.compare  GE, [[VAR_iterArg_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_163_2_:%.+]] = stablehlo.reshape [[VAR_162_2_]] : (tensor<1xi1>) -> tensor<i1>
-// CHECK:             stablehlo.return [[VAR_163_2_]] : tensor<i1>
+// CHECK:             [[VAR_63_2_:%.+]] = stablehlo.compare  GE, [[VAR_iterArg_1_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
+// CHECK:             [[VAR_64_2_:%.+]] = stablehlo.reshape [[VAR_63_2_]] : (tensor<1xi1>) -> tensor<i1>
+// CHECK:             stablehlo.return [[VAR_64_2_]] : tensor<i1>
 // CHECK:           } do {
-// CHECK-DAG:         [[VAR_162_3_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_1_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_163_3_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_21_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_164_1_:%.+]] = stablehlo.add [[VAR_162_3_]], [[VAR_163_3_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_165_1_:%.+]] = stablehlo.slice [[VAR_iterArg_1_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_166_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_167_1_:%.+]] = stablehlo.slice [[VAR_164_1_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_168_1_:%.+]] = stablehlo.compare  LT, [[VAR_166_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_169_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_168_1_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<128x16x512xi1>
-// CHECK-DAG:         [[VAR_170_1_:%.+]] = stablehlo.negate [[VAR_166_1_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_171_1_:%.+]] = stablehlo.add [[VAR_167_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_172_1_:%.+]] = stablehlo.add [[VAR_165_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_173_1_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<128x16x512xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_174_1_:%.+]] = stablehlo.select [[VAR_168_1_]], [[VAR_171_1_]], [[VAR_165_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_175_1_:%.+]] = stablehlo.select [[VAR_168_1_]], [[VAR_172_1_]], [[VAR_167_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_176_1_:%.+]] = stablehlo.select [[VAR_168_1_]], [[VAR_170_1_]], [[VAR_166_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_177_1_:%.+]] = stablehlo.select [[VAR_169_1_]], [[VAR_173_1_]], [[PARAM_0_]] : tensor<128x16x512xi1>, tensor<128x16x512xf32>
-// CHECK:             [[VAR_178_1_:%.+]] = stablehlo.compare  GT, [[VAR_175_1_]], [[VAR_15_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_179_1_:%.+]] = stablehlo.select [[VAR_178_1_]], [[VAR_15_]], [[VAR_175_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_180_1_:%.+]] = stablehlo.compare  LT, [[VAR_179_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_181_1_:%.+]] = stablehlo.add [[VAR_179_1_]], [[VAR_15_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_182_1_:%.+]] = stablehlo.select [[VAR_180_1_]], [[VAR_181_1_]], [[VAR_179_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_183_1_:%.+]] = stablehlo.compare  LT, [[VAR_174_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_184_1_:%.+]] = stablehlo.add [[VAR_174_1_]], [[VAR_15_]] : tensor<1xi64>
-// CHECK:             [[VAR_185_1_:%.+]] = stablehlo.select [[VAR_183_1_]], [[VAR_184_1_]], [[VAR_174_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_186_1_:%.+]] = stablehlo.concatenate [[VAR_185_1_]], [[VAR_20_]], [[VAR_20_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:         [[VAR_187_1_:%.+]] = stablehlo.concatenate [[VAR_182_1_]], [[VAR_17_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:         [[VAR_188_1_:%.+]] = stablehlo.concatenate [[VAR_176_1_]], [[VAR_21_]], [[VAR_21_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:             [[VAR_189_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_177_1_]], [[VAR_186_1_]], [[VAR_187_1_]], [[VAR_188_1_]] : (tensor<128x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
-// CHECK:             [[VAR_190_1_:%.+]] = stablehlo.dynamic_reshape [[VAR_189_1_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
-// CHECK-DAG:         [[VAR_191_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_190_1_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
-// CHECK-DAG:         [[VAR_192_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_137_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_193_1_:%.+]] = stablehlo.dot [[VAR_191_1_]], [[VAR_192_1_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:         [[VAR_194_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_iterArg_1_1_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_195_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_138_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_196_1_:%.+]] = stablehlo.dot [[VAR_194_1_]], [[VAR_195_1_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:         [[VAR_197_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_193_1_]], [[VAR_12_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK:             [[VAR_198_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_196_1_]], [[VAR_12_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK-DAG:         [[VAR_199_1_:%.+]] = stablehlo.add [[VAR_197_1_]], [[VAR_198_1_]] : tensor<16x1024xf32>
-// CHECK-DAG:         [[VAR_200_1_:%.+]] = stablehlo.slice [[VAR_20_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_201_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_202_1_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:             [[VAR_203_1_:%.+]] = stablehlo.compare  LT, [[VAR_201_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_204_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_203_1_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:         [[VAR_205_1_:%.+]] = stablehlo.negate [[VAR_201_1_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_206_1_:%.+]] = stablehlo.add [[VAR_202_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_207_1_:%.+]] = stablehlo.add [[VAR_200_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_208_1_:%.+]] = stablehlo.reverse [[VAR_199_1_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_209_1_:%.+]] = stablehlo.select [[VAR_203_1_]], [[VAR_206_1_]], [[VAR_200_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_210_1_:%.+]] = stablehlo.select [[VAR_203_1_]], [[VAR_207_1_]], [[VAR_202_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_211_1_:%.+]] = stablehlo.select [[VAR_203_1_]], [[VAR_205_1_]], [[VAR_201_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_212_1_:%.+]] = stablehlo.select [[VAR_204_1_]], [[VAR_208_1_]], [[VAR_199_1_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:             [[VAR_213_1_:%.+]] = stablehlo.compare  GT, [[VAR_210_1_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_214_1_:%.+]] = stablehlo.select [[VAR_213_1_]], [[VAR_10_]], [[VAR_210_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_215_1_:%.+]] = stablehlo.compare  LT, [[VAR_214_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_216_1_:%.+]] = stablehlo.add [[VAR_214_1_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_217_1_:%.+]] = stablehlo.select [[VAR_215_1_]], [[VAR_216_1_]], [[VAR_214_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_218_1_:%.+]] = stablehlo.compare  LT, [[VAR_209_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_219_1_:%.+]] = stablehlo.add [[VAR_209_1_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK:             [[VAR_220_1_:%.+]] = stablehlo.select [[VAR_218_1_]], [[VAR_219_1_]], [[VAR_209_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_221_1_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_220_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_222_1_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_217_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_223_1_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_21_]]1, dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_224_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_212_1_]], [[VAR_221_1_]], [[VAR_222_1_]], [[VAR_223_1_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_225_1_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_226_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_227_1_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:             [[VAR_228_1_:%.+]] = stablehlo.compare  LT, [[VAR_226_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_229_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_228_1_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:         [[VAR_230_1_:%.+]] = stablehlo.negate [[VAR_226_1_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_231_1_:%.+]] = stablehlo.add [[VAR_227_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_232_1_:%.+]] = stablehlo.add [[VAR_225_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_233_1_:%.+]] = stablehlo.reverse [[VAR_199_1_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_234_1_:%.+]] = stablehlo.select [[VAR_228_1_]], [[VAR_231_1_]], [[VAR_225_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_235_1_:%.+]] = stablehlo.select [[VAR_228_1_]], [[VAR_232_1_]], [[VAR_227_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_236_1_:%.+]] = stablehlo.select [[VAR_228_1_]], [[VAR_230_1_]], [[VAR_226_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_237_1_:%.+]] = stablehlo.select [[VAR_229_1_]], [[VAR_233_1_]], [[VAR_199_1_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:             [[VAR_238_1_:%.+]] = stablehlo.compare  GT, [[VAR_235_1_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_239_1_:%.+]] = stablehlo.select [[VAR_238_1_]], [[VAR_10_]], [[VAR_235_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_240_1_:%.+]] = stablehlo.compare  LT, [[VAR_239_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_241_1_:%.+]] = stablehlo.add [[VAR_239_1_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_242_1_:%.+]] = stablehlo.select [[VAR_240_1_]], [[VAR_241_1_]], [[VAR_239_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_243_1_:%.+]] = stablehlo.compare  LT, [[VAR_234_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_244_1_:%.+]] = stablehlo.add [[VAR_234_1_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK:             [[VAR_245_1_:%.+]] = stablehlo.select [[VAR_243_1_]], [[VAR_244_1_]], [[VAR_234_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_246_1_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_245_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_247_1_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_242_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_248_1_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_236_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_249_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_237_1_]], [[VAR_246_1_]], [[VAR_247_1_]], [[VAR_248_1_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_250_1_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_251_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_252_1_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:             [[VAR_253_1_:%.+]] = stablehlo.compare  LT, [[VAR_251_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_254_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_253_1_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:         [[VAR_255_1_:%.+]] = stablehlo.negate [[VAR_251_1_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_256_1_:%.+]] = stablehlo.add [[VAR_252_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_257_1_:%.+]] = stablehlo.add [[VAR_250_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_258_1_:%.+]] = stablehlo.reverse [[VAR_199_1_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_259_1_:%.+]] = stablehlo.select [[VAR_253_1_]], [[VAR_256_1_]], [[VAR_250_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_260_1_:%.+]] = stablehlo.select [[VAR_253_1_]], [[VAR_257_1_]], [[VAR_252_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_261_1_:%.+]] = stablehlo.select [[VAR_253_1_]], [[VAR_255_1_]], [[VAR_251_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_262_1_:%.+]] = stablehlo.select [[VAR_254_1_]], [[VAR_258_1_]], [[VAR_199_1_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:             [[VAR_263_1_:%.+]] = stablehlo.compare  GT, [[VAR_260_1_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_264_1_:%.+]] = stablehlo.select [[VAR_263_1_]], [[VAR_10_]], [[VAR_260_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_265_1_:%.+]] = stablehlo.compare  LT, [[VAR_264_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_266_1_:%.+]] = stablehlo.add [[VAR_264_1_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_267_1_:%.+]] = stablehlo.select [[VAR_265_1_]], [[VAR_266_1_]], [[VAR_264_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_268_1_:%.+]] = stablehlo.compare  LT, [[VAR_259_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_269_1_:%.+]] = stablehlo.add [[VAR_259_1_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK:             [[VAR_270_1_:%.+]] = stablehlo.select [[VAR_268_1_]], [[VAR_269_1_]], [[VAR_259_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_271_1_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_270_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_272_1_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_267_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_273_1_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_261_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_274_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_262_1_]], [[VAR_271_1_]], [[VAR_272_1_]], [[VAR_273_1_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_275_1_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_276_1_:%.+]] = stablehlo.slice [[VAR_21_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_277_1_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:             [[VAR_278_1_:%.+]] = stablehlo.compare  LT, [[VAR_276_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_279_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_278_1_]], [[VAR_12_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:         [[VAR_280_1_:%.+]] = stablehlo.negate [[VAR_276_1_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_281_1_:%.+]] = stablehlo.add [[VAR_277_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_282_1_:%.+]] = stablehlo.add [[VAR_275_1_]], [[VAR_21_]] : tensor<1xi64>
-// CHECK-DAG:         [[VAR_283_1_:%.+]] = stablehlo.reverse [[VAR_199_1_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_284_1_:%.+]] = stablehlo.select [[VAR_278_1_]], [[VAR_281_1_]], [[VAR_275_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_285_1_:%.+]] = stablehlo.select [[VAR_278_1_]], [[VAR_282_1_]], [[VAR_277_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_286_1_:%.+]] = stablehlo.select [[VAR_278_1_]], [[VAR_280_1_]], [[VAR_276_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_287_1_:%.+]] = stablehlo.select [[VAR_279_1_]], [[VAR_283_1_]], [[VAR_199_1_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:             [[VAR_288_1_:%.+]] = stablehlo.compare  GT, [[VAR_285_1_]], [[VAR_10_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:             [[VAR_289_1_:%.+]] = stablehlo.select [[VAR_288_1_]], [[VAR_10_]], [[VAR_285_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_290_1_:%.+]] = stablehlo.compare  LT, [[VAR_289_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_291_1_:%.+]] = stablehlo.add [[VAR_289_1_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_292_1_:%.+]] = stablehlo.select [[VAR_290_1_]], [[VAR_291_1_]], [[VAR_289_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_293_1_:%.+]] = stablehlo.compare  LT, [[VAR_284_1_]], [[VAR_20_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:         [[VAR_294_1_:%.+]] = stablehlo.add [[VAR_284_1_]], [[VAR_10_]] : tensor<1xi64>
-// CHECK:             [[VAR_295_1_:%.+]] = stablehlo.select [[VAR_293_1_]], [[VAR_294_1_]], [[VAR_284_1_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:         [[VAR_296_1_:%.+]] = stablehlo.concatenate [[VAR_20_]], [[VAR_295_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_297_1_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_292_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:         [[VAR_298_1_:%.+]] = stablehlo.concatenate [[VAR_21_]], [[VAR_286_1_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_299_1_:%.+]] = stablehlo.real_dynamic_slice [[VAR_287_1_]], [[VAR_296_1_]], [[VAR_297_1_]], [[VAR_298_1_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_300_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_224_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_301_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_151_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_302_1_:%.+]] = stablehlo.add [[VAR_300_1_]], [[VAR_301_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_303_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_302_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_304_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_155_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_305_1_:%.+]] = stablehlo.add [[VAR_303_1_]], [[VAR_304_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_306_1_:%.+]] = stablehlo.logistic [[VAR_305_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_307_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_274_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_308_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_153_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_309_1_:%.+]] = stablehlo.add [[VAR_307_1_]], [[VAR_308_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_310_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_309_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_311_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_157_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_312_1_:%.+]] = stablehlo.add [[VAR_310_1_]], [[VAR_311_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_313_1_:%.+]] = stablehlo.logistic [[VAR_312_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_314_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_299_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_315_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_154_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_316_1_:%.+]] = stablehlo.add [[VAR_314_1_]], [[VAR_315_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_317_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_316_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_318_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_158_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_319_1_:%.+]] = stablehlo.add [[VAR_317_1_]], [[VAR_318_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_320_1_:%.+]] = stablehlo.tanh [[VAR_319_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_321_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_313_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_322_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_2_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_323_1_:%.+]] = stablehlo.multiply [[VAR_321_1_]], [[VAR_322_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_324_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_306_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_325_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_320_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_326_1_:%.+]] = stablehlo.multiply [[VAR_324_1_]], [[VAR_325_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_327_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_323_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_328_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_326_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_329_1_:%.+]] = stablehlo.add [[VAR_327_1_]], [[VAR_328_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_330_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_249_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_331_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_152_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_332_1_:%.+]] = stablehlo.add [[VAR_330_1_]], [[VAR_331_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_333_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_332_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_334_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_156_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_335_1_:%.+]] = stablehlo.add [[VAR_333_1_]], [[VAR_334_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_336_1_:%.+]] = stablehlo.logistic [[VAR_335_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_337_1_:%.+]] = stablehlo.tanh [[VAR_329_1_]] : tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_338_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_336_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_339_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_337_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:             [[VAR_340_1_:%.+]] = stablehlo.multiply [[VAR_338_1_]], [[VAR_339_1_]] : tensor<16x256xf32>
-// CHECK-DAG:         [[VAR_341_1_:%.+]] = stablehlo.dynamic_reshape [[VAR_340_1_]], [[VAR_2_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
-// CHECK-DAG:         [[VAR_342_1_:%.+]] = stablehlo.dynamic_reshape [[VAR_iterArg_1_]], [[VAR_1_]] : (tensor<1xi64>, tensor<2xindex>) -> tensor<1x1xi64>
-// CHECK:             [[VAR_343_1_:%.+]] = "stablehlo.scatter"([[VAR_iterArg_0_1_]], [[VAR_342_1_]], [[VAR_341_1_]]) ({
-// CHECK:             ^bb0([[arg4_1_:%.+]]: tensor<f32>, [[arg5_1_:%.+]]: tensor<f32>):
-// CHECK:               stablehlo.return [[arg5_1_]] : tensor<f32>
+// CHECK:             [[VAR_63_3_:%.+]] = stablehlo.reshape [[VAR_iterArg_1_]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:             [[VAR_64_3_:%.+]] = stablehlo.dynamic_slice [[PARAM_0_]], [[VAR_63_3_]], [[VAR_17_]], [[VAR_17_]], sizes = [1, 16, 512] : (tensor<128x16x512xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x512xf32>
+// CHECK:             [[VAR_65_1_:%.+]] = stablehlo.dynamic_reshape [[VAR_64_3_]], [[VAR_5_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_66_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_65_1_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:         [[VAR_67_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_38_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_68_1_:%.+]] = stablehlo.dot [[VAR_66_1_]], [[VAR_67_1_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_69_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_iterArg_1_1_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_70_1_:%.+]] = stablehlo.broadcast_in_dim [[VAR_39_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_71_1_:%.+]] = stablehlo.dot [[VAR_69_1_]], [[VAR_70_1_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_72_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_68_1_]], [[VAR_4_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:             [[VAR_73_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_71_1_]], [[VAR_4_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:             [[VAR_74_1_:%.+]] = stablehlo.add [[VAR_72_1_]], [[VAR_73_1_]] : tensor<16x1024xf32>
+// CHECK-DAG:         [[VAR_75_1_:%.+]] = stablehlo.dynamic_slice [[VAR_74_1_]], [[VAR_17_]], [[VAR_17_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_76_1_:%.+]] = stablehlo.dynamic_slice [[VAR_74_1_]], [[VAR_17_]], [[VAR_13_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_77_1_:%.+]] = stablehlo.dynamic_slice [[VAR_74_1_]], [[VAR_17_]], [[VAR_12_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_78_1_:%.+]] = stablehlo.dynamic_slice [[VAR_74_1_]], [[VAR_17_]], [[VAR_11_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_79_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_75_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_80_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_52_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_81_1_:%.+]] = stablehlo.add [[VAR_79_1_]], [[VAR_80_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_82_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_81_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_83_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_56_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_84_1_:%.+]] = stablehlo.add [[VAR_82_1_]], [[VAR_83_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_85_1_:%.+]] = stablehlo.logistic [[VAR_84_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_86_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_77_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_87_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_54_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_88_1_:%.+]] = stablehlo.add [[VAR_86_1_]], [[VAR_87_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_89_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_88_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_90_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_58_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_91_1_:%.+]] = stablehlo.add [[VAR_89_1_]], [[VAR_90_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_92_1_:%.+]] = stablehlo.logistic [[VAR_91_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_93_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_78_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_94_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_55_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_95_1_:%.+]] = stablehlo.add [[VAR_93_1_]], [[VAR_94_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_96_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_95_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_97_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_59_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_98_1_:%.+]] = stablehlo.add [[VAR_96_1_]], [[VAR_97_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_99_1_:%.+]] = stablehlo.tanh [[VAR_98_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_100_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_92_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_101_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_2_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_102_1_:%.+]] = stablehlo.multiply [[VAR_100_1_]], [[VAR_101_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_103_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_85_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_104_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_99_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_105_1_:%.+]] = stablehlo.multiply [[VAR_103_1_]], [[VAR_104_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_106_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_102_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_107_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_105_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_108_1_:%.+]] = stablehlo.add [[VAR_106_1_]], [[VAR_107_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_109_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_76_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_110_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_53_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_111_1_:%.+]] = stablehlo.add [[VAR_109_1_]], [[VAR_110_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_112_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_111_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_113_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_57_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_114_1_:%.+]] = stablehlo.add [[VAR_112_1_]], [[VAR_113_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_115_1_:%.+]] = stablehlo.logistic [[VAR_114_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_116_1_:%.+]] = stablehlo.tanh [[VAR_108_1_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:         [[VAR_117_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_115_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_118_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_116_1_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:             [[VAR_119_1_:%.+]] = stablehlo.multiply [[VAR_117_1_]], [[VAR_118_1_]] : tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_120_1_:%.+]] = stablehlo.dynamic_reshape [[VAR_119_1_]], [[VAR_3_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:         [[VAR_121_1_:%.+]] = stablehlo.dynamic_reshape [[VAR_iterArg_1_]], [[VAR_2_]] : (tensor<1xi64>, tensor<2xindex>) -> tensor<1x1xi64>
+// CHECK:             [[VAR_122_1_:%.+]] = "stablehlo.scatter"([[VAR_iterArg_0_1_]], [[VAR_121_1_]], [[VAR_120_1_]]) ({
+// CHECK:             ^bb0([[arg4_:%.+]]: tensor<f32>, [[arg5_:%.+]]: tensor<f32>):
+// CHECK:               stablehlo.return [[arg5_]] : tensor<f32>
 // CHECK:             }) {indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [1, 2, 3], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<128x1x16x256xf32>, tensor<1x1xi64>, tensor<1x1x16x256xf32>) -> tensor<128x1x16x256xf32>
-// CHECK-DAG:         [[VAR_344_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_1_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-DAG:         [[VAR_345_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_21_]], [[VAR_14_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK:             [[VAR_346_1_:%.+]] = stablehlo.subtract [[VAR_344_1_]], [[VAR_345_1_]] : tensor<1xi64>
-// CHECK:             stablehlo.return [[VAR_346_1_]], [[VAR_343_1_]], [[VAR_340_1_]], [[VAR_329_1_]] : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
+// CHECK-DAG:         [[VAR_123_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_iterArg_1_]], [[VAR_1_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK-DAG:         [[VAR_124_1_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_10_]], [[VAR_1_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
+// CHECK:             [[VAR_125_1_:%.+]] = stablehlo.subtract [[VAR_123_1_]], [[VAR_124_1_]] : tensor<1xi64>
+// CHECK:             stablehlo.return [[VAR_125_1_]], [[VAR_122_1_]], [[VAR_119_1_]], [[VAR_108_1_]] : tensor<1xi64>, tensor<128x1x16x256xf32>, tensor<16x256xf32>, tensor<16x256xf32>
 // CHECK:           }
-// CHECK:           [[VAR_161_:%.+]] = stablehlo.concatenate [[VAR_159_]]#1, [[VAR_160_]]#1, dim = 1 : (tensor<128x1x16x256xf32>, tensor<128x1x16x256xf32>) -> tensor<128x2x16x256xf32>
-// CHECK:           return [[VAR_161_]] : tensor<128x2x16x256xf32>
+// CHECK:           [[VAR_62_:%.+]] = stablehlo.concatenate [[VAR_60_]]#1, [[VAR_61_]]#1, dim = 1 : (tensor<128x1x16x256xf32>, tensor<128x1x16x256xf32>) -> tensor<128x2x16x256xf32>
+// CHECK:           return [[VAR_62_]] : tensor<128x2x16x256xf32>
 // CHECK:         }
 }
\ No newline at end of file
diff --git a/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM.mlir b/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM.mlir
index 4001df8765..ee62fd8104 100644
--- a/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM.mlir
+++ b/test/mlir/conversion/onnx_to_stablehlo/RNN/LSTM.mlir
@@ -10,982 +10,324 @@ func.func @test_lstm(%arg0 : tensor<2x16x512xf32>, %arg1 : tensor<2x2048xf32>, %
 // CHECK-DAG:       [[VAR_1_:%.+]] = shape.const_shape [1, 1, 16, 256] : tensor<4xindex>
 // CHECK-DAG:       [[VAR_2_:%.+]] = shape.const_shape [16, 1024] : tensor<2xindex>
 // CHECK-DAG:       [[VAR_3_:%.+]] = shape.const_shape [16, 512] : tensor<2xindex>
-// CHECK-DAG:       [[VAR_4_:%.+]] = shape.const_shape [2, 16, 512] : tensor<3xindex>
-// CHECK-DAG:       [[VAR_5_:%.+]] = shape.const_shape [2048] : tensor<1xindex>
-// CHECK-DAG:       [[VAR_6_:%.+]] = shape.const_shape [1024, 256] : tensor<2xindex>
-// CHECK-DAG:       [[VAR_7_:%.+]] = shape.const_shape [1024, 512] : tensor<2xindex>
-// CHECK-DAG:       [[VAR_8_:%.+]] = shape.const_shape [2, 16, 256] : tensor<3xindex>
-// CHECK-DAG:       [[VAR_9_:%.+]] = stablehlo.constant dense<1024> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_10_:%.+]] = stablehlo.constant dense<768> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_11_:%.+]] = stablehlo.constant dense<512> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_12_:%.+]] = shape.const_shape [1] : tensor<1xindex>
-// CHECK-DAG:       [[VAR_13_:%.+]] = stablehlo.constant dense<256> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_14_:%.+]] = stablehlo.constant dense<16> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_15_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<2x16x256xf32>
-// CHECK-DAG:       [[VAR_16_:%.+]] = stablehlo.constant dense<0> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_17_:%.+]] = stablehlo.constant dense<1> : tensor<1xi64>
-// CHECK-DAG:       [[VAR_18_:%.+]] = stablehlo.constant dense<2> : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_19_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_20_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_21_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_22_:%.+]] = stablehlo.compare  LT, [[VAR_20_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_23_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_22_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
-// CHECK-DAG:       [[VAR_24_:%.+]] = stablehlo.negate [[VAR_20_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_25_:%.+]] = stablehlo.add [[VAR_21_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_26_:%.+]] = stablehlo.add [[VAR_19_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_27_:%.+]] = stablehlo.reverse [[VAR_15_]], dims = [0] : tensor<2x16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_28_:%.+]] = stablehlo.select [[VAR_22_]], [[VAR_25_]], [[VAR_19_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_29_:%.+]] = stablehlo.select [[VAR_22_]], [[VAR_26_]], [[VAR_21_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_30_:%.+]] = stablehlo.select [[VAR_22_]], [[VAR_24_]], [[VAR_20_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_31_:%.+]] = stablehlo.select [[VAR_23_]], [[VAR_27_]], [[VAR_15_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
-// CHECK:           [[VAR_32_:%.+]] = stablehlo.compare  GT, [[VAR_29_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_33_:%.+]] = stablehlo.select [[VAR_32_]], [[VAR_18_]], [[VAR_29_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_34_:%.+]] = stablehlo.compare  LT, [[VAR_33_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_35_:%.+]] = stablehlo.add [[VAR_33_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_36_:%.+]] = stablehlo.select [[VAR_34_]], [[VAR_35_]], [[VAR_33_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_37_:%.+]] = stablehlo.compare  LT, [[VAR_28_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_38_:%.+]] = stablehlo.add [[VAR_28_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK:           [[VAR_39_:%.+]] = stablehlo.select [[VAR_37_]], [[VAR_38_]], [[VAR_28_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_40_:%.+]] = stablehlo.concatenate [[VAR_39_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_41_:%.+]] = stablehlo.concatenate [[VAR_36_]], [[VAR_14_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_42_:%.+]] = stablehlo.concatenate [[VAR_30_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_43_:%.+]] = stablehlo.real_dynamic_slice [[VAR_31_]], [[VAR_40_]], [[VAR_41_]], [[VAR_42_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
-// CHECK-DAG:       [[VAR_44_:%.+]] = stablehlo.dynamic_reshape [[VAR_43_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_45_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_46_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_47_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_48_:%.+]] = stablehlo.compare  LT, [[VAR_46_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_49_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_48_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
-// CHECK-DAG:       [[VAR_50_:%.+]] = stablehlo.negate [[VAR_46_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_51_:%.+]] = stablehlo.add [[VAR_47_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_52_:%.+]] = stablehlo.add [[VAR_45_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_53_:%.+]] = stablehlo.reverse [[VAR_15_]], dims = [0] : tensor<2x16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_54_:%.+]] = stablehlo.select [[VAR_48_]], [[VAR_51_]], [[VAR_45_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_55_:%.+]] = stablehlo.select [[VAR_48_]], [[VAR_52_]], [[VAR_47_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_56_:%.+]] = stablehlo.select [[VAR_48_]], [[VAR_50_]], [[VAR_46_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_57_:%.+]] = stablehlo.select [[VAR_49_]], [[VAR_53_]], [[VAR_15_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
-// CHECK:           [[VAR_58_:%.+]] = stablehlo.compare  GT, [[VAR_55_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_59_:%.+]] = stablehlo.select [[VAR_58_]], [[VAR_18_]], [[VAR_55_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_60_:%.+]] = stablehlo.compare  LT, [[VAR_59_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_61_:%.+]] = stablehlo.add [[VAR_59_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_62_:%.+]] = stablehlo.select [[VAR_60_]], [[VAR_61_]], [[VAR_59_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_63_:%.+]] = stablehlo.compare  LT, [[VAR_54_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_64_:%.+]] = stablehlo.add [[VAR_54_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK:           [[VAR_65_:%.+]] = stablehlo.select [[VAR_63_]], [[VAR_64_]], [[VAR_54_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_66_:%.+]] = stablehlo.concatenate [[VAR_65_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_67_:%.+]] = stablehlo.concatenate [[VAR_62_]], [[VAR_14_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_68_:%.+]] = stablehlo.concatenate [[VAR_56_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_69_:%.+]] = stablehlo.real_dynamic_slice [[VAR_57_]], [[VAR_66_]], [[VAR_67_]], [[VAR_68_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
-// CHECK-DAG:       [[VAR_70_:%.+]] = stablehlo.dynamic_reshape [[VAR_69_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_71_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_72_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_73_:%.+]] = stablehlo.slice [[VAR_18_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_74_:%.+]] = stablehlo.compare  LT, [[VAR_72_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_75_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_74_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
-// CHECK-DAG:       [[VAR_76_:%.+]] = stablehlo.negate [[VAR_72_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_77_:%.+]] = stablehlo.add [[VAR_73_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_78_:%.+]] = stablehlo.add [[VAR_71_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_79_:%.+]] = stablehlo.reverse [[VAR_15_]], dims = [0] : tensor<2x16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_80_:%.+]] = stablehlo.select [[VAR_74_]], [[VAR_77_]], [[VAR_71_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_81_:%.+]] = stablehlo.select [[VAR_74_]], [[VAR_78_]], [[VAR_73_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_82_:%.+]] = stablehlo.select [[VAR_74_]], [[VAR_76_]], [[VAR_72_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_83_:%.+]] = stablehlo.select [[VAR_75_]], [[VAR_79_]], [[VAR_15_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
-// CHECK:           [[VAR_84_:%.+]] = stablehlo.compare  GT, [[VAR_81_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_85_:%.+]] = stablehlo.select [[VAR_84_]], [[VAR_18_]], [[VAR_81_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_86_:%.+]] = stablehlo.compare  LT, [[VAR_85_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_87_:%.+]] = stablehlo.add [[VAR_85_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_88_:%.+]] = stablehlo.select [[VAR_86_]], [[VAR_87_]], [[VAR_85_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_89_:%.+]] = stablehlo.compare  LT, [[VAR_80_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_90_:%.+]] = stablehlo.add [[VAR_80_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK:           [[VAR_91_:%.+]] = stablehlo.select [[VAR_89_]], [[VAR_90_]], [[VAR_80_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_92_:%.+]] = stablehlo.concatenate [[VAR_91_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_93_:%.+]] = stablehlo.concatenate [[VAR_88_]], [[VAR_14_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_94_:%.+]] = stablehlo.concatenate [[VAR_82_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_95_:%.+]] = stablehlo.real_dynamic_slice [[VAR_83_]], [[VAR_92_]], [[VAR_93_]], [[VAR_94_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
-// CHECK-DAG:       [[VAR_96_:%.+]] = stablehlo.dynamic_reshape [[VAR_95_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_97_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_98_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_99_:%.+]] = stablehlo.slice [[VAR_18_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_100_:%.+]] = stablehlo.compare  LT, [[VAR_98_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_101_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_100_]], [[VAR_8_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x256xi1>
-// CHECK-DAG:       [[VAR_102_:%.+]] = stablehlo.negate [[VAR_98_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_103_:%.+]] = stablehlo.add [[VAR_99_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_104_:%.+]] = stablehlo.add [[VAR_97_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_105_:%.+]] = stablehlo.reverse [[VAR_15_]], dims = [0] : tensor<2x16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_106_:%.+]] = stablehlo.select [[VAR_100_]], [[VAR_103_]], [[VAR_97_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_107_:%.+]] = stablehlo.select [[VAR_100_]], [[VAR_104_]], [[VAR_99_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_108_:%.+]] = stablehlo.select [[VAR_100_]], [[VAR_102_]], [[VAR_98_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_109_:%.+]] = stablehlo.select [[VAR_101_]], [[VAR_105_]], [[VAR_15_]] : tensor<2x16x256xi1>, tensor<2x16x256xf32>
-// CHECK:           [[VAR_110_:%.+]] = stablehlo.compare  GT, [[VAR_107_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_111_:%.+]] = stablehlo.select [[VAR_110_]], [[VAR_18_]], [[VAR_107_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_112_:%.+]] = stablehlo.compare  LT, [[VAR_111_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_113_:%.+]] = stablehlo.add [[VAR_111_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_114_:%.+]] = stablehlo.select [[VAR_112_]], [[VAR_113_]], [[VAR_111_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_115_:%.+]] = stablehlo.compare  LT, [[VAR_106_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_116_:%.+]] = stablehlo.add [[VAR_106_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK:           [[VAR_117_:%.+]] = stablehlo.select [[VAR_115_]], [[VAR_116_]], [[VAR_106_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_118_:%.+]] = stablehlo.concatenate [[VAR_117_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_119_:%.+]] = stablehlo.concatenate [[VAR_114_]], [[VAR_14_]], [[VAR_13_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_120_:%.+]] = stablehlo.concatenate [[VAR_108_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_121_:%.+]] = stablehlo.real_dynamic_slice [[VAR_109_]], [[VAR_118_]], [[VAR_119_]], [[VAR_120_]] : (tensor<2x16x256xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x256xf32>
-// CHECK-DAG:       [[VAR_122_:%.+]] = stablehlo.dynamic_reshape [[VAR_121_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_123_:%.+]] = stablehlo.slice [[PARAM_2_]] [0:1, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
-// CHECK-DAG:       [[VAR_124_:%.+]] = stablehlo.slice [[PARAM_2_]] [1:2, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_125_:%.+]] = stablehlo.dynamic_reshape [[VAR_123_]], [[VAR_7_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
-// CHECK-DAG:       [[VAR_126_:%.+]] = stablehlo.dynamic_reshape [[VAR_124_]], [[VAR_7_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
-// CHECK-DAG:       [[VAR_127_:%.+]] = stablehlo.slice [[PARAM_3_]] [0:1, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
-// CHECK-DAG:       [[VAR_128_:%.+]] = stablehlo.slice [[PARAM_3_]] [1:2, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_129_:%.+]] = stablehlo.dynamic_reshape [[VAR_127_]], [[VAR_6_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
-// CHECK-DAG:       [[VAR_130_:%.+]] = stablehlo.dynamic_reshape [[VAR_128_]], [[VAR_6_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
-// CHECK-DAG:       [[VAR_131_:%.+]] = stablehlo.transpose [[VAR_125_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_132_:%.+]] = stablehlo.transpose [[VAR_129_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
-// CHECK-DAG:       [[VAR_133_:%.+]] = stablehlo.transpose [[VAR_126_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
-// CHECK-DAG:       [[VAR_134_:%.+]] = stablehlo.transpose [[VAR_130_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
-// CHECK-DAG:       [[VAR_135_:%.+]] = stablehlo.slice [[PARAM_1_]] [0:1, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
-// CHECK-DAG:       [[VAR_136_:%.+]] = stablehlo.slice [[PARAM_1_]] [1:2, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_137_:%.+]] = stablehlo.dynamic_reshape [[VAR_135_]], [[VAR_5_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
-// CHECK-DAG:       [[VAR_138_:%.+]] = stablehlo.dynamic_reshape [[VAR_136_]], [[VAR_5_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_139_:%.+]] = stablehlo.slice [[VAR_137_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_140_:%.+]] = stablehlo.slice [[VAR_137_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_141_:%.+]] = stablehlo.slice [[VAR_137_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_142_:%.+]] = stablehlo.slice [[VAR_137_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_143_:%.+]] = stablehlo.slice [[VAR_137_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_144_:%.+]] = stablehlo.slice [[VAR_137_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_145_:%.+]] = stablehlo.slice [[VAR_137_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_146_:%.+]] = stablehlo.slice [[VAR_137_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_147_:%.+]] = stablehlo.slice [[VAR_138_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_148_:%.+]] = stablehlo.slice [[VAR_138_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_149_:%.+]] = stablehlo.slice [[VAR_138_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_150_:%.+]] = stablehlo.slice [[VAR_138_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_151_:%.+]] = stablehlo.slice [[VAR_138_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_152_:%.+]] = stablehlo.slice [[VAR_138_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_153_:%.+]] = stablehlo.slice [[VAR_138_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_154_:%.+]] = stablehlo.slice [[VAR_138_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
-// CHECK-DAG:       [[VAR_155_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_16_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_156_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_157_:%.+]] = stablehlo.add [[VAR_155_]], [[VAR_156_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_158_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_159_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_160_:%.+]] = stablehlo.slice [[VAR_157_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_161_:%.+]] = stablehlo.compare  LT, [[VAR_159_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_162_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_161_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x512xi1>
-// CHECK-DAG:       [[VAR_163_:%.+]] = stablehlo.negate [[VAR_159_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_164_:%.+]] = stablehlo.add [[VAR_160_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_165_:%.+]] = stablehlo.add [[VAR_158_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_166_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<2x16x512xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_167_:%.+]] = stablehlo.select [[VAR_161_]], [[VAR_164_]], [[VAR_158_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_168_:%.+]] = stablehlo.select [[VAR_161_]], [[VAR_165_]], [[VAR_160_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_169_:%.+]] = stablehlo.select [[VAR_161_]], [[VAR_163_]], [[VAR_159_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_170_:%.+]] = stablehlo.select [[VAR_162_]], [[VAR_166_]], [[PARAM_0_]] : tensor<2x16x512xi1>, tensor<2x16x512xf32>
-// CHECK:           [[VAR_171_:%.+]] = stablehlo.compare  GT, [[VAR_168_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_172_:%.+]] = stablehlo.select [[VAR_171_]], [[VAR_18_]], [[VAR_168_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_173_:%.+]] = stablehlo.compare  LT, [[VAR_172_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_174_:%.+]] = stablehlo.add [[VAR_172_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_175_:%.+]] = stablehlo.select [[VAR_173_]], [[VAR_174_]], [[VAR_172_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_176_:%.+]] = stablehlo.compare  LT, [[VAR_167_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_177_:%.+]] = stablehlo.add [[VAR_167_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK:           [[VAR_178_:%.+]] = stablehlo.select [[VAR_176_]], [[VAR_177_]], [[VAR_167_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_179_:%.+]] = stablehlo.concatenate [[VAR_178_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_180_:%.+]] = stablehlo.concatenate [[VAR_175_]], [[VAR_14_]], [[VAR_11_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_181_:%.+]] = stablehlo.concatenate [[VAR_169_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_182_:%.+]] = stablehlo.real_dynamic_slice [[VAR_170_]], [[VAR_179_]], [[VAR_180_]], [[VAR_181_]] : (tensor<2x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
-// CHECK:           [[VAR_183_:%.+]] = stablehlo.dynamic_reshape [[VAR_182_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
-// CHECK-DAG:       [[VAR_184_:%.+]] = stablehlo.broadcast_in_dim [[VAR_183_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
-// CHECK-DAG:       [[VAR_185_:%.+]] = stablehlo.broadcast_in_dim [[VAR_131_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_186_:%.+]] = stablehlo.dot [[VAR_184_]], [[VAR_185_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_187_:%.+]] = stablehlo.broadcast_in_dim [[VAR_44_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_188_:%.+]] = stablehlo.broadcast_in_dim [[VAR_132_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_189_:%.+]] = stablehlo.dot [[VAR_187_]], [[VAR_188_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_190_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_186_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK:           [[VAR_191_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_189_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_192_:%.+]] = stablehlo.add [[VAR_190_]], [[VAR_191_]] : tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_193_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_194_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_195_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_196_:%.+]] = stablehlo.compare  LT, [[VAR_194_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_197_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_196_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_198_:%.+]] = stablehlo.negate [[VAR_194_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_199_:%.+]] = stablehlo.add [[VAR_195_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_200_:%.+]] = stablehlo.add [[VAR_193_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_201_:%.+]] = stablehlo.reverse [[VAR_192_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_202_:%.+]] = stablehlo.select [[VAR_196_]], [[VAR_199_]], [[VAR_193_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_203_:%.+]] = stablehlo.select [[VAR_196_]], [[VAR_200_]], [[VAR_195_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_204_:%.+]] = stablehlo.select [[VAR_196_]], [[VAR_198_]], [[VAR_194_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_205_:%.+]] = stablehlo.select [[VAR_197_]], [[VAR_201_]], [[VAR_192_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_206_:%.+]] = stablehlo.compare  GT, [[VAR_203_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_207_:%.+]] = stablehlo.select [[VAR_206_]], [[VAR_9_]], [[VAR_203_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_208_:%.+]] = stablehlo.compare  LT, [[VAR_207_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_209_:%.+]] = stablehlo.add [[VAR_207_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_210_:%.+]] = stablehlo.select [[VAR_208_]], [[VAR_209_]], [[VAR_207_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_211_:%.+]] = stablehlo.compare  LT, [[VAR_202_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_212_:%.+]] = stablehlo.add [[VAR_202_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_213_:%.+]] = stablehlo.select [[VAR_211_]], [[VAR_212_]], [[VAR_202_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_214_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_213_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_215_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_210_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_216_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_204_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_217_:%.+]] = stablehlo.real_dynamic_slice [[VAR_205_]], [[VAR_214_]], [[VAR_215_]], [[VAR_216_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_218_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_219_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_220_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_221_:%.+]] = stablehlo.compare  LT, [[VAR_219_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_222_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_221_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_223_:%.+]] = stablehlo.negate [[VAR_219_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_224_:%.+]] = stablehlo.add [[VAR_220_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_225_:%.+]] = stablehlo.add [[VAR_218_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_226_:%.+]] = stablehlo.reverse [[VAR_192_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_227_:%.+]] = stablehlo.select [[VAR_221_]], [[VAR_224_]], [[VAR_218_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_228_:%.+]] = stablehlo.select [[VAR_221_]], [[VAR_225_]], [[VAR_220_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_229_:%.+]] = stablehlo.select [[VAR_221_]], [[VAR_223_]], [[VAR_219_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_230_:%.+]] = stablehlo.select [[VAR_222_]], [[VAR_226_]], [[VAR_192_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_231_:%.+]] = stablehlo.compare  GT, [[VAR_228_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_232_:%.+]] = stablehlo.select [[VAR_231_]], [[VAR_9_]], [[VAR_228_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_233_:%.+]] = stablehlo.compare  LT, [[VAR_232_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_234_:%.+]] = stablehlo.add [[VAR_232_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_235_:%.+]] = stablehlo.select [[VAR_233_]], [[VAR_234_]], [[VAR_232_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_236_:%.+]] = stablehlo.compare  LT, [[VAR_227_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_237_:%.+]] = stablehlo.add [[VAR_227_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_238_:%.+]] = stablehlo.select [[VAR_236_]], [[VAR_237_]], [[VAR_227_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_239_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_238_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_240_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_235_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_241_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_229_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_242_:%.+]] = stablehlo.real_dynamic_slice [[VAR_230_]], [[VAR_239_]], [[VAR_240_]], [[VAR_241_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_243_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_244_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_245_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_246_:%.+]] = stablehlo.compare  LT, [[VAR_244_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_247_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_246_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_248_:%.+]] = stablehlo.negate [[VAR_244_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_249_:%.+]] = stablehlo.add [[VAR_245_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_250_:%.+]] = stablehlo.add [[VAR_243_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_251_:%.+]] = stablehlo.reverse [[VAR_192_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_252_:%.+]] = stablehlo.select [[VAR_246_]], [[VAR_249_]], [[VAR_243_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_253_:%.+]] = stablehlo.select [[VAR_246_]], [[VAR_250_]], [[VAR_245_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_254_:%.+]] = stablehlo.select [[VAR_246_]], [[VAR_248_]], [[VAR_244_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_255_:%.+]] = stablehlo.select [[VAR_247_]], [[VAR_251_]], [[VAR_192_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_256_:%.+]] = stablehlo.compare  GT, [[VAR_253_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_257_:%.+]] = stablehlo.select [[VAR_256_]], [[VAR_9_]], [[VAR_253_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_258_:%.+]] = stablehlo.compare  LT, [[VAR_257_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_259_:%.+]] = stablehlo.add [[VAR_257_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_260_:%.+]] = stablehlo.select [[VAR_258_]], [[VAR_259_]], [[VAR_257_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_261_:%.+]] = stablehlo.compare  LT, [[VAR_252_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_262_:%.+]] = stablehlo.add [[VAR_252_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_263_:%.+]] = stablehlo.select [[VAR_261_]], [[VAR_262_]], [[VAR_252_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_264_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_263_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_265_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_260_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_266_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_254_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_267_:%.+]] = stablehlo.real_dynamic_slice [[VAR_255_]], [[VAR_264_]], [[VAR_265_]], [[VAR_266_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_268_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_269_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_270_:%.+]] = stablehlo.slice [[VAR_9_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_271_:%.+]] = stablehlo.compare  LT, [[VAR_269_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_272_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_271_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_273_:%.+]] = stablehlo.negate [[VAR_269_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_274_:%.+]] = stablehlo.add [[VAR_270_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_275_:%.+]] = stablehlo.add [[VAR_268_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_276_:%.+]] = stablehlo.reverse [[VAR_192_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_277_:%.+]] = stablehlo.select [[VAR_271_]], [[VAR_274_]], [[VAR_268_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_278_:%.+]] = stablehlo.select [[VAR_271_]], [[VAR_275_]], [[VAR_270_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_279_:%.+]] = stablehlo.select [[VAR_271_]], [[VAR_273_]], [[VAR_269_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_280_:%.+]] = stablehlo.select [[VAR_272_]], [[VAR_276_]], [[VAR_192_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_281_:%.+]] = stablehlo.compare  GT, [[VAR_278_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_282_:%.+]] = stablehlo.select [[VAR_281_]], [[VAR_9_]], [[VAR_278_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_283_:%.+]] = stablehlo.compare  LT, [[VAR_282_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_284_:%.+]] = stablehlo.add [[VAR_282_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_285_:%.+]] = stablehlo.select [[VAR_283_]], [[VAR_284_]], [[VAR_282_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_286_:%.+]] = stablehlo.compare  LT, [[VAR_277_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_287_:%.+]] = stablehlo.add [[VAR_277_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_288_:%.+]] = stablehlo.select [[VAR_286_]], [[VAR_287_]], [[VAR_277_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_289_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_288_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_290_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_285_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_291_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_279_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_292_:%.+]] = stablehlo.real_dynamic_slice [[VAR_280_]], [[VAR_289_]], [[VAR_290_]], [[VAR_291_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_293_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_217_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_294_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_139_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_295_:%.+]] = stablehlo.add [[VAR_293_]], [[VAR_294_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_296_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_295_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_297_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_143_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_298_:%.+]] = stablehlo.add [[VAR_296_]], [[VAR_297_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_299_:%.+]] = stablehlo.logistic [[VAR_298_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_300_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_267_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_301_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_141_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_302_:%.+]] = stablehlo.add [[VAR_300_]], [[VAR_301_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_303_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_302_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_304_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_145_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_305_:%.+]] = stablehlo.add [[VAR_303_]], [[VAR_304_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_306_:%.+]] = stablehlo.logistic [[VAR_305_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_307_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_292_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_308_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_142_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_309_:%.+]] = stablehlo.add [[VAR_307_]], [[VAR_308_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_310_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_309_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_311_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_146_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_312_:%.+]] = stablehlo.add [[VAR_310_]], [[VAR_311_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_313_:%.+]] = stablehlo.tanh [[VAR_312_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_314_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_306_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_315_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_70_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_316_:%.+]] = stablehlo.multiply [[VAR_314_]], [[VAR_315_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_317_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_299_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_318_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_313_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_319_:%.+]] = stablehlo.multiply [[VAR_317_]], [[VAR_318_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_320_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_316_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_321_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_319_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_322_:%.+]] = stablehlo.add [[VAR_320_]], [[VAR_321_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_323_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_242_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_324_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_140_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_325_:%.+]] = stablehlo.add [[VAR_323_]], [[VAR_324_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_326_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_325_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_327_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_144_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_328_:%.+]] = stablehlo.add [[VAR_326_]], [[VAR_327_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_329_:%.+]] = stablehlo.logistic [[VAR_328_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_330_:%.+]] = stablehlo.tanh [[VAR_322_]] : tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_331_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_329_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_332_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_330_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_333_:%.+]] = stablehlo.multiply [[VAR_331_]], [[VAR_332_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_334_:%.+]] = stablehlo.dynamic_reshape [[VAR_333_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
-// CHECK-DAG:       [[VAR_335_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_336_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_337_:%.+]] = stablehlo.add [[VAR_335_]], [[VAR_336_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_338_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_339_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_340_:%.+]] = stablehlo.slice [[VAR_337_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_341_:%.+]] = stablehlo.compare  LT, [[VAR_339_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_342_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_341_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x512xi1>
-// CHECK-DAG:       [[VAR_343_:%.+]] = stablehlo.negate [[VAR_339_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_344_:%.+]] = stablehlo.add [[VAR_340_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_345_:%.+]] = stablehlo.add [[VAR_338_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_346_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<2x16x512xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_347_:%.+]] = stablehlo.select [[VAR_341_]], [[VAR_344_]], [[VAR_338_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_348_:%.+]] = stablehlo.select [[VAR_341_]], [[VAR_345_]], [[VAR_340_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_349_:%.+]] = stablehlo.select [[VAR_341_]], [[VAR_343_]], [[VAR_339_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_350_:%.+]] = stablehlo.select [[VAR_342_]], [[VAR_346_]], [[PARAM_0_]] : tensor<2x16x512xi1>, tensor<2x16x512xf32>
-// CHECK:           [[VAR_351_:%.+]] = stablehlo.compare  GT, [[VAR_348_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_352_:%.+]] = stablehlo.select [[VAR_351_]], [[VAR_18_]], [[VAR_348_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_353_:%.+]] = stablehlo.compare  LT, [[VAR_352_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_354_:%.+]] = stablehlo.add [[VAR_352_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_355_:%.+]] = stablehlo.select [[VAR_353_]], [[VAR_354_]], [[VAR_352_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_356_:%.+]] = stablehlo.compare  LT, [[VAR_347_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_357_:%.+]] = stablehlo.add [[VAR_347_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK:           [[VAR_358_:%.+]] = stablehlo.select [[VAR_356_]], [[VAR_357_]], [[VAR_347_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_359_:%.+]] = stablehlo.concatenate [[VAR_358_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_360_:%.+]] = stablehlo.concatenate [[VAR_355_]], [[VAR_14_]], [[VAR_11_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_361_:%.+]] = stablehlo.concatenate [[VAR_349_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_362_:%.+]] = stablehlo.real_dynamic_slice [[VAR_350_]], [[VAR_359_]], [[VAR_360_]], [[VAR_361_]] : (tensor<2x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
-// CHECK:           [[VAR_363_:%.+]] = stablehlo.dynamic_reshape [[VAR_362_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
-// CHECK-DAG:       [[VAR_364_:%.+]] = stablehlo.broadcast_in_dim [[VAR_363_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
-// CHECK-DAG:       [[VAR_365_:%.+]] = stablehlo.broadcast_in_dim [[VAR_131_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_366_:%.+]] = stablehlo.dot [[VAR_364_]], [[VAR_365_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_367_:%.+]] = stablehlo.broadcast_in_dim [[VAR_333_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_368_:%.+]] = stablehlo.broadcast_in_dim [[VAR_132_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_369_:%.+]] = stablehlo.dot [[VAR_367_]], [[VAR_368_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_370_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_366_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK:           [[VAR_371_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_369_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_372_:%.+]] = stablehlo.add [[VAR_370_]], [[VAR_371_]] : tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_373_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_374_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_375_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_376_:%.+]] = stablehlo.compare  LT, [[VAR_374_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_377_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_376_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_378_:%.+]] = stablehlo.negate [[VAR_374_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_379_:%.+]] = stablehlo.add [[VAR_375_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_380_:%.+]] = stablehlo.add [[VAR_373_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_381_:%.+]] = stablehlo.reverse [[VAR_372_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_382_:%.+]] = stablehlo.select [[VAR_376_]], [[VAR_379_]], [[VAR_373_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_383_:%.+]] = stablehlo.select [[VAR_376_]], [[VAR_380_]], [[VAR_375_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_384_:%.+]] = stablehlo.select [[VAR_376_]], [[VAR_378_]], [[VAR_374_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_385_:%.+]] = stablehlo.select [[VAR_377_]], [[VAR_381_]], [[VAR_372_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_386_:%.+]] = stablehlo.compare  GT, [[VAR_383_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_387_:%.+]] = stablehlo.select [[VAR_386_]], [[VAR_9_]], [[VAR_383_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_388_:%.+]] = stablehlo.compare  LT, [[VAR_387_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_389_:%.+]] = stablehlo.add [[VAR_387_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_390_:%.+]] = stablehlo.select [[VAR_388_]], [[VAR_389_]], [[VAR_387_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_391_:%.+]] = stablehlo.compare  LT, [[VAR_382_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_392_:%.+]] = stablehlo.add [[VAR_382_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_393_:%.+]] = stablehlo.select [[VAR_391_]], [[VAR_392_]], [[VAR_382_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_394_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_393_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_395_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_390_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_396_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_384_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_397_:%.+]] = stablehlo.real_dynamic_slice [[VAR_385_]], [[VAR_394_]], [[VAR_395_]], [[VAR_396_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_398_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_399_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_400_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_401_:%.+]] = stablehlo.compare  LT, [[VAR_399_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_402_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_401_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_403_:%.+]] = stablehlo.negate [[VAR_399_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_404_:%.+]] = stablehlo.add [[VAR_400_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_405_:%.+]] = stablehlo.add [[VAR_398_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_406_:%.+]] = stablehlo.reverse [[VAR_372_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_407_:%.+]] = stablehlo.select [[VAR_401_]], [[VAR_404_]], [[VAR_398_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_408_:%.+]] = stablehlo.select [[VAR_401_]], [[VAR_405_]], [[VAR_400_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_409_:%.+]] = stablehlo.select [[VAR_401_]], [[VAR_403_]], [[VAR_399_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_410_:%.+]] = stablehlo.select [[VAR_402_]], [[VAR_406_]], [[VAR_372_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_411_:%.+]] = stablehlo.compare  GT, [[VAR_408_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_412_:%.+]] = stablehlo.select [[VAR_411_]], [[VAR_9_]], [[VAR_408_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_413_:%.+]] = stablehlo.compare  LT, [[VAR_412_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_414_:%.+]] = stablehlo.add [[VAR_412_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_415_:%.+]] = stablehlo.select [[VAR_413_]], [[VAR_414_]], [[VAR_412_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_416_:%.+]] = stablehlo.compare  LT, [[VAR_407_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_417_:%.+]] = stablehlo.add [[VAR_407_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_418_:%.+]] = stablehlo.select [[VAR_416_]], [[VAR_417_]], [[VAR_407_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_419_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_418_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_420_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_415_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_421_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_409_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_422_:%.+]] = stablehlo.real_dynamic_slice [[VAR_410_]], [[VAR_419_]], [[VAR_420_]], [[VAR_421_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_423_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_424_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_425_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_426_:%.+]] = stablehlo.compare  LT, [[VAR_424_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_427_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_426_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_428_:%.+]] = stablehlo.negate [[VAR_424_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_429_:%.+]] = stablehlo.add [[VAR_425_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_430_:%.+]] = stablehlo.add [[VAR_423_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_431_:%.+]] = stablehlo.reverse [[VAR_372_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_432_:%.+]] = stablehlo.select [[VAR_426_]], [[VAR_429_]], [[VAR_423_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_433_:%.+]] = stablehlo.select [[VAR_426_]], [[VAR_430_]], [[VAR_425_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_434_:%.+]] = stablehlo.select [[VAR_426_]], [[VAR_428_]], [[VAR_424_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_435_:%.+]] = stablehlo.select [[VAR_427_]], [[VAR_431_]], [[VAR_372_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_436_:%.+]] = stablehlo.compare  GT, [[VAR_433_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_437_:%.+]] = stablehlo.select [[VAR_436_]], [[VAR_9_]], [[VAR_433_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_438_:%.+]] = stablehlo.compare  LT, [[VAR_437_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_439_:%.+]] = stablehlo.add [[VAR_437_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_440_:%.+]] = stablehlo.select [[VAR_438_]], [[VAR_439_]], [[VAR_437_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_441_:%.+]] = stablehlo.compare  LT, [[VAR_432_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_442_:%.+]] = stablehlo.add [[VAR_432_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_443_:%.+]] = stablehlo.select [[VAR_441_]], [[VAR_442_]], [[VAR_432_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_444_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_443_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_445_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_440_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_446_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_434_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_447_:%.+]] = stablehlo.real_dynamic_slice [[VAR_435_]], [[VAR_444_]], [[VAR_445_]], [[VAR_446_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_448_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_449_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_450_:%.+]] = stablehlo.slice [[VAR_9_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_451_:%.+]] = stablehlo.compare  LT, [[VAR_449_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_452_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_451_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_453_:%.+]] = stablehlo.negate [[VAR_449_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_454_:%.+]] = stablehlo.add [[VAR_450_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_455_:%.+]] = stablehlo.add [[VAR_448_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_456_:%.+]] = stablehlo.reverse [[VAR_372_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_457_:%.+]] = stablehlo.select [[VAR_451_]], [[VAR_454_]], [[VAR_448_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_458_:%.+]] = stablehlo.select [[VAR_451_]], [[VAR_455_]], [[VAR_450_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_459_:%.+]] = stablehlo.select [[VAR_451_]], [[VAR_453_]], [[VAR_449_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_460_:%.+]] = stablehlo.select [[VAR_452_]], [[VAR_456_]], [[VAR_372_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_461_:%.+]] = stablehlo.compare  GT, [[VAR_458_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_462_:%.+]] = stablehlo.select [[VAR_461_]], [[VAR_9_]], [[VAR_458_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_463_:%.+]] = stablehlo.compare  LT, [[VAR_462_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_464_:%.+]] = stablehlo.add [[VAR_462_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_465_:%.+]] = stablehlo.select [[VAR_463_]], [[VAR_464_]], [[VAR_462_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_466_:%.+]] = stablehlo.compare  LT, [[VAR_457_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_467_:%.+]] = stablehlo.add [[VAR_457_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_468_:%.+]] = stablehlo.select [[VAR_466_]], [[VAR_467_]], [[VAR_457_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_469_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_468_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_470_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_465_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_471_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_459_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_472_:%.+]] = stablehlo.real_dynamic_slice [[VAR_460_]], [[VAR_469_]], [[VAR_470_]], [[VAR_471_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_473_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_397_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_474_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_139_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_475_:%.+]] = stablehlo.add [[VAR_473_]], [[VAR_474_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_476_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_475_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_477_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_143_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_478_:%.+]] = stablehlo.add [[VAR_476_]], [[VAR_477_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_479_:%.+]] = stablehlo.logistic [[VAR_478_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_480_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_447_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_481_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_141_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_482_:%.+]] = stablehlo.add [[VAR_480_]], [[VAR_481_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_483_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_482_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_484_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_145_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_485_:%.+]] = stablehlo.add [[VAR_483_]], [[VAR_484_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_486_:%.+]] = stablehlo.logistic [[VAR_485_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_487_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_472_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_488_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_142_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_489_:%.+]] = stablehlo.add [[VAR_487_]], [[VAR_488_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_490_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_489_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_491_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_146_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_492_:%.+]] = stablehlo.add [[VAR_490_]], [[VAR_491_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_493_:%.+]] = stablehlo.tanh [[VAR_492_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_494_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_486_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_495_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_322_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_496_:%.+]] = stablehlo.multiply [[VAR_494_]], [[VAR_495_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_497_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_479_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_498_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_493_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_499_:%.+]] = stablehlo.multiply [[VAR_497_]], [[VAR_498_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_500_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_496_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_501_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_499_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_502_:%.+]] = stablehlo.add [[VAR_500_]], [[VAR_501_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_503_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_422_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_504_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_140_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_505_:%.+]] = stablehlo.add [[VAR_503_]], [[VAR_504_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_506_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_505_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_507_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_144_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_508_:%.+]] = stablehlo.add [[VAR_506_]], [[VAR_507_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_509_:%.+]] = stablehlo.logistic [[VAR_508_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_510_:%.+]] = stablehlo.tanh [[VAR_502_]] : tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_511_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_509_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_512_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_510_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_513_:%.+]] = stablehlo.multiply [[VAR_511_]], [[VAR_512_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_514_:%.+]] = stablehlo.dynamic_reshape [[VAR_513_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
-// CHECK-DAG:       [[VAR_515_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_516_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_517_:%.+]] = stablehlo.add [[VAR_515_]], [[VAR_516_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_518_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_519_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_520_:%.+]] = stablehlo.slice [[VAR_517_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_521_:%.+]] = stablehlo.compare  LT, [[VAR_519_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_522_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_521_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x512xi1>
-// CHECK-DAG:       [[VAR_523_:%.+]] = stablehlo.negate [[VAR_519_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_524_:%.+]] = stablehlo.add [[VAR_520_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_525_:%.+]] = stablehlo.add [[VAR_518_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_526_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<2x16x512xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_527_:%.+]] = stablehlo.select [[VAR_521_]], [[VAR_524_]], [[VAR_518_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_528_:%.+]] = stablehlo.select [[VAR_521_]], [[VAR_525_]], [[VAR_520_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_529_:%.+]] = stablehlo.select [[VAR_521_]], [[VAR_523_]], [[VAR_519_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_530_:%.+]] = stablehlo.select [[VAR_522_]], [[VAR_526_]], [[PARAM_0_]] : tensor<2x16x512xi1>, tensor<2x16x512xf32>
-// CHECK:           [[VAR_531_:%.+]] = stablehlo.compare  GT, [[VAR_528_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_532_:%.+]] = stablehlo.select [[VAR_531_]], [[VAR_18_]], [[VAR_528_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_533_:%.+]] = stablehlo.compare  LT, [[VAR_532_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_534_:%.+]] = stablehlo.add [[VAR_532_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_535_:%.+]] = stablehlo.select [[VAR_533_]], [[VAR_534_]], [[VAR_532_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_536_:%.+]] = stablehlo.compare  LT, [[VAR_527_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_537_:%.+]] = stablehlo.add [[VAR_527_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK:           [[VAR_538_:%.+]] = stablehlo.select [[VAR_536_]], [[VAR_537_]], [[VAR_527_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_539_:%.+]] = stablehlo.concatenate [[VAR_538_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_540_:%.+]] = stablehlo.concatenate [[VAR_535_]], [[VAR_14_]], [[VAR_11_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_541_:%.+]] = stablehlo.concatenate [[VAR_529_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_542_:%.+]] = stablehlo.real_dynamic_slice [[VAR_530_]], [[VAR_539_]], [[VAR_540_]], [[VAR_541_]] : (tensor<2x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
-// CHECK:           [[VAR_543_:%.+]] = stablehlo.dynamic_reshape [[VAR_542_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
-// CHECK-DAG:       [[VAR_544_:%.+]] = stablehlo.broadcast_in_dim [[VAR_543_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
-// CHECK-DAG:       [[VAR_545_:%.+]] = stablehlo.broadcast_in_dim [[VAR_133_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_546_:%.+]] = stablehlo.dot [[VAR_544_]], [[VAR_545_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_547_:%.+]] = stablehlo.broadcast_in_dim [[VAR_96_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_548_:%.+]] = stablehlo.broadcast_in_dim [[VAR_134_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_549_:%.+]] = stablehlo.dot [[VAR_547_]], [[VAR_548_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_550_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_546_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK:           [[VAR_551_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_549_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_552_:%.+]] = stablehlo.add [[VAR_550_]], [[VAR_551_]] : tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_553_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_554_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_555_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_556_:%.+]] = stablehlo.compare  LT, [[VAR_554_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_557_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_556_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_558_:%.+]] = stablehlo.negate [[VAR_554_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_559_:%.+]] = stablehlo.add [[VAR_555_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_560_:%.+]] = stablehlo.add [[VAR_553_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_561_:%.+]] = stablehlo.reverse [[VAR_552_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_562_:%.+]] = stablehlo.select [[VAR_556_]], [[VAR_559_]], [[VAR_553_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_563_:%.+]] = stablehlo.select [[VAR_556_]], [[VAR_560_]], [[VAR_555_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_564_:%.+]] = stablehlo.select [[VAR_556_]], [[VAR_558_]], [[VAR_554_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_565_:%.+]] = stablehlo.select [[VAR_557_]], [[VAR_561_]], [[VAR_552_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_566_:%.+]] = stablehlo.compare  GT, [[VAR_563_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_567_:%.+]] = stablehlo.select [[VAR_566_]], [[VAR_9_]], [[VAR_563_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_568_:%.+]] = stablehlo.compare  LT, [[VAR_567_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_569_:%.+]] = stablehlo.add [[VAR_567_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_570_:%.+]] = stablehlo.select [[VAR_568_]], [[VAR_569_]], [[VAR_567_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_571_:%.+]] = stablehlo.compare  LT, [[VAR_562_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_572_:%.+]] = stablehlo.add [[VAR_562_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_573_:%.+]] = stablehlo.select [[VAR_571_]], [[VAR_572_]], [[VAR_562_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_574_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_573_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_575_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_570_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_576_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_564_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_577_:%.+]] = stablehlo.real_dynamic_slice [[VAR_565_]], [[VAR_574_]], [[VAR_575_]], [[VAR_576_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_578_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_579_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_580_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_581_:%.+]] = stablehlo.compare  LT, [[VAR_579_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_582_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_581_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_583_:%.+]] = stablehlo.negate [[VAR_579_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_584_:%.+]] = stablehlo.add [[VAR_580_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_585_:%.+]] = stablehlo.add [[VAR_578_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_586_:%.+]] = stablehlo.reverse [[VAR_552_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_587_:%.+]] = stablehlo.select [[VAR_581_]], [[VAR_584_]], [[VAR_578_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_588_:%.+]] = stablehlo.select [[VAR_581_]], [[VAR_585_]], [[VAR_580_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_589_:%.+]] = stablehlo.select [[VAR_581_]], [[VAR_583_]], [[VAR_579_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_590_:%.+]] = stablehlo.select [[VAR_582_]], [[VAR_586_]], [[VAR_552_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_591_:%.+]] = stablehlo.compare  GT, [[VAR_588_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_592_:%.+]] = stablehlo.select [[VAR_591_]], [[VAR_9_]], [[VAR_588_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_593_:%.+]] = stablehlo.compare  LT, [[VAR_592_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_594_:%.+]] = stablehlo.add [[VAR_592_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_595_:%.+]] = stablehlo.select [[VAR_593_]], [[VAR_594_]], [[VAR_592_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_596_:%.+]] = stablehlo.compare  LT, [[VAR_587_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_597_:%.+]] = stablehlo.add [[VAR_587_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_598_:%.+]] = stablehlo.select [[VAR_596_]], [[VAR_597_]], [[VAR_587_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_599_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_598_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_600_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_595_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_601_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_589_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_602_:%.+]] = stablehlo.real_dynamic_slice [[VAR_590_]], [[VAR_599_]], [[VAR_600_]], [[VAR_601_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_603_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_604_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_605_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_606_:%.+]] = stablehlo.compare  LT, [[VAR_604_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_607_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_606_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_608_:%.+]] = stablehlo.negate [[VAR_604_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_609_:%.+]] = stablehlo.add [[VAR_605_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_610_:%.+]] = stablehlo.add [[VAR_603_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_611_:%.+]] = stablehlo.reverse [[VAR_552_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_612_:%.+]] = stablehlo.select [[VAR_606_]], [[VAR_609_]], [[VAR_603_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_613_:%.+]] = stablehlo.select [[VAR_606_]], [[VAR_610_]], [[VAR_605_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_614_:%.+]] = stablehlo.select [[VAR_606_]], [[VAR_608_]], [[VAR_604_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_615_:%.+]] = stablehlo.select [[VAR_607_]], [[VAR_611_]], [[VAR_552_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_616_:%.+]] = stablehlo.compare  GT, [[VAR_613_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_617_:%.+]] = stablehlo.select [[VAR_616_]], [[VAR_9_]], [[VAR_613_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_618_:%.+]] = stablehlo.compare  LT, [[VAR_617_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_619_:%.+]] = stablehlo.add [[VAR_617_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_620_:%.+]] = stablehlo.select [[VAR_618_]], [[VAR_619_]], [[VAR_617_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_621_:%.+]] = stablehlo.compare  LT, [[VAR_612_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_622_:%.+]] = stablehlo.add [[VAR_612_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_623_:%.+]] = stablehlo.select [[VAR_621_]], [[VAR_622_]], [[VAR_612_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_624_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_623_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_625_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_620_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_626_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_614_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_627_:%.+]] = stablehlo.real_dynamic_slice [[VAR_615_]], [[VAR_624_]], [[VAR_625_]], [[VAR_626_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_628_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_629_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_630_:%.+]] = stablehlo.slice [[VAR_9_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_631_:%.+]] = stablehlo.compare  LT, [[VAR_629_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_632_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_631_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_633_:%.+]] = stablehlo.negate [[VAR_629_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_634_:%.+]] = stablehlo.add [[VAR_630_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_635_:%.+]] = stablehlo.add [[VAR_628_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_636_:%.+]] = stablehlo.reverse [[VAR_552_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_637_:%.+]] = stablehlo.select [[VAR_631_]], [[VAR_634_]], [[VAR_628_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_638_:%.+]] = stablehlo.select [[VAR_631_]], [[VAR_635_]], [[VAR_630_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_639_:%.+]] = stablehlo.select [[VAR_631_]], [[VAR_633_]], [[VAR_629_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_640_:%.+]] = stablehlo.select [[VAR_632_]], [[VAR_636_]], [[VAR_552_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_641_:%.+]] = stablehlo.compare  GT, [[VAR_638_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_642_:%.+]] = stablehlo.select [[VAR_641_]], [[VAR_9_]], [[VAR_638_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_643_:%.+]] = stablehlo.compare  LT, [[VAR_642_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_644_:%.+]] = stablehlo.add [[VAR_642_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_645_:%.+]] = stablehlo.select [[VAR_643_]], [[VAR_644_]], [[VAR_642_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_646_:%.+]] = stablehlo.compare  LT, [[VAR_637_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_647_:%.+]] = stablehlo.add [[VAR_637_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_648_:%.+]] = stablehlo.select [[VAR_646_]], [[VAR_647_]], [[VAR_637_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_649_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_648_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_650_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_645_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_651_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_639_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_652_:%.+]] = stablehlo.real_dynamic_slice [[VAR_640_]], [[VAR_649_]], [[VAR_650_]], [[VAR_651_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_653_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_577_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_654_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_147_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_655_:%.+]] = stablehlo.add [[VAR_653_]], [[VAR_654_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_656_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_655_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_657_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_151_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_658_:%.+]] = stablehlo.add [[VAR_656_]], [[VAR_657_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_659_:%.+]] = stablehlo.logistic [[VAR_658_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_660_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_627_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_661_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_149_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_662_:%.+]] = stablehlo.add [[VAR_660_]], [[VAR_661_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_663_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_662_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_664_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_153_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_665_:%.+]] = stablehlo.add [[VAR_663_]], [[VAR_664_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_666_:%.+]] = stablehlo.logistic [[VAR_665_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_667_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_652_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_668_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_150_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_669_:%.+]] = stablehlo.add [[VAR_667_]], [[VAR_668_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_670_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_669_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_671_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_154_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_672_:%.+]] = stablehlo.add [[VAR_670_]], [[VAR_671_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_673_:%.+]] = stablehlo.tanh [[VAR_672_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_674_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_666_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_675_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_122_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_676_:%.+]] = stablehlo.multiply [[VAR_674_]], [[VAR_675_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_677_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_659_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_678_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_673_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_679_:%.+]] = stablehlo.multiply [[VAR_677_]], [[VAR_678_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_680_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_676_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_681_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_679_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_682_:%.+]] = stablehlo.add [[VAR_680_]], [[VAR_681_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_683_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_602_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_684_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_148_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_685_:%.+]] = stablehlo.add [[VAR_683_]], [[VAR_684_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_686_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_685_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_687_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_152_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_688_:%.+]] = stablehlo.add [[VAR_686_]], [[VAR_687_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_689_:%.+]] = stablehlo.logistic [[VAR_688_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_690_:%.+]] = stablehlo.tanh [[VAR_682_]] : tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_691_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_689_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_692_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_690_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_693_:%.+]] = stablehlo.multiply [[VAR_691_]], [[VAR_692_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_694_:%.+]] = stablehlo.dynamic_reshape [[VAR_693_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
-// CHECK-DAG:       [[VAR_695_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_16_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_696_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_17_]], [[VAR_12_]], dims = [0] : (tensor<1xi64>, tensor<1xindex>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_697_:%.+]] = stablehlo.add [[VAR_695_]], [[VAR_696_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_698_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_699_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_700_:%.+]] = stablehlo.slice [[VAR_697_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_701_:%.+]] = stablehlo.compare  LT, [[VAR_699_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_702_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_701_]], [[VAR_4_]], dims = [0] : (tensor<1xi1>, tensor<3xindex>) -> tensor<2x16x512xi1>
-// CHECK-DAG:       [[VAR_703_:%.+]] = stablehlo.negate [[VAR_699_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_704_:%.+]] = stablehlo.add [[VAR_700_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_705_:%.+]] = stablehlo.add [[VAR_698_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_706_:%.+]] = stablehlo.reverse [[PARAM_0_]], dims = [0] : tensor<2x16x512xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_707_:%.+]] = stablehlo.select [[VAR_701_]], [[VAR_704_]], [[VAR_698_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_708_:%.+]] = stablehlo.select [[VAR_701_]], [[VAR_705_]], [[VAR_700_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_709_:%.+]] = stablehlo.select [[VAR_701_]], [[VAR_703_]], [[VAR_699_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_710_:%.+]] = stablehlo.select [[VAR_702_]], [[VAR_706_]], [[PARAM_0_]] : tensor<2x16x512xi1>, tensor<2x16x512xf32>
-// CHECK:           [[VAR_711_:%.+]] = stablehlo.compare  GT, [[VAR_708_]], [[VAR_18_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_712_:%.+]] = stablehlo.select [[VAR_711_]], [[VAR_18_]], [[VAR_708_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_713_:%.+]] = stablehlo.compare  LT, [[VAR_712_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_714_:%.+]] = stablehlo.add [[VAR_712_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_715_:%.+]] = stablehlo.select [[VAR_713_]], [[VAR_714_]], [[VAR_712_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_716_:%.+]] = stablehlo.compare  LT, [[VAR_707_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_717_:%.+]] = stablehlo.add [[VAR_707_]], [[VAR_18_]] : tensor<1xi64>
-// CHECK:           [[VAR_718_:%.+]] = stablehlo.select [[VAR_716_]], [[VAR_717_]], [[VAR_707_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_719_:%.+]] = stablehlo.concatenate [[VAR_718_]], [[VAR_16_]], [[VAR_16_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_720_:%.+]] = stablehlo.concatenate [[VAR_715_]], [[VAR_14_]], [[VAR_11_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK-DAG:       [[VAR_721_:%.+]] = stablehlo.concatenate [[VAR_709_]], [[VAR_17_]], [[VAR_17_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<3xi64>
-// CHECK:           [[VAR_722_:%.+]] = stablehlo.real_dynamic_slice [[VAR_710_]], [[VAR_719_]], [[VAR_720_]], [[VAR_721_]] : (tensor<2x16x512xf32>, tensor<3xi64>, tensor<3xi64>, tensor<3xi64>) -> tensor<1x16x512xf32>
-// CHECK:           [[VAR_723_:%.+]] = stablehlo.dynamic_reshape [[VAR_722_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
-// CHECK-DAG:       [[VAR_724_:%.+]] = stablehlo.broadcast_in_dim [[VAR_723_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
-// CHECK-DAG:       [[VAR_725_:%.+]] = stablehlo.broadcast_in_dim [[VAR_133_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_726_:%.+]] = stablehlo.dot [[VAR_724_]], [[VAR_725_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_727_:%.+]] = stablehlo.broadcast_in_dim [[VAR_693_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_728_:%.+]] = stablehlo.broadcast_in_dim [[VAR_134_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_729_:%.+]] = stablehlo.dot [[VAR_727_]], [[VAR_728_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_730_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_726_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK:           [[VAR_731_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_729_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_732_:%.+]] = stablehlo.add [[VAR_730_]], [[VAR_731_]] : tensor<16x1024xf32>
-// CHECK-DAG:       [[VAR_733_:%.+]] = stablehlo.slice [[VAR_16_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_734_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_735_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_736_:%.+]] = stablehlo.compare  LT, [[VAR_734_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_737_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_736_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_738_:%.+]] = stablehlo.negate [[VAR_734_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_739_:%.+]] = stablehlo.add [[VAR_735_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_740_:%.+]] = stablehlo.add [[VAR_733_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_741_:%.+]] = stablehlo.reverse [[VAR_732_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_742_:%.+]] = stablehlo.select [[VAR_736_]], [[VAR_739_]], [[VAR_733_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_743_:%.+]] = stablehlo.select [[VAR_736_]], [[VAR_740_]], [[VAR_735_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_744_:%.+]] = stablehlo.select [[VAR_736_]], [[VAR_738_]], [[VAR_734_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_745_:%.+]] = stablehlo.select [[VAR_737_]], [[VAR_741_]], [[VAR_732_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_746_:%.+]] = stablehlo.compare  GT, [[VAR_743_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_747_:%.+]] = stablehlo.select [[VAR_746_]], [[VAR_9_]], [[VAR_743_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_748_:%.+]] = stablehlo.compare  LT, [[VAR_747_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_749_:%.+]] = stablehlo.add [[VAR_747_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_750_:%.+]] = stablehlo.select [[VAR_748_]], [[VAR_749_]], [[VAR_747_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_751_:%.+]] = stablehlo.compare  LT, [[VAR_742_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_752_:%.+]] = stablehlo.add [[VAR_742_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_753_:%.+]] = stablehlo.select [[VAR_751_]], [[VAR_752_]], [[VAR_742_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_754_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_753_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_755_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_750_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_756_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_744_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_757_:%.+]] = stablehlo.real_dynamic_slice [[VAR_745_]], [[VAR_754_]], [[VAR_755_]], [[VAR_756_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_758_:%.+]] = stablehlo.slice [[VAR_13_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_759_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_760_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_761_:%.+]] = stablehlo.compare  LT, [[VAR_759_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_762_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_761_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_763_:%.+]] = stablehlo.negate [[VAR_759_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_764_:%.+]] = stablehlo.add [[VAR_760_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_765_:%.+]] = stablehlo.add [[VAR_758_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_766_:%.+]] = stablehlo.reverse [[VAR_732_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_767_:%.+]] = stablehlo.select [[VAR_761_]], [[VAR_764_]], [[VAR_758_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_768_:%.+]] = stablehlo.select [[VAR_761_]], [[VAR_765_]], [[VAR_760_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_769_:%.+]] = stablehlo.select [[VAR_761_]], [[VAR_763_]], [[VAR_759_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_770_:%.+]] = stablehlo.select [[VAR_762_]], [[VAR_766_]], [[VAR_732_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_771_:%.+]] = stablehlo.compare  GT, [[VAR_768_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_772_:%.+]] = stablehlo.select [[VAR_771_]], [[VAR_9_]], [[VAR_768_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_773_:%.+]] = stablehlo.compare  LT, [[VAR_772_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_774_:%.+]] = stablehlo.add [[VAR_772_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_775_:%.+]] = stablehlo.select [[VAR_773_]], [[VAR_774_]], [[VAR_772_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_776_:%.+]] = stablehlo.compare  LT, [[VAR_767_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_777_:%.+]] = stablehlo.add [[VAR_767_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_778_:%.+]] = stablehlo.select [[VAR_776_]], [[VAR_777_]], [[VAR_767_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_779_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_778_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_780_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_775_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_781_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_769_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_782_:%.+]] = stablehlo.real_dynamic_slice [[VAR_770_]], [[VAR_779_]], [[VAR_780_]], [[VAR_781_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_783_:%.+]] = stablehlo.slice [[VAR_11_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_784_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_785_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_786_:%.+]] = stablehlo.compare  LT, [[VAR_784_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_787_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_786_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_788_:%.+]] = stablehlo.negate [[VAR_784_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_789_:%.+]] = stablehlo.add [[VAR_785_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_790_:%.+]] = stablehlo.add [[VAR_783_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_791_:%.+]] = stablehlo.reverse [[VAR_732_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_792_:%.+]] = stablehlo.select [[VAR_786_]], [[VAR_789_]], [[VAR_783_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_793_:%.+]] = stablehlo.select [[VAR_786_]], [[VAR_790_]], [[VAR_785_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_794_:%.+]] = stablehlo.select [[VAR_786_]], [[VAR_788_]], [[VAR_784_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_795_:%.+]] = stablehlo.select [[VAR_787_]], [[VAR_791_]], [[VAR_732_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_796_:%.+]] = stablehlo.compare  GT, [[VAR_793_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_797_:%.+]] = stablehlo.select [[VAR_796_]], [[VAR_9_]], [[VAR_793_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_798_:%.+]] = stablehlo.compare  LT, [[VAR_797_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_799_:%.+]] = stablehlo.add [[VAR_797_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_800_:%.+]] = stablehlo.select [[VAR_798_]], [[VAR_799_]], [[VAR_797_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_801_:%.+]] = stablehlo.compare  LT, [[VAR_792_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_802_:%.+]] = stablehlo.add [[VAR_792_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_803_:%.+]] = stablehlo.select [[VAR_801_]], [[VAR_802_]], [[VAR_792_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_804_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_803_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_805_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_800_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_806_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_794_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_807_:%.+]] = stablehlo.real_dynamic_slice [[VAR_795_]], [[VAR_804_]], [[VAR_805_]], [[VAR_806_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_808_:%.+]] = stablehlo.slice [[VAR_10_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_809_:%.+]] = stablehlo.slice [[VAR_17_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK-DAG:       [[VAR_810_:%.+]] = stablehlo.slice [[VAR_9_]] [0:1] : (tensor<1xi64>) -> tensor<1xi64>
-// CHECK:           [[VAR_811_:%.+]] = stablehlo.compare  LT, [[VAR_809_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_812_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_811_]], [[VAR_2_]], dims = [0] : (tensor<1xi1>, tensor<2xindex>) -> tensor<16x1024xi1>
-// CHECK-DAG:       [[VAR_813_:%.+]] = stablehlo.negate [[VAR_809_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_814_:%.+]] = stablehlo.add [[VAR_810_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_815_:%.+]] = stablehlo.add [[VAR_808_]], [[VAR_17_]] : tensor<1xi64>
-// CHECK-DAG:       [[VAR_816_:%.+]] = stablehlo.reverse [[VAR_732_]], dims = [1] : tensor<16x1024xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_817_:%.+]] = stablehlo.select [[VAR_811_]], [[VAR_814_]], [[VAR_808_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_818_:%.+]] = stablehlo.select [[VAR_811_]], [[VAR_815_]], [[VAR_810_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_819_:%.+]] = stablehlo.select [[VAR_811_]], [[VAR_813_]], [[VAR_809_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_820_:%.+]] = stablehlo.select [[VAR_812_]], [[VAR_816_]], [[VAR_732_]] : tensor<16x1024xi1>, tensor<16x1024xf32>
-// CHECK:           [[VAR_821_:%.+]] = stablehlo.compare  GT, [[VAR_818_]], [[VAR_9_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK:           [[VAR_822_:%.+]] = stablehlo.select [[VAR_821_]], [[VAR_9_]], [[VAR_818_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_823_:%.+]] = stablehlo.compare  LT, [[VAR_822_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_824_:%.+]] = stablehlo.add [[VAR_822_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_825_:%.+]] = stablehlo.select [[VAR_823_]], [[VAR_824_]], [[VAR_822_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_826_:%.+]] = stablehlo.compare  LT, [[VAR_817_]], [[VAR_16_]],  NOTYPE : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
-// CHECK-DAG:       [[VAR_827_:%.+]] = stablehlo.add [[VAR_817_]], [[VAR_9_]] : tensor<1xi64>
-// CHECK:           [[VAR_828_:%.+]] = stablehlo.select [[VAR_826_]], [[VAR_827_]], [[VAR_817_]] : tensor<1xi1>, tensor<1xi64>
-// CHECK-DAG:       [[VAR_829_:%.+]] = stablehlo.concatenate [[VAR_16_]], [[VAR_828_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_830_:%.+]] = stablehlo.concatenate [[VAR_14_]], [[VAR_825_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-DAG:       [[VAR_831_:%.+]] = stablehlo.concatenate [[VAR_17_]], [[VAR_819_]], dim = 0 : (tensor<1xi64>, tensor<1xi64>) -> tensor<2xi64>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_832_:%.+]] = stablehlo.real_dynamic_slice [[VAR_820_]], [[VAR_829_]], [[VAR_830_]], [[VAR_831_]] : (tensor<16x1024xf32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_833_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_757_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_834_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_147_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_835_:%.+]] = stablehlo.add [[VAR_833_]], [[VAR_834_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_836_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_835_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_837_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_151_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_838_:%.+]] = stablehlo.add [[VAR_836_]], [[VAR_837_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_839_:%.+]] = stablehlo.logistic [[VAR_838_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_840_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_807_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_841_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_149_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_842_:%.+]] = stablehlo.add [[VAR_840_]], [[VAR_841_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_843_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_842_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_844_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_153_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_845_:%.+]] = stablehlo.add [[VAR_843_]], [[VAR_844_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_846_:%.+]] = stablehlo.logistic [[VAR_845_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_847_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_832_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_848_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_150_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_849_:%.+]] = stablehlo.add [[VAR_847_]], [[VAR_848_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_850_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_849_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_851_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_154_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_852_:%.+]] = stablehlo.add [[VAR_850_]], [[VAR_851_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_853_:%.+]] = stablehlo.tanh [[VAR_852_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_854_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_846_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_855_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_682_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_856_:%.+]] = stablehlo.multiply [[VAR_854_]], [[VAR_855_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_857_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_839_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_858_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_853_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_859_:%.+]] = stablehlo.multiply [[VAR_857_]], [[VAR_858_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_860_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_856_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_861_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_859_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_862_:%.+]] = stablehlo.add [[VAR_860_]], [[VAR_861_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_863_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_782_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_864_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_148_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_865_:%.+]] = stablehlo.add [[VAR_863_]], [[VAR_864_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_866_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_865_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_867_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_152_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_868_:%.+]] = stablehlo.add [[VAR_866_]], [[VAR_867_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_869_:%.+]] = stablehlo.logistic [[VAR_868_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_870_:%.+]] = stablehlo.tanh [[VAR_862_]] : tensor<16x256xf32>
-// CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_871_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_869_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_872_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_870_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
-// CHECK:           [[VAR_873_:%.+]] = stablehlo.multiply [[VAR_871_]], [[VAR_872_]] : tensor<16x256xf32>
-// CHECK-DAG:       [[VAR_874_:%.+]] = stablehlo.dynamic_reshape [[VAR_873_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
-// CHECK-DAG:       [[VAR_875_:%.+]] = stablehlo.concatenate [[VAR_334_]], [[VAR_514_]], dim = 0 : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
-// CHECK:           [[VAR_876_:%.+]] = stablehlo.concatenate [[VAR_874_]], [[VAR_694_]], dim = 0 : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
-// CHECK:           [[VAR_877_:%.+]] = stablehlo.concatenate [[VAR_875_]], [[VAR_876_]], dim = 1 : (tensor<2x1x16x256xf32>, tensor<2x1x16x256xf32>) -> tensor<2x2x16x256xf32>
-// CHECK:           return [[VAR_877_]] : tensor<2x2x16x256xf32>
+// CHECK-DAG:       [[VAR_4_:%.+]] = shape.const_shape [2048] : tensor<1xindex>
+// CHECK-DAG:       [[VAR_5_:%.+]] = shape.const_shape [1024, 256] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_6_:%.+]] = shape.const_shape [1024, 512] : tensor<2xindex>
+// CHECK-DAG:       [[VAR_7_:%.+]] = stablehlo.constant dense<1> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_8_:%.+]] = stablehlo.constant dense<768> : tensor<i64>
+// CHECK-DAG:       [[VAR_9_:%.+]] = stablehlo.constant dense<512> : tensor<i64>
+// CHECK-DAG:       [[VAR_10_:%.+]] = stablehlo.constant dense<256> : tensor<i64>
+// CHECK-DAG:       [[VAR_11_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<2x16x256xf32>
+// CHECK-DAG:       [[VAR_12_:%.+]] = stablehlo.constant dense<0> : tensor<i64>
+// CHECK-DAG:       [[VAR_13_:%.+]] = stablehlo.constant dense<0> : tensor<1xi64>
+// CHECK-DAG:       [[VAR_14_:%.+]] = stablehlo.constant dense<1> : tensor<i64>
+// CHECK:           [[VAR_15_:%.+]] = stablehlo.dynamic_slice [[VAR_11_]], [[VAR_12_]], [[VAR_12_]], [[VAR_12_]], sizes = [1, 16, 256] : (tensor<2x16x256xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x256xf32>
+// CHECK-DAG:       [[VAR_16_:%.+]] = stablehlo.dynamic_reshape [[VAR_15_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_17_:%.+]] = stablehlo.dynamic_slice [[VAR_11_]], [[VAR_12_]], [[VAR_12_]], [[VAR_12_]], sizes = [1, 16, 256] : (tensor<2x16x256xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_18_:%.+]] = stablehlo.dynamic_reshape [[VAR_17_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_19_:%.+]] = stablehlo.dynamic_slice [[VAR_11_]], [[VAR_14_]], [[VAR_12_]], [[VAR_12_]], sizes = [1, 16, 256] : (tensor<2x16x256xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_20_:%.+]] = stablehlo.dynamic_reshape [[VAR_19_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_21_:%.+]] = stablehlo.dynamic_slice [[VAR_11_]], [[VAR_14_]], [[VAR_12_]], [[VAR_12_]], sizes = [1, 16, 256] : (tensor<2x16x256xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_22_:%.+]] = stablehlo.dynamic_reshape [[VAR_21_]], [[VAR_0_]] : (tensor<1x16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_23_:%.+]] = stablehlo.slice [[PARAM_2_]] [0:1, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-DAG:       [[VAR_24_:%.+]] = stablehlo.slice [[PARAM_2_]] [1:2, 0:1024, 0:512] : (tensor<2x1024x512xf32>) -> tensor<1x1024x512xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_25_:%.+]] = stablehlo.dynamic_reshape [[VAR_23_]], [[VAR_6_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_26_:%.+]] = stablehlo.dynamic_reshape [[VAR_24_]], [[VAR_6_]] : (tensor<1x1024x512xf32>, tensor<2xindex>) -> tensor<1024x512xf32>
+// CHECK-DAG:       [[VAR_27_:%.+]] = stablehlo.slice [[PARAM_3_]] [0:1, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-DAG:       [[VAR_28_:%.+]] = stablehlo.slice [[PARAM_3_]] [1:2, 0:1024, 0:256] : (tensor<2x1024x256xf32>) -> tensor<1x1024x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_29_:%.+]] = stablehlo.dynamic_reshape [[VAR_27_]], [[VAR_5_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_30_:%.+]] = stablehlo.dynamic_reshape [[VAR_28_]], [[VAR_5_]] : (tensor<1x1024x256xf32>, tensor<2xindex>) -> tensor<1024x256xf32>
+// CHECK-DAG:       [[VAR_31_:%.+]] = stablehlo.transpose [[VAR_25_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_32_:%.+]] = stablehlo.transpose [[VAR_29_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_33_:%.+]] = stablehlo.transpose [[VAR_26_]], dims = [1, 0] : (tensor<1024x512xf32>) -> tensor<512x1024xf32>
+// CHECK-DAG:       [[VAR_34_:%.+]] = stablehlo.transpose [[VAR_30_]], dims = [1, 0] : (tensor<1024x256xf32>) -> tensor<256x1024xf32>
+// CHECK-DAG:       [[VAR_35_:%.+]] = stablehlo.slice [[PARAM_1_]] [0:1, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-DAG:       [[VAR_36_:%.+]] = stablehlo.slice [[PARAM_1_]] [1:2, 0:2048] : (tensor<2x2048xf32>) -> tensor<1x2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_37_:%.+]] = stablehlo.dynamic_reshape [[VAR_35_]], [[VAR_4_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
+// CHECK-DAG:       [[VAR_38_:%.+]] = stablehlo.dynamic_reshape [[VAR_36_]], [[VAR_4_]] : (tensor<1x2048xf32>, tensor<1xindex>) -> tensor<2048xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_39_:%.+]] = stablehlo.slice [[VAR_37_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_40_:%.+]] = stablehlo.slice [[VAR_37_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_41_:%.+]] = stablehlo.slice [[VAR_37_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_42_:%.+]] = stablehlo.slice [[VAR_37_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_43_:%.+]] = stablehlo.slice [[VAR_37_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_44_:%.+]] = stablehlo.slice [[VAR_37_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_45_:%.+]] = stablehlo.slice [[VAR_37_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_46_:%.+]] = stablehlo.slice [[VAR_37_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_47_:%.+]] = stablehlo.slice [[VAR_38_]] [0:256] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_48_:%.+]] = stablehlo.slice [[VAR_38_]] [256:512] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_49_:%.+]] = stablehlo.slice [[VAR_38_]] [512:768] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_50_:%.+]] = stablehlo.slice [[VAR_38_]] [768:1024] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_51_:%.+]] = stablehlo.slice [[VAR_38_]] [1024:1280] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_52_:%.+]] = stablehlo.slice [[VAR_38_]] [1280:1536] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_53_:%.+]] = stablehlo.slice [[VAR_38_]] [1536:1792] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_54_:%.+]] = stablehlo.slice [[VAR_38_]] [1792:2048] : (tensor<2048xf32>) -> tensor<256xf32>
+// CHECK-DAG:       [[VAR_55_:%.+]] = stablehlo.reshape [[VAR_13_]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:           [[VAR_56_:%.+]] = stablehlo.dynamic_slice [[PARAM_0_]], [[VAR_55_]], [[VAR_12_]], [[VAR_12_]], sizes = [1, 16, 512] : (tensor<2x16x512xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_57_:%.+]] = stablehlo.dynamic_reshape [[VAR_56_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_58_:%.+]] = stablehlo.broadcast_in_dim [[VAR_57_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_59_:%.+]] = stablehlo.broadcast_in_dim [[VAR_31_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_60_:%.+]] = stablehlo.dot [[VAR_58_]], [[VAR_59_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_61_:%.+]] = stablehlo.broadcast_in_dim [[VAR_16_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_62_:%.+]] = stablehlo.broadcast_in_dim [[VAR_32_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_63_:%.+]] = stablehlo.dot [[VAR_61_]], [[VAR_62_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_64_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_60_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_65_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_63_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_66_:%.+]] = stablehlo.add [[VAR_64_]], [[VAR_65_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_67_:%.+]] = stablehlo.dynamic_slice [[VAR_66_]], [[VAR_12_]], [[VAR_12_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_68_:%.+]] = stablehlo.dynamic_slice [[VAR_66_]], [[VAR_12_]], [[VAR_10_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_69_:%.+]] = stablehlo.dynamic_slice [[VAR_66_]], [[VAR_12_]], [[VAR_9_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_70_:%.+]] = stablehlo.dynamic_slice [[VAR_66_]], [[VAR_12_]], [[VAR_8_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_71_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_67_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_72_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_39_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_73_:%.+]] = stablehlo.add [[VAR_71_]], [[VAR_72_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_74_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_73_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_75_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_43_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_76_:%.+]] = stablehlo.add [[VAR_74_]], [[VAR_75_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_77_:%.+]] = stablehlo.logistic [[VAR_76_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_78_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_69_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_79_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_41_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_80_:%.+]] = stablehlo.add [[VAR_78_]], [[VAR_79_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_81_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_80_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_82_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_45_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_83_:%.+]] = stablehlo.add [[VAR_81_]], [[VAR_82_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_84_:%.+]] = stablehlo.logistic [[VAR_83_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_85_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_70_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_86_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_42_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_87_:%.+]] = stablehlo.add [[VAR_85_]], [[VAR_86_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_88_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_87_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_89_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_46_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_90_:%.+]] = stablehlo.add [[VAR_88_]], [[VAR_89_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_91_:%.+]] = stablehlo.tanh [[VAR_90_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_92_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_84_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_93_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_18_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_94_:%.+]] = stablehlo.multiply [[VAR_92_]], [[VAR_93_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_95_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_77_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_96_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_91_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_97_:%.+]] = stablehlo.multiply [[VAR_95_]], [[VAR_96_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_98_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_94_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_99_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_97_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_100_:%.+]] = stablehlo.add [[VAR_98_]], [[VAR_99_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_101_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_68_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_102_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_40_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_103_:%.+]] = stablehlo.add [[VAR_101_]], [[VAR_102_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_104_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_103_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_105_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_44_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_106_:%.+]] = stablehlo.add [[VAR_104_]], [[VAR_105_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_107_:%.+]] = stablehlo.logistic [[VAR_106_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_108_:%.+]] = stablehlo.tanh [[VAR_100_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_109_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_107_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_110_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_108_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_111_:%.+]] = stablehlo.multiply [[VAR_109_]], [[VAR_110_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_112_:%.+]] = stablehlo.dynamic_reshape [[VAR_111_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_113_:%.+]] = stablehlo.reshape [[VAR_7_]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:           [[VAR_114_:%.+]] = stablehlo.dynamic_slice [[PARAM_0_]], [[VAR_113_]], [[VAR_12_]], [[VAR_12_]], sizes = [1, 16, 512] : (tensor<2x16x512xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_115_:%.+]] = stablehlo.dynamic_reshape [[VAR_114_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_116_:%.+]] = stablehlo.broadcast_in_dim [[VAR_115_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_117_:%.+]] = stablehlo.broadcast_in_dim [[VAR_31_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_118_:%.+]] = stablehlo.dot [[VAR_116_]], [[VAR_117_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_119_:%.+]] = stablehlo.broadcast_in_dim [[VAR_111_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_120_:%.+]] = stablehlo.broadcast_in_dim [[VAR_32_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_121_:%.+]] = stablehlo.dot [[VAR_119_]], [[VAR_120_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_122_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_118_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_123_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_121_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_124_:%.+]] = stablehlo.add [[VAR_122_]], [[VAR_123_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_125_:%.+]] = stablehlo.dynamic_slice [[VAR_124_]], [[VAR_12_]], [[VAR_12_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_126_:%.+]] = stablehlo.dynamic_slice [[VAR_124_]], [[VAR_12_]], [[VAR_10_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_127_:%.+]] = stablehlo.dynamic_slice [[VAR_124_]], [[VAR_12_]], [[VAR_9_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_128_:%.+]] = stablehlo.dynamic_slice [[VAR_124_]], [[VAR_12_]], [[VAR_8_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_129_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_125_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_130_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_39_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_131_:%.+]] = stablehlo.add [[VAR_129_]], [[VAR_130_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_132_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_131_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_133_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_43_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_134_:%.+]] = stablehlo.add [[VAR_132_]], [[VAR_133_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_135_:%.+]] = stablehlo.logistic [[VAR_134_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_136_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_127_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_137_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_41_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_138_:%.+]] = stablehlo.add [[VAR_136_]], [[VAR_137_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_139_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_138_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_140_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_45_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_141_:%.+]] = stablehlo.add [[VAR_139_]], [[VAR_140_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_142_:%.+]] = stablehlo.logistic [[VAR_141_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_143_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_128_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_144_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_42_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_145_:%.+]] = stablehlo.add [[VAR_143_]], [[VAR_144_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_146_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_145_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_147_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_46_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_148_:%.+]] = stablehlo.add [[VAR_146_]], [[VAR_147_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_149_:%.+]] = stablehlo.tanh [[VAR_148_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_150_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_142_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_151_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_100_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_152_:%.+]] = stablehlo.multiply [[VAR_150_]], [[VAR_151_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_153_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_135_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_154_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_149_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_155_:%.+]] = stablehlo.multiply [[VAR_153_]], [[VAR_154_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_156_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_152_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_157_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_155_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_158_:%.+]] = stablehlo.add [[VAR_156_]], [[VAR_157_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_159_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_126_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_160_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_40_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_161_:%.+]] = stablehlo.add [[VAR_159_]], [[VAR_160_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_162_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_161_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_163_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_44_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_164_:%.+]] = stablehlo.add [[VAR_162_]], [[VAR_163_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_165_:%.+]] = stablehlo.logistic [[VAR_164_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_166_:%.+]] = stablehlo.tanh [[VAR_158_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_167_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_165_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_168_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_166_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_169_:%.+]] = stablehlo.multiply [[VAR_167_]], [[VAR_168_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_170_:%.+]] = stablehlo.dynamic_reshape [[VAR_169_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_171_:%.+]] = stablehlo.reshape [[VAR_7_]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:           [[VAR_172_:%.+]] = stablehlo.dynamic_slice [[PARAM_0_]], [[VAR_171_]], [[VAR_12_]], [[VAR_12_]], sizes = [1, 16, 512] : (tensor<2x16x512xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_173_:%.+]] = stablehlo.dynamic_reshape [[VAR_172_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_174_:%.+]] = stablehlo.broadcast_in_dim [[VAR_173_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_175_:%.+]] = stablehlo.broadcast_in_dim [[VAR_33_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_176_:%.+]] = stablehlo.dot [[VAR_174_]], [[VAR_175_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_177_:%.+]] = stablehlo.broadcast_in_dim [[VAR_20_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_178_:%.+]] = stablehlo.broadcast_in_dim [[VAR_34_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_179_:%.+]] = stablehlo.dot [[VAR_177_]], [[VAR_178_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_180_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_176_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_181_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_179_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_182_:%.+]] = stablehlo.add [[VAR_180_]], [[VAR_181_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_183_:%.+]] = stablehlo.dynamic_slice [[VAR_182_]], [[VAR_12_]], [[VAR_12_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_184_:%.+]] = stablehlo.dynamic_slice [[VAR_182_]], [[VAR_12_]], [[VAR_10_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_185_:%.+]] = stablehlo.dynamic_slice [[VAR_182_]], [[VAR_12_]], [[VAR_9_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_186_:%.+]] = stablehlo.dynamic_slice [[VAR_182_]], [[VAR_12_]], [[VAR_8_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_187_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_183_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_188_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_47_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_189_:%.+]] = stablehlo.add [[VAR_187_]], [[VAR_188_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_190_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_189_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_191_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_51_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_192_:%.+]] = stablehlo.add [[VAR_190_]], [[VAR_191_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_193_:%.+]] = stablehlo.logistic [[VAR_192_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_194_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_185_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_195_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_49_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_196_:%.+]] = stablehlo.add [[VAR_194_]], [[VAR_195_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_197_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_196_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_198_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_53_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_199_:%.+]] = stablehlo.add [[VAR_197_]], [[VAR_198_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_200_:%.+]] = stablehlo.logistic [[VAR_199_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_201_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_186_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_202_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_50_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_203_:%.+]] = stablehlo.add [[VAR_201_]], [[VAR_202_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_204_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_203_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_205_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_54_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_206_:%.+]] = stablehlo.add [[VAR_204_]], [[VAR_205_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_207_:%.+]] = stablehlo.tanh [[VAR_206_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_208_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_200_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_209_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_22_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_210_:%.+]] = stablehlo.multiply [[VAR_208_]], [[VAR_209_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_211_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_193_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_212_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_207_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_213_:%.+]] = stablehlo.multiply [[VAR_211_]], [[VAR_212_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_214_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_210_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_215_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_213_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_216_:%.+]] = stablehlo.add [[VAR_214_]], [[VAR_215_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_217_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_184_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_218_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_48_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_219_:%.+]] = stablehlo.add [[VAR_217_]], [[VAR_218_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_220_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_219_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_221_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_52_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_222_:%.+]] = stablehlo.add [[VAR_220_]], [[VAR_221_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_223_:%.+]] = stablehlo.logistic [[VAR_222_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_224_:%.+]] = stablehlo.tanh [[VAR_216_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_225_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_223_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_226_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_224_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_227_:%.+]] = stablehlo.multiply [[VAR_225_]], [[VAR_226_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_228_:%.+]] = stablehlo.dynamic_reshape [[VAR_227_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_229_:%.+]] = stablehlo.reshape [[VAR_13_]] : (tensor<1xi64>) -> tensor<i64>
+// CHECK:           [[VAR_230_:%.+]] = stablehlo.dynamic_slice [[PARAM_0_]], [[VAR_229_]], [[VAR_12_]], [[VAR_12_]], sizes = [1, 16, 512] : (tensor<2x16x512xf32>, tensor<i64>, tensor<i64>, tensor<i64>) -> tensor<1x16x512xf32>
+// CHECK:           [[VAR_231_:%.+]] = stablehlo.dynamic_reshape [[VAR_230_]], [[VAR_3_]] : (tensor<1x16x512xf32>, tensor<2xindex>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_232_:%.+]] = stablehlo.broadcast_in_dim [[VAR_231_]], dims = [0, 1] : (tensor<16x512xf32>) -> tensor<16x512xf32>
+// CHECK-DAG:       [[VAR_233_:%.+]] = stablehlo.broadcast_in_dim [[VAR_33_]], dims = [0, 1] : (tensor<512x1024xf32>) -> tensor<512x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_234_:%.+]] = stablehlo.dot [[VAR_232_]], [[VAR_233_]] : (tensor<16x512xf32>, tensor<512x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_235_:%.+]] = stablehlo.broadcast_in_dim [[VAR_227_]], dims = [0, 1] : (tensor<16x256xf32>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_236_:%.+]] = stablehlo.broadcast_in_dim [[VAR_34_]], dims = [0, 1] : (tensor<256x1024xf32>) -> tensor<256x1024xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_237_:%.+]] = stablehlo.dot [[VAR_235_]], [[VAR_236_]] : (tensor<16x256xf32>, tensor<256x1024xf32>) -> tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_238_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_234_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_239_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_237_]], [[VAR_2_]], dims = [0, 1] : (tensor<16x1024xf32>, tensor<2xindex>) -> tensor<16x1024xf32>
+// CHECK:           [[VAR_240_:%.+]] = stablehlo.add [[VAR_238_]], [[VAR_239_]] : tensor<16x1024xf32>
+// CHECK-DAG:       [[VAR_241_:%.+]] = stablehlo.dynamic_slice [[VAR_240_]], [[VAR_12_]], [[VAR_12_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_242_:%.+]] = stablehlo.dynamic_slice [[VAR_240_]], [[VAR_12_]], [[VAR_10_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_243_:%.+]] = stablehlo.dynamic_slice [[VAR_240_]], [[VAR_12_]], [[VAR_9_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_244_:%.+]] = stablehlo.dynamic_slice [[VAR_240_]], [[VAR_12_]], [[VAR_8_]], sizes = [16, 256] : (tensor<16x1024xf32>, tensor<i64>, tensor<i64>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_245_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_241_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_246_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_47_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_247_:%.+]] = stablehlo.add [[VAR_245_]], [[VAR_246_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_248_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_247_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_249_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_51_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_250_:%.+]] = stablehlo.add [[VAR_248_]], [[VAR_249_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_251_:%.+]] = stablehlo.logistic [[VAR_250_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_252_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_243_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_253_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_49_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_254_:%.+]] = stablehlo.add [[VAR_252_]], [[VAR_253_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_255_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_254_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_256_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_53_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_257_:%.+]] = stablehlo.add [[VAR_255_]], [[VAR_256_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_258_:%.+]] = stablehlo.logistic [[VAR_257_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_259_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_244_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_260_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_50_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_261_:%.+]] = stablehlo.add [[VAR_259_]], [[VAR_260_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_262_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_261_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_263_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_54_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_264_:%.+]] = stablehlo.add [[VAR_262_]], [[VAR_263_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_265_:%.+]] = stablehlo.tanh [[VAR_264_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_266_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_258_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_267_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_216_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_268_:%.+]] = stablehlo.multiply [[VAR_266_]], [[VAR_267_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_269_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_251_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_270_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_265_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_271_:%.+]] = stablehlo.multiply [[VAR_269_]], [[VAR_270_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_272_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_268_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_273_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_271_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_274_:%.+]] = stablehlo.add [[VAR_272_]], [[VAR_273_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_275_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_242_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_276_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_48_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_277_:%.+]] = stablehlo.add [[VAR_275_]], [[VAR_276_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_278_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_277_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_279_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_52_]], [[VAR_0_]], dims = [1] : (tensor<256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_280_:%.+]] = stablehlo.add [[VAR_278_]], [[VAR_279_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_281_:%.+]] = stablehlo.logistic [[VAR_280_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_282_:%.+]] = stablehlo.tanh [[VAR_274_]] : tensor<16x256xf32>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_283_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_281_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_284_:%.+]] = stablehlo.dynamic_broadcast_in_dim [[VAR_282_]], [[VAR_0_]], dims = [0, 1] : (tensor<16x256xf32>, tensor<2xindex>) -> tensor<16x256xf32>
+// CHECK:           [[VAR_285_:%.+]] = stablehlo.multiply [[VAR_283_]], [[VAR_284_]] : tensor<16x256xf32>
+// CHECK-DAG:       [[VAR_286_:%.+]] = stablehlo.dynamic_reshape [[VAR_285_]], [[VAR_1_]] : (tensor<16x256xf32>, tensor<4xindex>) -> tensor<1x1x16x256xf32>
+// CHECK-DAG:       [[VAR_287_:%.+]] = stablehlo.concatenate [[VAR_112_]], [[VAR_170_]], dim = 0 : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
+// CHECK:           [[VAR_288_:%.+]] = stablehlo.concatenate [[VAR_286_]], [[VAR_228_]], dim = 0 : (tensor<1x1x16x256xf32>, tensor<1x1x16x256xf32>) -> tensor<2x1x16x256xf32>
+// CHECK:           [[VAR_289_:%.+]] = stablehlo.concatenate [[VAR_287_]], [[VAR_288_]], dim = 1 : (tensor<2x1x16x256xf32>, tensor<2x1x16x256xf32>) -> tensor<2x2x16x256xf32>
+// CHECK:           return [[VAR_289_]] : tensor<2x2x16x256xf32>
 // CHECK:         }
 }

From 937ec34977580903643af69505c70c7aae4bfdf3 Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Wed, 27 Dec 2023 01:34:17 -0500
Subject: [PATCH 05/11] add HardSigmoid

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 .../ONNXToStableHlo/Math/Elementwise.cpp      | 31 +++++++++++++++++++
 .../onnx_to_stablehlo/Math/Elementwise.mlir   | 17 ++++++++++
 2 files changed, 48 insertions(+)

diff --git a/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp b/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
index a161d0bea4..e02728f360 100644
--- a/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
@@ -173,6 +173,36 @@ struct ONNXElementwiseUnaryOpLoweringToStableHlo : public ConversionPattern {
   }
 };

+// ONNXHardSigmoid(x) = max(0, min(1, alpha * x + beta))
+template <>
+struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXHardSigmoidOp>
+    : public ConversionPattern {
+  ONNXElementwiseUnaryOpLoweringToStableHlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXHardSigmoidOp::getOperationName(), 1, ctx) {}
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    ONNXHardSigmoidOpAdaptor operandAdaptor(operands);
+    ONNXHardSigmoidOp HardSigmoidOp = llvm::cast<ONNXHardSigmoidOp>(op);
+    double alpha = HardSigmoidOp.getAlpha().convertToDouble();
+    double beta = HardSigmoidOp.getBeta().convertToDouble();
+    Value inp = operandAdaptor.getX();
+    ShapedType inpType = inp.getType().dyn_cast_or_null<ShapedType>();
+    if (inpType == nullptr)
+      return failure();
+    Value alphaVal = getShapedFloat(loc, rewriter, alpha, inp);
+    Value betaVal = getShapedFloat(loc, rewriter, beta, inp);
+    Value zeroVal = getShapedFloat(loc, rewriter, 0.0f, inp);
+    Value oneVal = getShapedFloat(loc, rewriter, 1.0f, inp);
+    Value productVal = rewriter.create<stablehlo::MulOp>(loc, inp, alphaVal);
+    Value sumVal = rewriter.create<stablehlo::AddOp>(loc, productVal, betaVal);
+    Value resultOp =
+        rewriter.create<stablehlo::ClampOp>(loc, zeroVal, sumVal, oneVal);
+    rewriter.replaceOp(op, resultOp);
+    return success();
+  }
+};
+
 // ONNXReluOp(x) is implemented using StableHlo Max(x, 0)
 template <>
 struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXReluOp>
@@ -376,6 +406,7 @@ void populateLoweringONNXElementwiseOpToStableHloPattern(
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCeilOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCosOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXExpOp>,
+      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXHardSigmoidOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXLeakyReluOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXLogOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXNegOp>,
diff --git a/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir b/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir
index 73ec128390..78240e19af 100644
--- a/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir
+++ b/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir
@@ -116,6 +116,23 @@ func.func @test_dynamic_sigmoid(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {

 // -----

+func.func @test_hard_sigmoid(%arg0 : tensor<20x40xf32>) -> tensor<20x40xf32> {
+  %0 = "onnx.HardSigmoid"(%arg0) {alpha = 5.000000e-01 : f32, beta = 5.000000e-01 : f32} : (tensor<20x40xf32>) -> tensor<20x40xf32>
+  "func.return"(%0) : (tensor<20x40xf32>) -> ()
+// CHECK-LABEL:  func.func @test_hard_sigmoid
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<20x40xf32>) -> tensor<20x40xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = stablehlo.constant dense<5.000000e-01> : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_1_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_2_:%.+]] = stablehlo.constant dense<1.000000e+00> : tensor<20x40xf32>
+// CHECK:           [[VAR_3_:%.+]] = stablehlo.multiply [[PARAM_0_]], [[VAR_0_]] : tensor<20x40xf32>
+// CHECK:           [[VAR_4_:%.+]] = stablehlo.add [[VAR_3_]], [[VAR_0_]] : tensor<20x40xf32>
+// CHECK:           [[VAR_5_:%.+]] = stablehlo.clamp [[VAR_1_]], [[VAR_4_]], [[VAR_2_]] : tensor<20x40xf32>
+// CHECK:           return [[VAR_5_]] : tensor<20x40xf32>
+// CHECK:         }
+}
+
+// -----
+
 func.func @test_abs(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
   %0 = "onnx.Abs"(%arg0) : (tensor<10x10xf32>) -> tensor<10x10xf32>
   "func.return"(%0) : (tensor<10x10xf32>) -> ()

From 1cc8fe833654262f0202560626c35645799b7ab5 Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Wed, 27 Dec 2023 01:49:07 -0500
Subject: [PATCH 06/11] add elu

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 .../ONNXToStableHlo/Math/Elementwise.cpp      | 34 +++++++++++++++++++
 .../onnx_to_stablehlo/Math/Elementwise.mlir   | 19 +++++++++++
 2 files changed, 53 insertions(+)

diff --git a/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp b/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
index e02728f360..1ae8e6d018 100644
--- a/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
@@ -173,6 +173,39 @@ struct ONNXElementwiseUnaryOpLoweringToStableHlo : public ConversionPattern {
   }
 };

+// ONNXElu(x) = alpha * (exp(x) - 1.) for x < 0, f(x) = x for x >= 0.
+template <>
+struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXEluOp>
+    : public ConversionPattern {
+  ONNXElementwiseUnaryOpLoweringToStableHlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXEluOp::getOperationName(), 1, ctx) {}
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    ONNXEluOpAdaptor operandAdaptor(operands);
+    ONNXEluOp EluOp = llvm::cast<ONNXEluOp>(op);
+    double alpha = EluOp.getAlpha().convertToDouble();
+
+    Type resultType = *op->result_type_begin();
+    Value inp = operandAdaptor.getX();
+    ShapedType inpType = inp.getType().dyn_cast_or_null<ShapedType>();
+    if (inpType == nullptr)
+      return failure();
+    Value alphaVal = getShapedFloat(loc, rewriter, alpha, inp);
+    Value oneVal = getShapedFloat(loc, rewriter, 1.0f, inp);
+    Value expVal = rewriter.create<stablehlo::ExpOp>(loc, inp);
+    Value subVal = rewriter.create<stablehlo::SubtractOp>(loc, expVal, oneVal);
+    Value mulVal = rewriter.create<stablehlo::MulOp>(loc, alphaVal, subVal);
+    Value broadcastedZero = getShapedZero(loc, rewriter, inp);
+    Value compareGeZero = rewriter.create<stablehlo::CompareOp>(
+        loc, inp, broadcastedZero, stablehlo::ComparisonDirection::GE);
+    Value resultOp = rewriter.create<stablehlo::SelectOp>(
+        loc, resultType, compareGeZero, inp, mulVal);
+    rewriter.replaceOp(op, resultOp);
+    return success();
+  }
+};
+
 // ONNXHardSigmoid(x) = max(0, min(1, alpha * x + beta))
 template <>
 struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXHardSigmoidOp>
@@ -405,6 +438,7 @@ void populateLoweringONNXElementwiseOpToStableHloPattern(
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCastOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCeilOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCosOp>,
+      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXEluOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXExpOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXHardSigmoidOp>,
       ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXLeakyReluOp>,
diff --git a/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir b/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir
index 78240e19af..733bd440e7 100644
--- a/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir
+++ b/test/mlir/conversion/onnx_to_stablehlo/Math/Elementwise.mlir
@@ -48,6 +48,25 @@ func.func @test_relu(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {

 // -----

+func.func @test_elu(%arg0 : tensor<20x40xf32>) -> tensor<20x40xf32> {
+  %0 = "onnx.Elu"(%arg0) {alpha = 1.500000e+00 : f32} : (tensor<20x40xf32>) -> tensor<20x40xf32>
+  "func.return"(%0) : (tensor<20x40xf32>) -> ()
+// CHECK-LABEL:  func.func @test_elu
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<20x40xf32>) -> tensor<20x40xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = stablehlo.constant dense<0.000000e+00> : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_1_:%.+]] = stablehlo.constant dense<1.500000e+00> : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_2_:%.+]] = stablehlo.constant dense<1.000000e+00> : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_3_:%.+]] = stablehlo.exponential [[PARAM_0_]] : tensor<20x40xf32>
+// CHECK:           [[VAR_4_:%.+]] = stablehlo.subtract [[VAR_3_]], [[VAR_2_]] : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_5_:%.+]] = stablehlo.multiply [[VAR_1_]], [[VAR_4_]] : tensor<20x40xf32>
+// CHECK-DAG:       [[VAR_6_:%.+]] = stablehlo.compare  GE, [[PARAM_0_]], [[VAR_0_]],  NOTYPE : (tensor<20x40xf32>, tensor<20x40xf32>) -> tensor<20x40xi1>
+// CHECK:           [[VAR_7_:%.+]] = stablehlo.select [[VAR_6_]], [[PARAM_0_]], [[VAR_5_]] : tensor<20x40xi1>, tensor<20x40xf32>
+// CHECK:           return [[VAR_7_]] : tensor<20x40xf32>
+// CHECK:         }
+}
+
+// -----
+
 func.func @test_relu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
   %0 = "onnx.Relu"(%arg0) : (tensor<?x10xf32>) -> tensor<?x10xf32>
   "func.return"(%0) : (tensor<?x10xf32>) -> ()

From 8d4810eb35957186f1d3379351b034600a701cd9 Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Wed, 27 Dec 2023 03:37:24 -0500
Subject: [PATCH 07/11] add DepthToSpace

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 src/Conversion/ONNXToStableHlo/CMakeLists.txt |   1 +
 .../ConvertONNXToStableHlo.cpp                |   1 +
 .../ONNXToStableHlo/DialectBuilder.cpp        | 113 +++++++++++++++---
 .../ONNXToStableHlo/DialectBuilder.hpp        |  42 ++++++-
 .../ONNXToStableHlo/ONNXToStableHloCommon.hpp |  12 +-
 .../ONNXToStableHlo/Tensor/DepthToSpace.cpp   |  99 +++++++++++++++
 .../ONNXToStableHlo/Tensor/Expand.cpp         |   6 +-
 .../ONNXToStableHlo/Tensor/OneHot.cpp         |   4 +
 .../ONNXToStableHlo/Tensor/ScatterND.cpp      |   4 +
 src/Dialect/Mlir/DialectBuilder.cpp           |   8 ++
 src/Dialect/Mlir/DialectBuilder.hpp           |   3 +
 .../Tensor/DepthToSpace.mlir                  |  49 ++++++++
 12 files changed, 313 insertions(+), 29 deletions(-)
 create mode 100644 src/Conversion/ONNXToStableHlo/Tensor/DepthToSpace.cpp
 create mode 100644 test/mlir/conversion/onnx_to_stablehlo/Tensor/DepthToSpace.mlir

diff --git a/src/Conversion/ONNXToStableHlo/CMakeLists.txt b/src/Conversion/ONNXToStableHlo/CMakeLists.txt
index e1cd1b2157..d1140c84e2 100644
--- a/src/Conversion/ONNXToStableHlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToStableHlo/CMakeLists.txt
@@ -55,6 +55,7 @@ add_onnx_mlir_library(OMONNXToStableHlo
   Tensor/ArgMax.cpp
   Tensor/Concat.cpp
   Tensor/Constant.cpp
+  Tensor/DepthToSpace.cpp
   Tensor/Expand.cpp
   Tensor/Flatten.cpp
   Tensor/Gather.cpp
diff --git a/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp b/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp
index 03a97790d5..08f4fe1c7b 100644
--- a/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp
+++ b/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp
@@ -41,6 +41,7 @@ void populateONNXToStableHloConversionPattern(
   populateLoweringONNXArgMaxOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXConcatOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXConstantOpToStableHloPattern(patterns, ctx);
+  populateLoweringONNXDepthToSpaceOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXExpandOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXFlattenOpToStableHloPattern(patterns, ctx);
   populateLoweringONNXGatherOpToStableHloPattern(patterns, ctx);
diff --git a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp b/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
index 8ea4148c53..3564ee4114 100644
--- a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
+++ b/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
@@ -13,19 +13,22 @@
 //
 //===----------------------------------------------------------------------===//

-#include "mlir/Dialect/Arith/IR/Arith.h"
-#include "stablehlo/dialect/StablehloOps.h"
-#include "llvm/ADT/TypeSwitch.h"
-
 #include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
+#include "mlir/Dialect/Arith/IR/Arith.h"
 #include "src/Dialect/ONNX/ONNXOps.hpp"
 #include "src/Support/TypeUtilities.hpp"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "llvm/ADT/TypeSwitch.h"

 using namespace mlir;

 namespace onnx_mlir {

-Value StablehloBuilder::constant(mlir::Type type, double val) const {
+// =============================================================================
+// stablehlo Builder
+// =============================================================================
+
+Value StablehloBuilder::constant(Type type, double val) const {
   Value constant = nullptr;
   // Could be a vector type; look at the element type.
   Type elementType = type;
@@ -79,7 +82,7 @@ Value StablehloBuilder::constantI64(int64_t val) const {
   return b().create<stablehlo::ConstantOp>(loc(), constantAttr);
 }

-Value StablehloBuilder::shaped_zero(mlir::Type type) const {
+Value StablehloBuilder::shaped_zero(Type type) const {
   return b().create<stablehlo::ConstantOp>(loc(), b().getZeroAttr(type));
 }

@@ -87,39 +90,115 @@ Value StablehloBuilder::reshape(Type resultType, Value operand) const {
   return b().create<stablehlo::ReshapeOp>(loc(), resultType, operand);
 }

-mlir::Value StablehloBuilder::real_dynamic_slice(mlir::Type type,
-    mlir::Value operand, mlir::Value startIndices, mlir::Value limitIndices,
-    mlir::Value strides) const {
+Value StablehloBuilder::dynamic_reshape(
+    Type type, Value input, Value shape) const {
+  return b().create<stablehlo::DynamicReshapeOp>(loc(), type, input, shape);
+}
+
+Value StablehloBuilder::real_dynamic_slice(Type type, Value operand,
+    Value startIndices, Value limitIndices, Value strides) const {
   return b().create<stablehlo::RealDynamicSliceOp>(
       loc(), type, operand, startIndices, limitIndices, strides);
 }

-mlir::Value StablehloBuilder::dynamic_slice(mlir::Value operand,
+Value StablehloBuilder::dynamic_slice(Value operand,
     SmallVector<Value> startIndices, SmallVector<int64_t> sliceSizes) const {
   return b().create<stablehlo::DynamicSliceOp>(
       loc(), operand, startIndices, sliceSizes);
 }

-mlir::Value StablehloBuilder::dynamic_slice(mlir::Value operand,
+Value StablehloBuilder::dynamic_slice(Value operand,
     SmallVector<Value> startIndices, DenseI64ArrayAttr sliceSizes) const {
   return b().create<stablehlo::DynamicSliceOp>(
       loc(), operand, startIndices, sliceSizes);
 }

-mlir::Value StablehloBuilder::slice(mlir::Value operand,
-    SmallVector<int64_t> startIndices, SmallVector<int64_t> limitIndices,
-    SmallVector<int64_t> strides) const {
+Value StablehloBuilder::slice(Value operand, SmallVector<int64_t> startIndices,
+    SmallVector<int64_t> limitIndices, SmallVector<int64_t> strides) const {
   return b().create<stablehlo::SliceOp>(
       loc(), operand, startIndices, limitIndices, strides);
 }

-mlir::Value StablehloBuilder::slice(mlir::Value operand,
-    DenseI64ArrayAttr startIndices, DenseI64ArrayAttr limitIndices,
-    DenseI64ArrayAttr strides) const {
+Value StablehloBuilder::slice(Value operand, DenseI64ArrayAttr startIndices,
+    DenseI64ArrayAttr limitIndices, DenseI64ArrayAttr strides) const {
   return b().create<stablehlo::SliceOp>(
       loc(), operand, startIndices, limitIndices, strides);
 }

+//===----------------------------------------------------------------------===//
+// Extends OnnxBuilder with member functions that might generate stablehlo
+// related dialect operations.
+//===----------------------------------------------------------------------===//
+
+Value OnnxToStablehloBuilder::reshape(
+    const Value input, const ArrayRef<DimIndexExpr> shapeDims) const {
+  assert(!shapeDims.empty() && "Shape dimensions should not be empty");
+
+  ShapedType inputType = input.getType().cast<ShapedType>();
+  Type elementType = inputType.getElementType();
+  MultiDialectBuilder<StablehloBuilder, OnnxBuilder, ShapeBuilder> create(
+      b(), loc());
+
+  // If the output dimensions are all literals the 'onnx/Reshape' operation
+  // can take the new shape via an 'onnx.Constant'.
+  if (llvm::all_of(
+          shapeDims, [](const DimIndexExpr &dim) { return dim.isLiteral(); })) {
+    SmallVector<int64_t, 6> shape;
+    for (const IndexExpr &dim : shapeDims)
+      shape.push_back(dim.getLiteral());
+
+    auto constantOp = create.onnx.constantInt64(shape);
+
+    Value reshapeRes = create.onnx.reshape(
+        RankedTensorType::get(shape, elementType), input, constantOp);
+
+    return reshapeRes;
+  }
+
+  // When the output dimensions aren't all literals we need to generate code
+  // to compute the shape.
+  int64_t length = shapeDims.size();
+  SmallVector<Value> dims;
+  for (int64_t i = 0; i < length; ++i) {
+    Value data = shapeDims[i].getValue();
+    dims.push_back(data);
+  }
+
+  Value shapeExtent = create.shape.fromExtents(dims);
+  Value shapeTensor = create.shape.toExtentTensor(
+      RankedTensorType::get({length}, b().getIndexType()), shapeExtent);
+  // result shape
+  SmallVector<int64_t, 6> outputShape;
+  for (const IndexExpr &dim : shapeDims)
+    outputShape.push_back(
+        dim.isLiteral() ? dim.getLiteral() : ShapedType::kDynamic);
+  Value res = create.stablehlo.dynamic_reshape(
+      RankedTensorType::get(outputShape, elementType), input, shapeTensor);
+  return res;
+}
+
+Value OnnxToStablehloBuilder::transpose(const Value input,
+    const ArrayRef<int64_t> perm,
+    const ArrayRef<DimIndexExpr> outputDims) const {
+  assert(!outputDims.empty() && "Output dimensions should not be empty");
+  assert(!perm.empty() && perm.size() == outputDims.size() &&
+         "Expecting valid permutation array");
+  MultiDialectBuilder<OnnxBuilder> create(b(), loc());
+
+  // Compute the shape of the 'onnx.Transpose' result.
+  SmallVector<int64_t, 6> shape;
+  for (const IndexExpr &dim : outputDims)
+    shape.push_back(dim.isLiteral() ? dim.getLiteral() : ShapedType::kDynamic);
+
+  // Create the "onnx.Transpose" operation.
+  ShapedType inputType = input.getType().cast<ShapedType>();
+  Value transposeRes = create.onnx.transpose(
+      RankedTensorType::get(shape, inputType.getElementType()), input,
+      b().getI64ArrayAttr(perm));
+
+  return transposeRes;
+}
+
 // =============================================================================
 // IndexExpr Builder for Lowering using Shape/StableHlo Dialect.
 // =============================================================================
diff --git a/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp b/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
index 2125d884bd..12b8b72f20 100644
--- a/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
+++ b/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
@@ -24,6 +24,7 @@
 #include "src/Dialect/Mlir/DialectBuilder.hpp"
 #include "src/Dialect/Mlir/IndexExpr.hpp"
 #include "src/Dialect/Mlir/IndexExprBuilder.hpp"
+#include "src/Dialect/ONNX/DialectBuilder.hpp"

 namespace onnx_mlir {

@@ -44,6 +45,8 @@ struct StablehloBuilder : DialectBuilder {
   mlir::Value shaped_zero(mlir::Type type) const;
   // ReshapeOp
   mlir::Value reshape(mlir::Type resultType, mlir::Value operand) const;
+  mlir::Value dynamic_reshape(
+      mlir::Type type, mlir::Value input, mlir::Value shape) const;
   // SliceOp
   mlir::Value real_dynamic_slice(mlir::Type type, mlir::Value operand,
       mlir::Value startIndices, mlir::Value limitIndices,
@@ -73,6 +76,30 @@ struct StablehloBuilder : DialectBuilder {
   mlir::OpBuilder *patternRewriter;
 };

+//===----------------------------------------------------------------------===//
+// Extends OnnxBuilder with member functions that might generate Stablehlo
+// related dialect operations.
+//===----------------------------------------------------------------------===//
+
+struct OnnxToStablehloBuilder : public OnnxBuilder {
+  OnnxToStablehloBuilder(mlir::Location loc) : OnnxBuilder(loc) {}
+  OnnxToStablehloBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : OnnxBuilder(b, loc) {}
+  OnnxToStablehloBuilder(const DialectBuilder &db) : OnnxBuilder(db) {}
+  virtual ~OnnxToStablehloBuilder() {}
+
+  // Generate an 'onnx.reshape' operation on the 'input' tensor, the new shape
+  // is provided by 'shapeDims'.
+  mlir::Value reshape(const mlir::Value input,
+      const llvm::ArrayRef<DimIndexExpr> shapeDims) const;
+
+  // Generate a 'onnx.Transpose' operation on the 'input' tensor given the
+  // permutation array 'perm' and the operator output dimensions 'outputDims'.
+  mlir::Value transpose(const mlir::Value input,
+      const llvm::ArrayRef<int64_t> perm,
+      const llvm::ArrayRef<DimIndexExpr> outputDims) const;
+};
+
 // =============================================================================
 // IndexExpr Builder for Shape lowering
 // =============================================================================
@@ -107,7 +134,20 @@ struct MultiDialectBuilder<StablehloBuilder, Ts...>
   StablehloBuilder stablehlo;
 };

-// Recursive class specialized for AffineBuilder refereed to as affine.
+// Recursive class specialized for OnnxToStablehloBuilder referred to as
+// stablehloOnnx.
+template <class... Ts>
+struct MultiDialectBuilder<OnnxToStablehloBuilder, Ts...>
+    : MultiDialectBuilder<Ts...> {
+  MultiDialectBuilder(mlir::OpBuilder &b, mlir::Location loc)
+      : MultiDialectBuilder<Ts...>(b, loc), stablehloOnnx(b, loc) {}
+  MultiDialectBuilder(const DialectBuilder &db)
+      : MultiDialectBuilder<Ts...>(db), stablehloOnnx(db) {}
+  OnnxToStablehloBuilder stablehloOnnx;
+};
+
+// Recursive class specialized for IndexExprBuilderForStableHlo referred to as
+// stableHloIE.
 template <class... Ts>
 struct MultiDialectBuilder<IndexExprBuilderForStableHlo, Ts...>
     : MultiDialectBuilder<Ts...> {
diff --git a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp b/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp
index 411c598ed6..8291a638f5 100644
--- a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp
+++ b/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp
@@ -24,12 +24,6 @@
 #include "mlir/IR/PatternMatch.h"
 #include "mlir/Pass/Pass.h"
 #include "mlir/Transforms/DialectConversion.h"
-#include "llvm/ADT/ArrayRef.h"
-#include "llvm/ADT/Sequence.h"
-#include "llvm/ADT/TypeSwitch.h"
-
-#include "stablehlo/dialect/StablehloOps.h"
-
 #include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
 #include "src/Dialect/Mlir/DialectBuilder.hpp"
 #include "src/Dialect/Mlir/IndexExpr.hpp"
@@ -37,6 +31,10 @@
 #include "src/Dialect/ONNX/ONNXOps.hpp"
 #include "src/Dialect/ONNX/ONNXOps/OpHelper.hpp"
 #include "src/Pass/Passes.hpp"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/Sequence.h"
+#include "llvm/ADT/TypeSwitch.h"

 using namespace mlir;

@@ -182,7 +180,7 @@ void populateLoweringONNXConcatOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXConstantOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXReshapeOpToStableHloPattern(
+void populateLoweringONNXDepthToSpaceOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXExpandOpToStableHloPattern(
     RewritePatternSet &, MLIRContext *);
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/DepthToSpace.cpp b/src/Conversion/ONNXToStableHlo/Tensor/DepthToSpace.cpp
new file mode 100644
index 0000000000..f1819ec37f
--- /dev/null
+++ b/src/Conversion/ONNXToStableHlo/Tensor/DepthToSpace.cpp
@@ -0,0 +1,99 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===------------ DepthToSpace.cpp - Lowering DepthToSpace Op -------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX DepthToSpace Operator to Stablehlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXDepthToSpaceOpLoweringToStablehlo
+    : public OpConversionPattern<ONNXDepthToSpaceOp> {
+  ONNXDepthToSpaceOpLoweringToStablehlo(MLIRContext *ctx)
+      : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXDepthToSpaceOp depthToSpaceOp,
+      ONNXDepthToSpaceOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = depthToSpaceOp.getOperation();
+    Location loc = ONNXLoc<ONNXDepthToSpaceOp>(op);
+    ValueRange operands = adaptor.getOperands();
+    Value input = adaptor.getInput();
+
+    MultiDialectBuilder<IndexExprBuilderForStableHlo, OnnxToStablehloBuilder>
+        create(rewriter, loc);
+    ONNXDepthToSpaceOpShapeHelper shapeHelper(
+        op, operands, &create.stableHloIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    int64_t bs = depthToSpaceOp.getBlocksize();
+    StringRef mode = depthToSpaceOp.getMode();
+    assert(create.stableHloIE.getShapedTypeRank(input) == 4 &&
+           "Input tensor should have rank equal to 4");
+
+    // Compute the new dimensions.
+
+    DimIndexExpr B(create.stableHloIE.getShapeAsDim(input, 0));
+    DimIndexExpr C(create.stableHloIE.getShapeAsDim(input, 1));
+    DimIndexExpr H(create.stableHloIE.getShapeAsDim(input, 2));
+    DimIndexExpr W(create.stableHloIE.getShapeAsDim(input, 3));
+    DimIndexExpr newC = C.floorDiv(bs * bs);
+    DimIndexExpr newH = H * bs;
+    DimIndexExpr newW = W * bs;
+
+    // Compute the output dimension of the first reshape operation, and the
+    // permutation array for the transpose operation.
+    LiteralIndexExpr bsLit(bs);
+    SmallVector<DimIndexExpr, 6> outputDims1;
+    SmallVector<int64_t, 6> perm;
+    if (mode == "DCR") {
+      outputDims1 = {B, bsLit, bsLit, newC, H, W};
+      perm = {0, 3, 4, 1, 5, 2};
+    } else {
+      assert(mode == "CRD" && "Unexpected mode");
+      outputDims1 = {B, newC, bsLit, bsLit, H, W};
+      perm = {0, 1, 4, 2, 5, 3};
+    }
+
+    // Reshape input tensor to shape:
+    //   [B, bs, bs, C/(bs*bs), H, W] when mode=DCR
+    //   [B, C/(bs*bs), bs, bs, H, W] when mode=CRD
+    Value reshapeRes1 = create.stablehloOnnx.reshape(input, outputDims1);
+
+    // Transpose the reshape result into shape [B, C/(bs*bs), H, bs, W, bs].
+    SmallVector<DimIndexExpr> outputDims2({B, newC, H, bsLit, W, bsLit});
+    Value transposeRes =
+        create.stablehloOnnx.transpose(reshapeRes1, perm, outputDims2);
+
+    // Reshape the transpose result into shape [B, C/(bs*bs), H*bs, W*bs].
+    SmallVector<DimIndexExpr> outputDims3({B, newC, newH, newW});
+    Value reshapeRes2 = create.stablehloOnnx.reshape(transposeRes, outputDims3);
+
+    rewriter.replaceOp(op, reshapeRes2);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXDepthToSpaceOpToStableHloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXDepthToSpaceOpLoweringToStablehlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Expand.cpp b/src/Conversion/ONNXToStableHlo/Tensor/Expand.cpp
index d56e5af51e..d0cdab8419 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Expand.cpp
+++ b/src/Conversion/ONNXToStableHlo/Tensor/Expand.cpp
@@ -69,11 +69,9 @@ struct ONNXExpandOpLoweringToStableHlo : public ConversionPattern {
       RankedTensorType onesType = RankedTensorType::get(onesShape, elementType);
       broadcastedOnes = rewriter.create<stablehlo::DynamicBroadcastInDimOp>(
           loc, onesType, ones, shape, rewriter.getI64TensorAttr({}));
-    } else if (ONNXConstantOp shapeOp =
-                   dyn_cast_or_null<ONNXConstantOp>(shapeDefOp)) {
+    } else if (mlir::ElementsAttr constShape =
+                   getElementAttributeFromConstValue(shape)) {
       llvm::SmallVector<int64_t, 4> shapeValues;
-      mlir::ElementsAttr constShape =
-          shapeOp.getValueAttr().cast<ElementsAttr>();
       for (mlir::IntegerAttr element : constShape.getValues<IntegerAttr>())
         shapeValues.push_back(element.getInt());
       RankedTensorType broadcastedType =
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp b/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
index d33ed591c6..411f3db4b5 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
+++ b/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
@@ -22,6 +22,8 @@ using namespace mlir;

 namespace onnx_mlir {

+namespace {
+
 struct ONNXOneHotOpLoweringToStableHlo
     : public OpConversionPattern<ONNXOneHotOp> {
   ONNXOneHotOpLoweringToStableHlo(MLIRContext *ctx)
@@ -115,6 +117,8 @@ struct ONNXOneHotOpLoweringToStableHlo
   }
 };

+} // namespace
+
 void populateLoweringONNXOneHotOpToStableHloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
   patterns.insert<ONNXOneHotOpLoweringToStableHlo>(ctx);
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp b/src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp
index aaa1d6bd3d..1dd2569264 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp
+++ b/src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp
@@ -19,6 +19,8 @@ using namespace mlir;

 namespace onnx_mlir {

+namespace {
+
 struct ONNXScatterNDOpLoweringToStableHlo
     : public OpConversionPattern<ONNXScatterNDOp> {
   ONNXScatterNDOpLoweringToStableHlo(MLIRContext *ctx)
@@ -84,6 +86,8 @@ struct ONNXScatterNDOpLoweringToStableHlo
   }
 };

+} // namespace
+
 void populateLoweringONNXScatterNDOpToStableHloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
   patterns.insert<ONNXScatterNDOpLoweringToStableHlo>(ctx);
diff --git a/src/Dialect/Mlir/DialectBuilder.cpp b/src/Dialect/Mlir/DialectBuilder.cpp
index fc93f8f492..593ab7a4a3 100644
--- a/src/Dialect/Mlir/DialectBuilder.cpp
+++ b/src/Dialect/Mlir/DialectBuilder.cpp
@@ -850,6 +850,14 @@ Value ShapeBuilder::shapeOf(Value val) const {
   return b().create<shape::ShapeOfOp>(loc(), val);
 }

+Value ShapeBuilder::fromExtents(ValueRange extents) const {
+  return b().create<shape::FromExtentsOp>(loc(), extents);
+}
+
+Value ShapeBuilder::toExtentTensor(Type type, Value shape) const {
+  return b().create<shape::ToExtentTensorOp>(loc(), type, shape);
+}
+
 Value ShapeBuilder::getExtent(Value val, int64_t index) const {
   return b().create<shape::GetExtentOp>(loc(), val, index);
 }
diff --git a/src/Dialect/Mlir/DialectBuilder.hpp b/src/Dialect/Mlir/DialectBuilder.hpp
index 7f40e0564b..f209de0b06 100644
--- a/src/Dialect/Mlir/DialectBuilder.hpp
+++ b/src/Dialect/Mlir/DialectBuilder.hpp
@@ -205,7 +205,10 @@ struct ShapeBuilder final : DialectBuilder {

   mlir::Value dim(mlir::Value val, int64_t index) const;
   mlir::Value shapeOf(mlir::Value val) const;
+
+  mlir::Value fromExtents(mlir::ValueRange extents) const;
   mlir::Value getExtent(mlir::Value val, int64_t index) const;
+  mlir::Value toExtentTensor(mlir::Type type, mlir::Value shape) const;
 };

 //===----------------------------------------------------------------------===//
diff --git a/test/mlir/conversion/onnx_to_stablehlo/Tensor/DepthToSpace.mlir b/test/mlir/conversion/onnx_to_stablehlo/Tensor/DepthToSpace.mlir
new file mode 100644
index 0000000000..954e6f5cfd
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_stablehlo/Tensor/DepthToSpace.mlir
@@ -0,0 +1,49 @@
+// RUN: onnx-mlir-opt --convert-onnx-to-stablehlo --canonicalize %s -split-input-file | FileCheck %s
+
+func.func @test_depth_to_space(%arg0 : tensor<2x16x20x20xf32>) -> tensor<2x4x40x40xf32> {
+  %0 = "onnx.DepthToSpace"(%arg0) {blocksize = 2 : si64, mode = "CRD"} : (tensor<2x16x20x20xf32>) -> tensor<2x4x40x40xf32>
+  "func.return"(%0) : (tensor<2x4x40x40xf32>) -> ()
+// CHECK-LABEL:  func.func @test_depth_to_space
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<2x16x20x20xf32>) -> tensor<2x4x40x40xf32> {
+// CHECK-DAG:       [[VAR_0_:%.+]] = shape.const_shape [2, 4, 40, 40] : tensor<4xindex>
+// CHECK-DAG:       [[VAR_1_:%.+]] = shape.const_shape [2, 4, 2, 2, 20, 20] : tensor<6xindex>
+// CHECK:           [[VAR_2_:%.+]] = stablehlo.dynamic_reshape [[PARAM_0_]], [[VAR_1_]] : (tensor<2x16x20x20xf32>, tensor<6xindex>) -> tensor<2x4x2x2x20x20xf32>
+// CHECK:           [[VAR_3_:%.+]] = stablehlo.transpose [[VAR_2_]], dims = [0, 1, 4, 2, 5, 3] : (tensor<2x4x2x2x20x20xf32>) -> tensor<2x4x20x2x20x2xf32>
+// CHECK:           [[VAR_4_:%.+]] = stablehlo.dynamic_reshape [[VAR_3_]], [[VAR_0_]] : (tensor<2x4x20x2x20x2xf32>, tensor<4xindex>) -> tensor<2x4x40x40xf32>
+// CHECK:           return [[VAR_4_]] : tensor<2x4x40x40xf32>
+// CHECK:         }
+}
+
+// -----
+
+func.func @test_depth_to_space_dynamic(%arg0 : tensor<2x?x20x?xf32>) -> tensor<2x?x40x?xf32> {
+  %0 = "onnx.DepthToSpace"(%arg0) {blocksize = 2 : si64, mode = "CRD"} : (tensor<2x?x20x?xf32>) -> tensor<2x?x40x?xf32>
+  "func.return"(%0) : (tensor<2x?x40x?xf32>) -> ()
+// CHECK-DAG:   [[MAP_0_:#.+]] = affine_map<()[s0] -> (s0 floordiv 4)>
+// CHECK-DAG:   [[MAP_1_:#.+]] = affine_map<()[s0] -> (s0 * 2)>
+// CHECK-LABEL:  func.func @test_depth_to_space_dynamic
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<2x?x20x?xf32>) -> tensor<2x?x40x?xf32> {
+// CHECK-DAG:       [[CST_40_:%.+]] = arith.constant 40 : index
+// CHECK-DAG:       [[CST_3_:%.+]] = arith.constant 3 : index
+// CHECK-DAG:       [[CST_20_:%.+]] = arith.constant 20 : index
+// CHECK-DAG:       [[CST_1_:%.+]] = arith.constant 1 : index
+// CHECK-DAG:       [[CST_2_:%.+]] = arith.constant 2 : index
+// CHECK-DAG:       [[VAR_0_:%.+]] = shape.shape_of [[PARAM_0_]] : tensor<2x?x20x?xf32> -> tensor<4xindex>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_1_:%.+]] = shape.get_extent [[VAR_0_]], [[CST_1_]] : tensor<4xindex>, index -> index
+// CHECK-DAG:       [[VAR_2_:%.+]] = shape.shape_of [[PARAM_0_]] : tensor<2x?x20x?xf32> -> tensor<4xindex>
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_3_:%.+]] = shape.get_extent [[VAR_2_]], [[CST_3_]] : tensor<4xindex>, index -> index
+// CHECK-DAG:       [[VAR_4_:%.+]] = affine.apply [[MAP_0_]](){{.}}[[VAR_1_]]{{.}}
+// CHECK-NOT: separator of consecutive DAGs
+// CHECK-DAG:       [[VAR_5_:%.+]] = affine.apply [[MAP_1_]](){{.}}[[VAR_3_]]{{.}}
+// CHECK-DAG:       [[VAR_6_:%.+]] = shape.from_extents [[CST_2_]], [[VAR_4_]], [[CST_2_]], [[CST_2_]], [[CST_2_]]0, [[VAR_3_]] : index, index, index, index, index, index
+// CHECK:           [[VAR_7_:%.+]] = shape.to_extent_tensor [[VAR_6_]] : !shape.shape -> tensor<6xindex>
+// CHECK:           [[VAR_8_:%.+]] = stablehlo.dynamic_reshape [[PARAM_0_]], [[VAR_7_]] : (tensor<2x?x20x?xf32>, tensor<6xindex>) -> tensor<2x?x2x2x20x?xf32>
+// CHECK-DAG:       [[VAR_9_:%.+]] = stablehlo.transpose [[VAR_8_]], dims = [0, 1, 4, 2, 5, 3] : (tensor<2x?x2x2x20x?xf32>) -> tensor<2x?x20x2x?x2xf32>
+// CHECK-DAG:       [[VAR_10_:%.+]] = shape.from_extents [[CST_2_]], [[VAR_4_]], [[CST_40_]], [[VAR_5_]] : index, index, index, index
+// CHECK:           [[VAR_11_:%.+]] = shape.to_extent_tensor [[VAR_10_]] : !shape.shape -> tensor<4xindex>
+// CHECK:           [[VAR_12_:%.+]] = stablehlo.dynamic_reshape [[VAR_9_]], [[VAR_11_]] : (tensor<2x?x20x2x?x2xf32>, tensor<4xindex>) -> tensor<2x?x40x?xf32>
+// CHECK:           return [[VAR_12_]] : tensor<2x?x40x?xf32>
+// CHECK:         }
+}

From 724f64b63f87fda3f9478219886a8b149f29ce2b Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Wed, 27 Dec 2023 03:59:50 -0500
Subject: [PATCH 08/11] rename StableHlo to Stablehlo

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 src/Conversion/CMakeLists.txt                 |   2 +-
 .../CMakeLists.txt                            |   6 +-
 .../ConvertONNXToStablehlo.cpp}               |  96 +++++-----
 .../DialectBuilder.cpp                        |  14 +-
 .../DialectBuilder.hpp                        |  20 +--
 .../Math/Clip.cpp                             |  16 +-
 .../Math/Elementwise.cpp                      | 170 +++++++++---------
 .../Math/Gemm.cpp                             |  14 +-
 .../Math/MatMul.cpp                           |  16 +-
 .../Math/Reduction.cpp                        |  20 +--
 .../NN/Conv.cpp                               |  18 +-
 .../NN/ConvTranspose.cpp                      |  16 +-
 .../NN/Normalization.cpp                      |  12 +-
 .../NN/Pooling.cpp                            |  20 +--
 .../ONNXToStablehloCommon.cpp}                |  14 +-
 .../ONNXToStablehloCommon.hpp}                |  80 ++++-----
 .../RNN/LSTM.cpp                              |  64 +++----
 .../RNN/RNNBase.cpp                           |   4 +-
 .../RNN/RNNBase.hpp                           |   2 +-
 .../Tensor/ArgMax.cpp                         |  18 +-
 .../Tensor/Concat.cpp                         |  12 +-
 .../Tensor/Constant.cpp                       |  12 +-
 .../Tensor/DepthToSpace.cpp                   |   8 +-
 .../Tensor/Expand.cpp                         |  20 +--
 .../Tensor/Flatten.cpp                        |  14 +-
 .../Tensor/Gather.cpp                         |  18 +-
 .../Tensor/GatherElements.cpp                 |  16 +-
 .../Tensor/Identity.cpp                       |  12 +-
 .../Tensor/OneHot.cpp                         |  16 +-
 .../Tensor/Pad.cpp                            |   4 +-
 .../Tensor/Reshape.cpp                        |  16 +-
 .../Tensor/ScatterND.cpp                      |  12 +-
 .../Tensor/Shape.cpp                          |  16 +-
 .../Tensor/Slice.cpp                          |  14 +-
 .../Tensor/Split.cpp                          |  18 +-
 .../Tensor/Squeeze.cpp                        |  18 +-
 .../Tensor/Tile.cpp                           |  16 +-
 .../Tensor/Transpose.cpp                      |  14 +-
 .../Tensor/Unsqueeze.cpp                      |  18 +-
 src/Conversion/ONNXToTOSA/DialectBuilder.cpp  |   2 +-
 src/Conversion/ONNXToTOSA/DialectBuilder.hpp  |   2 +-
 src/Dialect/Mlir/IndexExprBuilder.hpp         |   8 +-
 src/Interface/ShapeHelperOpInterface.hpp      |   4 +-
 src/Pass/Passes.hpp                           |   6 +-
 src/Tools/onnx-mlir-opt/RegisterPasses.cpp    |   2 +-
 src/Transform/ONNX/Decompose.cpp              |   6 +-
 46 files changed, 463 insertions(+), 463 deletions(-)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/CMakeLists.txt (94%)
 rename src/Conversion/{ONNXToStableHlo/ConvertONNXToStableHlo.cpp => ONNXToStablehlo/ConvertONNXToStablehlo.cpp} (53%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/DialectBuilder.cpp (95%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/DialectBuilder.hpp (90%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Math/Clip.cpp (82%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Math/Elementwise.cpp (73%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Math/Gemm.cpp (93%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Math/MatMul.cpp (90%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Math/Reduction.cpp (95%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/NN/Conv.cpp (91%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/NN/ConvTranspose.cpp (94%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/NN/Normalization.cpp (83%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/NN/Pooling.cpp (93%)
 rename src/Conversion/{ONNXToStableHlo/ONNXToStableHloCommon.cpp => ONNXToStablehlo/ONNXToStablehloCommon.cpp} (95%)
 rename src/Conversion/{ONNXToStableHlo/ONNXToStableHloCommon.hpp => ONNXToStablehlo/ONNXToStablehloCommon.hpp} (77%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/RNN/LSTM.cpp (94%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/RNN/RNNBase.cpp (98%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/RNN/RNNBase.hpp (99%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/ArgMax.cpp (91%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Concat.cpp (82%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Constant.cpp (78%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/DepthToSpace.cpp (92%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Expand.cpp (86%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Flatten.cpp (85%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Gather.cpp (85%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/GatherElements.cpp (92%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Identity.cpp (70%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/OneHot.cpp (91%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Pad.cpp (96%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Reshape.cpp (77%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/ScatterND.cpp (90%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Shape.cpp (78%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Slice.cpp (95%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Split.cpp (86%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Squeeze.cpp (83%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Tile.cpp (91%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Transpose.cpp (84%)
 rename src/Conversion/{ONNXToStableHlo => ONNXToStablehlo}/Tensor/Unsqueeze.cpp (84%)

diff --git a/src/Conversion/CMakeLists.txt b/src/Conversion/CMakeLists.txt
index 221583ecc3..19c7b4f046 100644
--- a/src/Conversion/CMakeLists.txt
+++ b/src/Conversion/CMakeLists.txt
@@ -7,5 +7,5 @@ add_subdirectory(KrnlSeqToMemref)
 add_subdirectory(ONNXToTOSA)

 if (ONNX_MLIR_ENABLE_STABLEHLO)
-  add_subdirectory(ONNXToStableHlo)
+  add_subdirectory(ONNXToStablehlo)
 endif()
diff --git a/src/Conversion/ONNXToStableHlo/CMakeLists.txt b/src/Conversion/ONNXToStablehlo/CMakeLists.txt
similarity index 94%
rename from src/Conversion/ONNXToStableHlo/CMakeLists.txt
rename to src/Conversion/ONNXToStablehlo/CMakeLists.txt
index d1140c84e2..8255395b36 100644
--- a/src/Conversion/ONNXToStableHlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToStablehlo/CMakeLists.txt
@@ -36,9 +36,9 @@ install(TARGETS
   DESTINATION lib
   )

-add_onnx_mlir_library(OMONNXToStableHlo
-  ConvertONNXToStableHlo.cpp
-  ONNXToStableHloCommon.cpp
+add_onnx_mlir_library(OMONNXToStablehlo
+  ConvertONNXToStablehlo.cpp
+  ONNXToStablehloCommon.cpp
   DialectBuilder.cpp

   Math/Clip.cpp
diff --git a/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp b/src/Conversion/ONNXToStablehlo/ConvertONNXToStablehlo.cpp
similarity index 53%
rename from src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp
rename to src/Conversion/ONNXToStablehlo/ConvertONNXToStablehlo.cpp
index 08f4fe1c7b..25b34ffeba 100644
--- a/src/Conversion/ONNXToStableHlo/ConvertONNXToStableHlo.cpp
+++ b/src/Conversion/ONNXToStablehlo/ConvertONNXToStablehlo.cpp
@@ -2,7 +2,7 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====------ ConvertONNXToStableHlo.cpp - ONNX dialects to StableHlo lowering
+//====------ ConvertONNXToStablehlo.cpp - ONNX dialects to Stablehlo lowering
 //-------===//
 //
 // Copyright 2022
@@ -10,77 +10,77 @@
 // =============================================================================
 //
 // This file implements the lowering of frontend operations to a combination of
-// StableHlo IR and standard operations.
+// Stablehlo IR and standard operations.
 //
 //===----------------------------------------------------------------------===//

 #include "mlir/Dialect/Affine/IR/AffineOps.h"

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"

 using namespace mlir;

 namespace onnx_mlir {

-void populateONNXToStableHloConversionPattern(
+void populateONNXToStablehloConversionPattern(
     RewritePatternSet &patterns, MLIRContext *ctx, bool enableUnroll) {
   // Math
-  populateLoweringONNXClipOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXElementwiseOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXGemmOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXMatMulOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXReductionOpToStableHloPattern(patterns, ctx);
+  populateLoweringONNXClipOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXElementwiseOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXGemmOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXMatMulOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXReductionOpToStablehloPattern(patterns, ctx);
   // Neural network
-  populateLoweringONNXConvOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXConvTransposeOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXNormalizationOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXPoolingOpToStableHloPattern(patterns, ctx);
+  populateLoweringONNXConvOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXConvTransposeOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXNormalizationOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXPoolingOpToStablehloPattern(patterns, ctx);
   // Recurrent neural network
-  populateLoweringONNXLSTMOpToStableHloPattern(patterns, ctx, enableUnroll);
+  populateLoweringONNXLSTMOpToStablehloPattern(patterns, ctx, enableUnroll);
   // Tensor
-  populateLoweringONNXArgMaxOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXConcatOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXConstantOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXDepthToSpaceOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXExpandOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXFlattenOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXGatherOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXGatherElementsOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXIdentityOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXOneHotOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXPadOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXReshapeOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXScatterNDOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXShapeOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXSliceOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXSplitOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXSqueezeOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXTileOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXTransposeOpToStableHloPattern(patterns, ctx);
-  populateLoweringONNXUnsqueezeOpToStableHloPattern(patterns, ctx);
+  populateLoweringONNXArgMaxOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXConcatOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXConstantOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXDepthToSpaceOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXExpandOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXFlattenOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXGatherOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXGatherElementsOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXIdentityOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXOneHotOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXPadOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXReshapeOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXScatterNDOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXShapeOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXSliceOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXSplitOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXSqueezeOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXTileOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXTransposeOpToStablehloPattern(patterns, ctx);
+  populateLoweringONNXUnsqueezeOpToStablehloPattern(patterns, ctx);
 }

 //===----------------------------------------------------------------------===//
-// Frontend to StableHlo Dialect lowering pass
+// Frontend to Stablehlo Dialect lowering pass
 //===----------------------------------------------------------------------===//

-struct FrontendToStableHloLoweringPass
-    : public PassWrapper<FrontendToStableHloLoweringPass,
+struct FrontendToStablehloLoweringPass
+    : public PassWrapper<FrontendToStablehloLoweringPass,
           OperationPass<ModuleOp>> {

   StringRef getArgument() const override { return "convert-onnx-to-stablehlo"; }

   StringRef getDescription() const override {
-    return "Lower frontend ops to StableHlo dialect.";
+    return "Lower frontend ops to Stablehlo dialect.";
   }

   // Make sure that we have a valid default constructor and copy
   // constructor to make sure that the options are initialized properly.
-  FrontendToStableHloLoweringPass() = default;
-  FrontendToStableHloLoweringPass(const FrontendToStableHloLoweringPass &pass)
-      : PassWrapper<FrontendToStableHloLoweringPass,
+  FrontendToStablehloLoweringPass() = default;
+  FrontendToStablehloLoweringPass(const FrontendToStablehloLoweringPass &pass)
+      : PassWrapper<FrontendToStablehloLoweringPass,
             OperationPass<ModuleOp>>() {}
-  FrontendToStableHloLoweringPass(bool enableUnroll) {
+  FrontendToStablehloLoweringPass(bool enableUnroll) {
     // Below, need explicit assignment to enable implicit conversion of bool
     // to Option<bool>.
     this->enableUnroll = enableUnroll;
@@ -103,7 +103,7 @@ struct FrontendToStableHloLoweringPass
       llvm::cl::init(true)};
 };

-void FrontendToStableHloLoweringPass::runOnOperation() {
+void FrontendToStablehloLoweringPass::runOnOperation() {
   ModuleOp module = getOperation();
   // The first thing to define is the conversion target. This will define the
   // final target for this lowering.
@@ -122,7 +122,7 @@ void FrontendToStableHloLoweringPass::runOnOperation() {
   RewritePatternSet patterns(&getContext());

   // Define patterns.
-  populateONNXToStableHloConversionPattern(
+  populateONNXToStablehloConversionPattern(
       patterns, &getContext(), enableUnroll);

   // add illegal op
@@ -136,12 +136,12 @@ void FrontendToStableHloLoweringPass::runOnOperation() {
   }
 }

-std::unique_ptr<Pass> createLowerToStableHloPass() {
-  return std::make_unique<FrontendToStableHloLoweringPass>();
+std::unique_ptr<Pass> createLowerToStablehloPass() {
+  return std::make_unique<FrontendToStablehloLoweringPass>();
 }

-std::unique_ptr<Pass> createLowerToStableHloPass(bool enableUnroll) {
-  return std::make_unique<FrontendToStableHloLoweringPass>(enableUnroll);
+std::unique_ptr<Pass> createLowerToStablehloPass(bool enableUnroll) {
+  return std::make_unique<FrontendToStablehloLoweringPass>(enableUnroll);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp b/src/Conversion/ONNXToStablehlo/DialectBuilder.cpp
similarity index 95%
rename from src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
rename to src/Conversion/ONNXToStablehlo/DialectBuilder.cpp
index 3564ee4114..e85c2c1643 100644
--- a/src/Conversion/ONNXToStableHlo/DialectBuilder.cpp
+++ b/src/Conversion/ONNXToStablehlo/DialectBuilder.cpp
@@ -2,18 +2,18 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====------ DialectBuilder.cpp - StableHlo dialect builder
+//====------ DialectBuilder.cpp - Stablehlo dialect builder
 //--------------------===//
 //
 // Copyright 2022
 //
 // =============================================================================
 //
-// This file contains dialect builder for StableHlo dialect.
+// This file contains dialect builder for Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
 #include "mlir/Dialect/Arith/IR/Arith.h"
 #include "src/Dialect/ONNX/ONNXOps.hpp"
 #include "src/Support/TypeUtilities.hpp"
@@ -200,11 +200,11 @@ Value OnnxToStablehloBuilder::transpose(const Value input,
 }

 // =============================================================================
-// IndexExpr Builder for Lowering using Shape/StableHlo Dialect.
+// IndexExpr Builder for Lowering using Shape/Stablehlo Dialect.
 // =============================================================================

 // Return null if none is found.
-ElementsAttr IndexExprBuilderForStableHlo::getConst(Value value) {
+ElementsAttr IndexExprBuilderForStablehlo::getConst(Value value) {
   auto definingOp = value.getDefiningOp();
   // If we have a cast between index/integer, skip it, i.e. get the defining op
   // that is the input to the cast.
@@ -222,7 +222,7 @@ ElementsAttr IndexExprBuilderForStableHlo::getConst(Value value) {
   return nullptr;
 }

-Value IndexExprBuilderForStableHlo::getVal(Value intArrayVal, uint64_t i) {
+Value IndexExprBuilderForStablehlo::getVal(Value intArrayVal, uint64_t i) {
   Type elemType = getElementType(intArrayVal.getType());
   if (!elemType.isa<IndexType>()) {
     Type indexTensorType = RankedTensorType::get(
@@ -235,7 +235,7 @@ Value IndexExprBuilderForStableHlo::getVal(Value intArrayVal, uint64_t i) {
   return createShape.getExtent(intArrayVal, i);
 }

-Value IndexExprBuilderForStableHlo::getShapeVal(
+Value IndexExprBuilderForStablehlo::getShapeVal(
     Value tensorOrMemrefValue, uint64_t i) {
   ShapeBuilder createShape(*this);
   return createShape.dim(tensorOrMemrefValue, i);
diff --git a/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp b/src/Conversion/ONNXToStablehlo/DialectBuilder.hpp
similarity index 90%
rename from src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
rename to src/Conversion/ONNXToStablehlo/DialectBuilder.hpp
index 12b8b72f20..0c32b6ea63 100644
--- a/src/Conversion/ONNXToStableHlo/DialectBuilder.hpp
+++ b/src/Conversion/ONNXToStablehlo/DialectBuilder.hpp
@@ -2,14 +2,14 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====------ DialectBuilder.hpp - StableHlo dialect builder
+//====------ DialectBuilder.hpp - Stablehlo dialect builder
 //--------------------===//
 //
 // Copyright 2022
 //
 // =============================================================================
 //
-// This file contains dialect builder for StableHlo dialect.
+// This file contains dialect builder for Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

@@ -104,13 +104,13 @@ struct OnnxToStablehloBuilder : public OnnxBuilder {
 // IndexExpr Builder for Shape lowering
 // =============================================================================

-struct IndexExprBuilderForStableHlo : IndexExprBuilder {
-  IndexExprBuilderForStableHlo(mlir::Location loc) : IndexExprBuilder(loc) {}
-  IndexExprBuilderForStableHlo(mlir::OpBuilder &b, mlir::Location loc)
+struct IndexExprBuilderForStablehlo : IndexExprBuilder {
+  IndexExprBuilderForStablehlo(mlir::Location loc) : IndexExprBuilder(loc) {}
+  IndexExprBuilderForStablehlo(mlir::OpBuilder &b, mlir::Location loc)
       : IndexExprBuilder(b, loc) {}
-  IndexExprBuilderForStableHlo(const DialectBuilder &db)
+  IndexExprBuilderForStablehlo(const DialectBuilder &db)
       : IndexExprBuilder(db) {}
-  virtual ~IndexExprBuilderForStableHlo() {}
+  virtual ~IndexExprBuilderForStablehlo() {}

 protected:
   mlir::ElementsAttr getConst(mlir::Value value) final;
@@ -146,16 +146,16 @@ struct MultiDialectBuilder<OnnxToStablehloBuilder, Ts...>
   OnnxToStablehloBuilder stablehloOnnx;
 };

-// Recursive class specialized for IndexExprBuilderForStableHlo referred to as
+// Recursive class specialized for IndexExprBuilderForStablehlo referred to as
 // stableHloIE.
 template <class... Ts>
-struct MultiDialectBuilder<IndexExprBuilderForStableHlo, Ts...>
+struct MultiDialectBuilder<IndexExprBuilderForStablehlo, Ts...>
     : MultiDialectBuilder<Ts...> {
   MultiDialectBuilder(mlir::OpBuilder &b, mlir::Location loc)
       : MultiDialectBuilder<Ts...>(b, loc), stableHloIE(b, loc) {}
   MultiDialectBuilder(const DialectBuilder &db)
       : MultiDialectBuilder<Ts...>(db), stableHloIE(db) {}
-  IndexExprBuilderForStableHlo stableHloIE;
+  IndexExprBuilderForStablehlo stableHloIE;
 };

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Math/Clip.cpp b/src/Conversion/ONNXToStablehlo/Math/Clip.cpp
similarity index 82%
rename from src/Conversion/ONNXToStableHlo/Math/Clip.cpp
rename to src/Conversion/ONNXToStablehlo/Math/Clip.cpp
index c37fc9e103..f36a5bce9c 100644
--- a/src/Conversion/ONNXToStableHlo/Math/Clip.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/Clip.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Clip Operator to StableHlo dialect.
+// This file lowers the ONNX Clip Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Support/TypeUtilities.hpp"

 using namespace mlir;
@@ -24,8 +24,8 @@ namespace onnx_mlir {
 // Scalar unary ops for lowering ONNXClipOp
 //===----------------------------------------------------------------------===//

-struct ONNXClipOpLoweringToStableHlo : public ConversionPattern {
-  ONNXClipOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXClipOpLoweringToStablehlo : public ConversionPattern {
+  ONNXClipOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ONNXClipOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -33,7 +33,7 @@ struct ONNXClipOpLoweringToStableHlo : public ConversionPattern {
     Location loc = op->getLoc();

     ONNXClipOpAdaptor operandAdaptor(operands);
-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXClipOpShapeHelper shapeHelper(op, operands, &createIE);
     auto shapeComputed = shapeHelper.computeShape();
     assert(succeeded(shapeComputed) && "Could not compute output shape");
@@ -66,9 +66,9 @@ struct ONNXClipOpLoweringToStableHlo : public ConversionPattern {
   }
 };

-void populateLoweringONNXClipOpToStableHloPattern(
+void populateLoweringONNXClipOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXClipOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXClipOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp b/src/Conversion/ONNXToStablehlo/Math/Elementwise.cpp
similarity index 73%
rename from src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
rename to src/Conversion/ONNXToStablehlo/Math/Elementwise.cpp
index 1ae8e6d018..71eaac6c9d 100644
--- a/src/Conversion/ONNXToStableHlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/Elementwise.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers ONNX element-wise operators to StableHlo dialect.
+// This file lowers ONNX element-wise operators to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "stablehlo/dialect/BroadcastUtils.h"

@@ -22,97 +22,97 @@ using namespace mlir;
 namespace onnx_mlir {

 template <>
-struct StableHloDialectOp<ONNXAbsOp> {
+struct StablehloDialectOp<ONNXAbsOp> {
   using Op = stablehlo::AbsOp;
 };

 template <>
-struct StableHloDialectOp<ONNXAndOp> {
+struct StablehloDialectOp<ONNXAndOp> {
   using Op = stablehlo::AndOp;
 };

 template <>
-struct StableHloDialectOp<ONNXAddOp> {
+struct StablehloDialectOp<ONNXAddOp> {
   using Op = stablehlo::AddOp;
 };

 template <>
-struct StableHloDialectOp<ONNXCeilOp> {
+struct StablehloDialectOp<ONNXCeilOp> {
   using Op = stablehlo::CeilOp;
 };

 template <>
-struct StableHloDialectOp<ONNXCosOp> {
+struct StablehloDialectOp<ONNXCosOp> {
   using Op = stablehlo::CosineOp;
 };

 template <>
-struct StableHloDialectOp<ONNXDivOp> {
+struct StablehloDialectOp<ONNXDivOp> {
   using Op = stablehlo::DivOp;
 };

 template <>
-struct StableHloDialectOp<ONNXExpOp> {
+struct StablehloDialectOp<ONNXExpOp> {
   using Op = stablehlo::ExpOp;
 };

 template <>
-struct StableHloDialectOp<ONNXLogOp> {
+struct StablehloDialectOp<ONNXLogOp> {
   using Op = stablehlo::LogOp;
 };

 template <>
-struct StableHloDialectOp<ONNXMaxOp> {
+struct StablehloDialectOp<ONNXMaxOp> {
   using Op = stablehlo::MaxOp;
 };

 template <>
-struct StableHloDialectOp<ONNXMinOp> {
+struct StablehloDialectOp<ONNXMinOp> {
   using Op = stablehlo::MinOp;
 };

 template <>
-struct StableHloDialectOp<ONNXMulOp> {
+struct StablehloDialectOp<ONNXMulOp> {
   using Op = stablehlo::MulOp;
 };

 template <>
-struct StableHloDialectOp<ONNXNegOp> {
+struct StablehloDialectOp<ONNXNegOp> {
   using Op = stablehlo::NegOp;
 };

 template <>
-struct StableHloDialectOp<ONNXPowOp> {
+struct StablehloDialectOp<ONNXPowOp> {
   using Op = stablehlo::PowOp;
 };

 template <>
-struct StableHloDialectOp<ONNXSigmoidOp> {
+struct StablehloDialectOp<ONNXSigmoidOp> {
   using Op = stablehlo::LogisticOp;
 };

 template <>
-struct StableHloDialectOp<ONNXSinOp> {
+struct StablehloDialectOp<ONNXSinOp> {
   using Op = stablehlo::SineOp;
 };

 template <>
-struct StableHloDialectOp<ONNXSqrtOp> {
+struct StablehloDialectOp<ONNXSqrtOp> {
   using Op = stablehlo::SqrtOp;
 };

 template <>
-struct StableHloDialectOp<ONNXSubOp> {
+struct StablehloDialectOp<ONNXSubOp> {
   using Op = stablehlo::SubtractOp;
 };

 template <>
-struct StableHloDialectOp<ONNXTanhOp> {
+struct StablehloDialectOp<ONNXTanhOp> {
   using Op = stablehlo::TanhOp;
 };

 template <>
-struct StableHloDialectOp<ONNXWhereOp> {
+struct StablehloDialectOp<ONNXWhereOp> {
   using Op = stablehlo::SelectOp;
 };

@@ -157,16 +157,16 @@ void createCompareOp<ONNXLessOrEqualOp>(Value &op,
       loc, lhs, rhs, stablehlo::ComparisonDirection::LE);
 }

-// Element-wise unary ops lowering to StableHlo dialect.
+// Element-wise unary ops lowering to Stablehlo dialect.
 //===----------------------------------------------------------------------===//
 template <typename ElementwiseUnaryOp>
-struct ONNXElementwiseUnaryOpLoweringToStableHlo : public ConversionPattern {
-  ONNXElementwiseUnaryOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXElementwiseUnaryOpLoweringToStablehlo : public ConversionPattern {
+  ONNXElementwiseUnaryOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ElementwiseUnaryOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
     Location loc = op->getLoc();
-    Value stableHloOp = rewriter.create<StableHloOp<ElementwiseUnaryOp>>(
+    Value stableHloOp = rewriter.create<StablehloOp<ElementwiseUnaryOp>>(
         loc, op->getResultTypes(), op->getOperands());
     rewriter.replaceOp(op, stableHloOp);
     return success();
@@ -175,9 +175,9 @@ struct ONNXElementwiseUnaryOpLoweringToStableHlo : public ConversionPattern {

 // ONNXElu(x) = alpha * (exp(x) - 1.) for x < 0, f(x) = x for x >= 0.
 template <>
-struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXEluOp>
+struct ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXEluOp>
     : public ConversionPattern {
-  ONNXElementwiseUnaryOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXElementwiseUnaryOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ONNXEluOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
@@ -208,9 +208,9 @@ struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXEluOp>

 // ONNXHardSigmoid(x) = max(0, min(1, alpha * x + beta))
 template <>
-struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXHardSigmoidOp>
+struct ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXHardSigmoidOp>
     : public ConversionPattern {
-  ONNXElementwiseUnaryOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXElementwiseUnaryOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ONNXHardSigmoidOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
@@ -236,11 +236,11 @@ struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXHardSigmoidOp>
   }
 };

-// ONNXReluOp(x) is implemented using StableHlo Max(x, 0)
+// ONNXReluOp(x) is implemented using Stablehlo Max(x, 0)
 template <>
-struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXReluOp>
+struct ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXReluOp>
     : public ConversionPattern {
-  ONNXElementwiseUnaryOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXElementwiseUnaryOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ONNXReluOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
@@ -261,9 +261,9 @@ struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXReluOp>

 // ONNXLeakyReluOp(x) = alpha * x if x < 0 else x.
 template <>
-struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXLeakyReluOp>
+struct ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXLeakyReluOp>
     : public ConversionPattern {
-  ONNXElementwiseUnaryOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXElementwiseUnaryOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ONNXLeakyReluOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
@@ -289,9 +289,9 @@ struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXLeakyReluOp>
 };

 template <>
-struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCastOp>
+struct ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXCastOp>
     : public ConversionPattern {
-  ONNXElementwiseUnaryOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXElementwiseUnaryOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ONNXCastOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
@@ -309,12 +309,12 @@ struct ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCastOp>
   }
 };

-// Element-wise compare binary ops lowering to StableHlo dialect.
+// Element-wise compare binary ops lowering to Stablehlo dialect.
 //===----------------------------------------------------------------------===//
 template <typename ElementwiseBinaryOp>
-struct ONNXElementwiseCompareBinaryOpLoweringToStableHlo
+struct ONNXElementwiseCompareBinaryOpLoweringToStablehlo
     : public ConversionPattern {
-  ONNXElementwiseCompareBinaryOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXElementwiseCompareBinaryOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ElementwiseBinaryOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
@@ -323,7 +323,7 @@ struct ONNXElementwiseCompareBinaryOpLoweringToStableHlo
     // Prior code here used the "analysis" version that did not generate code.
     // Since code is actually not needed here at this time, one could use
     // IndexExprBuilderForAnalysis createIE(loc) instead.
-    IndexExprBuilderForStableHlo createShapeIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createShapeIE(rewriter, loc);
     ONNXBroadcastOpShapeHelper shapeHelper(op, operands, &createShapeIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -340,11 +340,11 @@ struct ONNXElementwiseCompareBinaryOpLoweringToStableHlo
   }
 };

-// Element-wise compare binary ops lowering to StableHlo dialect.
+// Element-wise compare binary ops lowering to Stablehlo dialect.
 //===----------------------------------------------------------------------===//
 template <typename ElementwiseBinaryOp>
-struct ONNXElementwiseBinaryOpLoweringToStableHlo : public ConversionPattern {
-  ONNXElementwiseBinaryOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXElementwiseBinaryOpLoweringToStablehlo : public ConversionPattern {
+  ONNXElementwiseBinaryOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ElementwiseBinaryOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
@@ -353,7 +353,7 @@ struct ONNXElementwiseBinaryOpLoweringToStableHlo : public ConversionPattern {
     // Prior code here used the "analysis" version that did not generate code.
     // Since code is actually not needed here at this time, one could use
     // IndexExprBuilderForAnalysis createIE(loc) instead.
-    IndexExprBuilderForStableHlo createShapeIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createShapeIE(rewriter, loc);
     ONNXBroadcastOpShapeHelper shapeHelper(op, operands, &createShapeIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -362,7 +362,7 @@ struct ONNXElementwiseBinaryOpLoweringToStableHlo : public ConversionPattern {
         getBroadcastedOperands(op, rewriter, loc, outputRank);
     Value broadcastedLHS = broadcastedOperands[0];
     Value broadcastedRHS = broadcastedOperands[1];
-    Value stableHloOp = rewriter.create<StableHloOp<ElementwiseBinaryOp>>(
+    Value stableHloOp = rewriter.create<StablehloOp<ElementwiseBinaryOp>>(
         loc, *op->result_type_begin(), broadcastedLHS, broadcastedRHS);
     rewriter.replaceOp(op, stableHloOp);
     return success();
@@ -371,9 +371,9 @@ struct ONNXElementwiseBinaryOpLoweringToStableHlo : public ConversionPattern {

 // ONNXPReluOp(x) = alpha * x if x < 0 else x.
 template <>
-struct ONNXElementwiseBinaryOpLoweringToStableHlo<ONNXPReluOp>
+struct ONNXElementwiseBinaryOpLoweringToStablehlo<ONNXPReluOp>
     : public ConversionPattern {
-  ONNXElementwiseBinaryOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXElementwiseBinaryOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ONNXPReluOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
@@ -381,7 +381,7 @@ struct ONNXElementwiseBinaryOpLoweringToStableHlo<ONNXPReluOp>
     // Prior code here used the "analysis" version that did not generate code.
     // Since code is actually not needed here at this time, one could use
     // IndexExprBuilderForAnalysis createIE(loc) instead.
-    IndexExprBuilderForStableHlo createShapeIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createShapeIE(rewriter, loc);
     ONNXBroadcastOpShapeHelper shapeHelper(op, operands, &createShapeIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -403,11 +403,11 @@ struct ONNXElementwiseBinaryOpLoweringToStableHlo<ONNXPReluOp>
   }
 };

-// Element-wise variadic ops lowering to StableHlo dialect.
+// Element-wise variadic ops lowering to Stablehlo dialect.
 //===----------------------------------------------------------------------===//
 template <typename ElementwiseVariadicOp>
-struct ONNXElementwiseVariadicOpLoweringToStableHlo : public ConversionPattern {
-  ONNXElementwiseVariadicOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXElementwiseVariadicOpLoweringToStablehlo : public ConversionPattern {
+  ONNXElementwiseVariadicOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(ElementwiseVariadicOp::getOperationName(), 1, ctx) {}
   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
@@ -416,14 +416,14 @@ struct ONNXElementwiseVariadicOpLoweringToStableHlo : public ConversionPattern {
     // Prior code here used the "analysis" version that did not generate code.
     // Since code is actually not needed here at this time, one could use
     // IndexExprBuilderForAnalysis createIE(loc) instead.
-    IndexExprBuilderForStableHlo createShapeIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createShapeIE(rewriter, loc);
     ONNXBroadcastOpShapeHelper shapeHelper(op, operands, &createShapeIE);
     shapeHelper.computeShapeAndAssertOnFailure();

     int64_t outputRank = shapeHelper.outputRank;
     llvm::SmallVector<Value, 4> broadcastedOperands =
         getBroadcastedOperands(op, rewriter, loc, outputRank);
-    Value stableHloOp = rewriter.create<StableHloOp<ElementwiseVariadicOp>>(
+    Value stableHloOp = rewriter.create<StablehloOp<ElementwiseVariadicOp>>(
         loc, op->getResultTypes(), broadcastedOperands);
     rewriter.replaceOp(op, stableHloOp);
     return success();
@@ -432,38 +432,38 @@ struct ONNXElementwiseVariadicOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXElementwiseOpToStableHloPattern(
+void populateLoweringONNXElementwiseOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXAbsOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCastOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCeilOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXCosOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXEluOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXExpOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXHardSigmoidOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXLeakyReluOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXLogOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXNegOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXSigmoidOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXSinOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXSqrtOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXReluOp>,
-      ONNXElementwiseUnaryOpLoweringToStableHlo<ONNXTanhOp>,
-      ONNXElementwiseCompareBinaryOpLoweringToStableHlo<ONNXEqualOp>,
-      ONNXElementwiseCompareBinaryOpLoweringToStableHlo<ONNXGreaterOp>,
-      ONNXElementwiseCompareBinaryOpLoweringToStableHlo<ONNXGreaterOrEqualOp>,
-      ONNXElementwiseCompareBinaryOpLoweringToStableHlo<ONNXLessOp>,
-      ONNXElementwiseCompareBinaryOpLoweringToStableHlo<ONNXLessOrEqualOp>,
-      ONNXElementwiseBinaryOpLoweringToStableHlo<ONNXPowOp>,
-      ONNXElementwiseBinaryOpLoweringToStableHlo<ONNXPReluOp>,
-      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXAddOp>,
-      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXAndOp>,
-      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXDivOp>,
-      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXMaxOp>,
-      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXMinOp>,
-      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXMulOp>,
-      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXSubOp>,
-      ONNXElementwiseVariadicOpLoweringToStableHlo<ONNXWhereOp>>(ctx);
+  patterns.insert<ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXAbsOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXCastOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXCeilOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXCosOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXEluOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXExpOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXHardSigmoidOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXLeakyReluOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXLogOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXNegOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXSigmoidOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXSinOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXSqrtOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXReluOp>,
+      ONNXElementwiseUnaryOpLoweringToStablehlo<ONNXTanhOp>,
+      ONNXElementwiseCompareBinaryOpLoweringToStablehlo<ONNXEqualOp>,
+      ONNXElementwiseCompareBinaryOpLoweringToStablehlo<ONNXGreaterOp>,
+      ONNXElementwiseCompareBinaryOpLoweringToStablehlo<ONNXGreaterOrEqualOp>,
+      ONNXElementwiseCompareBinaryOpLoweringToStablehlo<ONNXLessOp>,
+      ONNXElementwiseCompareBinaryOpLoweringToStablehlo<ONNXLessOrEqualOp>,
+      ONNXElementwiseBinaryOpLoweringToStablehlo<ONNXPowOp>,
+      ONNXElementwiseBinaryOpLoweringToStablehlo<ONNXPReluOp>,
+      ONNXElementwiseVariadicOpLoweringToStablehlo<ONNXAddOp>,
+      ONNXElementwiseVariadicOpLoweringToStablehlo<ONNXAndOp>,
+      ONNXElementwiseVariadicOpLoweringToStablehlo<ONNXDivOp>,
+      ONNXElementwiseVariadicOpLoweringToStablehlo<ONNXMaxOp>,
+      ONNXElementwiseVariadicOpLoweringToStablehlo<ONNXMinOp>,
+      ONNXElementwiseVariadicOpLoweringToStablehlo<ONNXMulOp>,
+      ONNXElementwiseVariadicOpLoweringToStablehlo<ONNXSubOp>,
+      ONNXElementwiseVariadicOpLoweringToStablehlo<ONNXWhereOp>>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Math/Gemm.cpp b/src/Conversion/ONNXToStablehlo/Math/Gemm.cpp
similarity index 93%
rename from src/Conversion/ONNXToStableHlo/Math/Gemm.cpp
rename to src/Conversion/ONNXToStablehlo/Math/Gemm.cpp
index 71d88efc70..b2128bbd5e 100644
--- a/src/Conversion/ONNXToStableHlo/Math/Gemm.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/Gemm.cpp
@@ -8,13 +8,13 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Gemm Operator to StableHlo dialect.
+// This file lowers the ONNX Gemm Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

 #include "llvm/Support/Debug.h"

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"

 using namespace mlir;
@@ -29,10 +29,10 @@ bool closeTo(float a, float b, int ulps = 2) {
          std::fabs(a - b) < std::numeric_limits<float>::min();
 }

-// ONNXGemmOp(A,B,C) is implemented using StableHlo a * Dot(A(T), B(T)) + b * C;
+// ONNXGemmOp(A,B,C) is implemented using Stablehlo a * Dot(A(T), B(T)) + b * C;
 template <typename GemmOp>
-struct ONNXGemmOpLoweringToStableHlo : public ConversionPattern {
-  ONNXGemmOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXGemmOpLoweringToStablehlo : public ConversionPattern {
+  ONNXGemmOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(GemmOp::getOperationName(), 1, ctx) {}

   void replaceGemmOp(ONNXGemmOp &gemmOp, Operation *op,
@@ -154,9 +154,9 @@ struct ONNXGemmOpLoweringToStableHlo : public ConversionPattern {
 };
 } // namespace

-void populateLoweringONNXGemmOpToStableHloPattern(
+void populateLoweringONNXGemmOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXGemmOpLoweringToStableHlo<ONNXGemmOp>>(ctx);
+  patterns.insert<ONNXGemmOpLoweringToStablehlo<ONNXGemmOp>>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Math/MatMul.cpp b/src/Conversion/ONNXToStablehlo/Math/MatMul.cpp
similarity index 90%
rename from src/Conversion/ONNXToStableHlo/Math/MatMul.cpp
rename to src/Conversion/ONNXToStablehlo/Math/MatMul.cpp
index d61bacf52b..8365152c35 100644
--- a/src/Conversion/ONNXToStableHlo/Math/MatMul.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/MatMul.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Matmul Operator to StableHlo dialect.
+// This file lowers the ONNX Matmul Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -27,15 +27,15 @@ int64_t getLiteralValue(const IndexExpr &idx) {
   return idx.isLiteral() ? idx.getLiteral() : ShapedType::kDynamic;
 }

-struct ONNXMatMulOpLoweringToStableHlo : public ConversionPattern {
-  ONNXMatMulOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXMatMulOpLoweringToStablehlo : public ConversionPattern {
+  ONNXMatMulOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXMatMulOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
       ConversionPatternRewriter &rewriter) const final {
     ONNXMatMulOpAdaptor operandAdaptor(operands);
     Location loc = op->getLoc();
-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXMatMulOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -124,9 +124,9 @@ struct ONNXMatMulOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXMatMulOpToStableHloPattern(
+void populateLoweringONNXMatMulOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXMatMulOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXMatMulOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Math/Reduction.cpp b/src/Conversion/ONNXToStablehlo/Math/Reduction.cpp
similarity index 95%
rename from src/Conversion/ONNXToStableHlo/Math/Reduction.cpp
rename to src/Conversion/ONNXToStablehlo/Math/Reduction.cpp
index e9509f4924..b2e5a5bd43 100644
--- a/src/Conversion/ONNXToStableHlo/Math/Reduction.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/Reduction.cpp
@@ -8,11 +8,11 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Reduction Operators to StableHlo dialect.
+// This file lowers the ONNX Reduction Operators to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"

 using namespace mlir;
@@ -277,10 +277,10 @@ Value createReduce(Location loc, Value operand, Value identity,
 // ONNXReductionOp is implmented as stablehlo.reduce with its body built
 // correspondingly.
 template <typename ONNXReductionOp>
-struct ONNXReductionOpLoweringToStableHlo : public ConversionPattern {
+struct ONNXReductionOpLoweringToStablehlo : public ConversionPattern {
   bool computeMean = false;

-  ONNXReductionOpLoweringToStableHlo(MLIRContext *ctx, bool computeMean = false)
+  ONNXReductionOpLoweringToStablehlo(MLIRContext *ctx, bool computeMean = false)
       : ConversionPattern(ONNXReductionOp::getOperationName(), 1, ctx) {
     this->computeMean = computeMean;
   }
@@ -353,14 +353,14 @@ struct ONNXReductionOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXReductionOpToStableHloPattern(
+void populateLoweringONNXReductionOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXReductionOpLoweringToStableHlo<mlir::ONNXReduceMaxV13Op>,
-      ONNXReductionOpLoweringToStableHlo<mlir::ONNXReduceMinV13Op>,
-      ONNXReductionOpLoweringToStableHlo<mlir::ONNXReduceSumOp>,
-      ONNXReductionOpLoweringToStableHlo<mlir::ONNXReduceSumV11Op>>(ctx);
+  patterns.insert<ONNXReductionOpLoweringToStablehlo<mlir::ONNXReduceMaxV13Op>,
+      ONNXReductionOpLoweringToStablehlo<mlir::ONNXReduceMinV13Op>,
+      ONNXReductionOpLoweringToStablehlo<mlir::ONNXReduceSumOp>,
+      ONNXReductionOpLoweringToStablehlo<mlir::ONNXReduceSumV11Op>>(ctx);
   patterns
-      .insert<ONNXReductionOpLoweringToStableHlo<mlir::ONNXReduceMeanV13Op>>(
+      .insert<ONNXReductionOpLoweringToStablehlo<mlir::ONNXReduceMeanV13Op>>(
           ctx, /*computeMean=*/true);
 }

diff --git a/src/Conversion/ONNXToStableHlo/NN/Conv.cpp b/src/Conversion/ONNXToStablehlo/NN/Conv.cpp
similarity index 91%
rename from src/Conversion/ONNXToStableHlo/NN/Conv.cpp
rename to src/Conversion/ONNXToStablehlo/NN/Conv.cpp
index f9e27b88d1..ff06ba5375 100644
--- a/src/Conversion/ONNXToStableHlo/NN/Conv.cpp
+++ b/src/Conversion/ONNXToStablehlo/NN/Conv.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers ONNX Conv Operators to StableHlo dialect.
+// This file lowers ONNX Conv Operators to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -23,8 +23,8 @@ namespace onnx_mlir {

 namespace {

-struct ONNXConvOpLoweringToStableHlo : public ConversionPattern {
-  ONNXConvOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXConvOpLoweringToStablehlo : public ConversionPattern {
+  ONNXConvOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXConvOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -34,8 +34,8 @@ struct ONNXConvOpLoweringToStableHlo : public ConversionPattern {
     ONNXConvOp convOp = llvm::dyn_cast<ONNXConvOp>(op);
     Location loc = op->getLoc();

-    IndexExprBuilderForStableHlo createStableHloIE(rewriter, loc);
-    ONNXConvOpShapeHelper shapeHelper(op, operands, &createStableHloIE);
+    IndexExprBuilderForStablehlo createStablehloIE(rewriter, loc);
+    ONNXConvOpShapeHelper shapeHelper(op, operands, &createStablehloIE);
     shapeHelper.computeShapeAndAssertOnFailure();

     llvm::SmallVector<IndexExpr, 2> kernelShape = shapeHelper.kernelShape;
@@ -146,9 +146,9 @@ struct ONNXConvOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXConvOpToStableHloPattern(
+void populateLoweringONNXConvOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXConvOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXConvOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/NN/ConvTranspose.cpp b/src/Conversion/ONNXToStablehlo/NN/ConvTranspose.cpp
similarity index 94%
rename from src/Conversion/ONNXToStableHlo/NN/ConvTranspose.cpp
rename to src/Conversion/ONNXToStablehlo/NN/ConvTranspose.cpp
index 4309386748..cf8fe9bd53 100644
--- a/src/Conversion/ONNXToStableHlo/NN/ConvTranspose.cpp
+++ b/src/Conversion/ONNXToStablehlo/NN/ConvTranspose.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers ONNX ConvTranspose Operators to StableHlo dialect.
+// This file lowers ONNX ConvTranspose Operators to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Support/TypeUtilities.hpp"

 using namespace mlir;
@@ -22,8 +22,8 @@ namespace onnx_mlir {

 namespace {

-struct ONNXConvTransposeOpLoweringToStableHlo : public ConversionPattern {
-  ONNXConvTransposeOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXConvTransposeOpLoweringToStablehlo : public ConversionPattern {
+  ONNXConvTransposeOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(
             mlir::ONNXConvTransposeOp::getOperationName(), 1, ctx) {}

@@ -69,7 +69,7 @@ struct ONNXConvTransposeOpLoweringToStableHlo : public ConversionPattern {
     ONNXConvTransposeOp convOp = llvm::dyn_cast<ONNXConvTransposeOp>(op);
     Location loc = op->getLoc();

-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXConvTransposeOpShapeHelper shapeHelper(op, operands, &createIE);
     LogicalResult shapecomputed = shapeHelper.computeShape();
     assert(succeeded(shapecomputed) && "Could not compute output shape");
@@ -215,9 +215,9 @@ struct ONNXConvTransposeOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXConvTransposeOpToStableHloPattern(
+void populateLoweringONNXConvTransposeOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXConvTransposeOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXConvTransposeOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/NN/Normalization.cpp b/src/Conversion/ONNXToStablehlo/NN/Normalization.cpp
similarity index 83%
rename from src/Conversion/ONNXToStableHlo/NN/Normalization.cpp
rename to src/Conversion/ONNXToStablehlo/NN/Normalization.cpp
index 08fad08653..59ccbf20ec 100644
--- a/src/Conversion/ONNXToStableHlo/NN/Normalization.cpp
+++ b/src/Conversion/ONNXToStablehlo/NN/Normalization.cpp
@@ -8,11 +8,11 @@
 //
 // =============================================================================
 //
-// This file lowers ONNX Normalization Operators to StableHlo dialect.
+// This file lowers ONNX Normalization Operators to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"

 using namespace mlir;

@@ -20,9 +20,9 @@ namespace onnx_mlir {

 namespace {

-struct ONNXBatchNormalizationInferenceModeOpLoweringToStableHlo
+struct ONNXBatchNormalizationInferenceModeOpLoweringToStablehlo
     : public ConversionPattern {
-  ONNXBatchNormalizationInferenceModeOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXBatchNormalizationInferenceModeOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(
             mlir::ONNXBatchNormalizationInferenceModeOp::getOperationName(), 1,
             ctx) {}
@@ -51,9 +51,9 @@ struct ONNXBatchNormalizationInferenceModeOpLoweringToStableHlo

 } // namespace

-void populateLoweringONNXNormalizationOpToStableHloPattern(
+void populateLoweringONNXNormalizationOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXBatchNormalizationInferenceModeOpLoweringToStableHlo>(
+  patterns.insert<ONNXBatchNormalizationInferenceModeOpLoweringToStablehlo>(
       ctx);
 }

diff --git a/src/Conversion/ONNXToStableHlo/NN/Pooling.cpp b/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp
similarity index 93%
rename from src/Conversion/ONNXToStableHlo/NN/Pooling.cpp
rename to src/Conversion/ONNXToStablehlo/NN/Pooling.cpp
index cc0d81c7a2..abbcc62c56 100644
--- a/src/Conversion/ONNXToStableHlo/NN/Pooling.cpp
+++ b/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Pooling Operators to StableHlo dialect.
+// This file lowers the ONNX Pooling Operators to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"

 using namespace mlir;
@@ -102,8 +102,8 @@ void padVector(
 // Template function that does pooling.
 //
 template <typename PoolOp, typename PoolOpAdaptor, typename PoolOpShapeHelper>
-struct ONNXPoolOpLoweringToStableHlo : public ConversionPattern {
-  ONNXPoolOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXPoolOpLoweringToStablehlo : public ConversionPattern {
+  ONNXPoolOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(PoolOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -113,8 +113,8 @@ struct ONNXPoolOpLoweringToStableHlo : public ConversionPattern {
     PoolOp poolOp = llvm::cast<PoolOp>(op);

     // Get shape.
-    IndexExprBuilderForStableHlo createStableHloIE(rewriter, loc);
-    PoolOpShapeHelper shapeHelper(op, operands, &createStableHloIE);
+    IndexExprBuilderForStablehlo createStablehloIE(rewriter, loc);
+    PoolOpShapeHelper shapeHelper(op, operands, &createStablehloIE);
     shapeHelper.computeShapeAndAssertOnFailure();

     llvm::SmallVector<IndexExpr, 2> kernelShape = shapeHelper.kernelShape;
@@ -217,11 +217,11 @@ struct ONNXPoolOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXPoolingOpToStableHloPattern(
+void populateLoweringONNXPoolingOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXPoolOpLoweringToStableHlo<ONNXMaxPoolSingleOutOp,
+  patterns.insert<ONNXPoolOpLoweringToStablehlo<ONNXMaxPoolSingleOutOp,
       ONNXMaxPoolSingleOutOpAdaptor, ONNXMaxPoolSingleOutOpShapeHelper>>(ctx);
-  patterns.insert<ONNXPoolOpLoweringToStableHlo<ONNXAveragePoolOp,
+  patterns.insert<ONNXPoolOpLoweringToStablehlo<ONNXAveragePoolOp,
       ONNXAveragePoolOpAdaptor, ONNXAveragePoolOpShapeHelper>>(ctx);
 }

diff --git a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.cpp b/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.cpp
similarity index 95%
rename from src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.cpp
rename to src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.cpp
index c5b4c006bc..4a5e09bf49 100644
--- a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.cpp
+++ b/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.cpp
@@ -2,7 +2,7 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====----- ONNXToStableHloCommon.cpp - ONNX dialects to StableHlo lowering
+//====----- ONNXToStablehloCommon.cpp - ONNX dialects to Stablehlo lowering
 //---------===//
 //
 // Copyright 2022
@@ -10,11 +10,11 @@
 // =============================================================================
 //
 // This file contains common code shared by the functions performing the
-// lowering to the StableHlo dialect.
+// lowering to the Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/OpHelper.hpp"
 #include "src/Dialect/ONNX/OnnxElementsAttrBuilder.hpp"

@@ -133,7 +133,7 @@ DenseElementsAttr getDenseElementAttrFromConstValue(mlir::Value value) {

 // Emit an ONNXSqueezeOp. If the input is constant, do const propagation,
 /// and return a constant.
-Value foldOrEmitONNXSqueezeOpStableHlo(ConversionPatternRewriter &rewriter,
+Value foldOrEmitONNXSqueezeOpStablehlo(ConversionPatternRewriter &rewriter,
     Location loc, Type resultType, Value input, int64_t axis) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
   TensorType tensorType = create.onnx.toTensor(resultType);
@@ -152,7 +152,7 @@ Value foldOrEmitONNXSqueezeOpStableHlo(ConversionPatternRewriter &rewriter,

 /// Emit an ONNXUnsqueezeOp. If the input is constant, do const
 /// propagation, and return a constant.
-Value foldOrEmitONNXUnsqueezeOpStableHlo(ConversionPatternRewriter &rewriter,
+Value foldOrEmitONNXUnsqueezeOpStablehlo(ConversionPatternRewriter &rewriter,
     Location loc, Type resultType, Value input, int64_t axis) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
   TensorType tensorType = create.onnx.toTensor(resultType);
@@ -172,7 +172,7 @@ Value foldOrEmitONNXUnsqueezeOpStableHlo(ConversionPatternRewriter &rewriter,
 /// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
 /// return constants.
 /// Only support evenly splitting.
-std::vector<Value> foldOrEmitONNXSplitOpStableHlo(
+std::vector<Value> foldOrEmitONNXSplitOpStablehlo(
     ConversionPatternRewriter &rewriter, Location loc,
     ArrayRef<Type> resultTypes, Value input, int64_t axis) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
@@ -214,7 +214,7 @@ std::vector<Value> foldOrEmitONNXSplitOpStableHlo(

 /// Emit an ONNXTransposeOp. If the input is constant, do const propagation,
 /// and return a constant.
-Value foldOrEmitONNXTransposeOpStableHlo(ConversionPatternRewriter &rewriter,
+Value foldOrEmitONNXTransposeOpStablehlo(ConversionPatternRewriter &rewriter,
     Location loc, Type resultType, Value input, ArrayAttr permAttr) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
   if (DenseElementsAttr inputElements =
diff --git a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp b/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp
similarity index 77%
rename from src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp
rename to src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp
index 8291a638f5..bc77fc217f 100644
--- a/src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp
+++ b/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp
@@ -2,7 +2,7 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====------ ONNXToStableHloCommon.hpp - ONNX dialects to StableHlo lowering
+//====------ ONNXToStablehloCommon.hpp - ONNX dialects to Stablehlo lowering
 //--------===//
 //
 // Copyright 2022
@@ -10,7 +10,7 @@
 // =============================================================================
 //
 // This file contains common code shared by the functions performing the
-// lowering to the StableHlo dialect.
+// lowering to the Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

@@ -24,7 +24,7 @@
 #include "mlir/IR/PatternMatch.h"
 #include "mlir/Pass/Pass.h"
 #include "mlir/Transforms/DialectConversion.h"
-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
 #include "src/Dialect/Mlir/DialectBuilder.hpp"
 #include "src/Dialect/Mlir/IndexExpr.hpp"
 #include "src/Dialect/ONNX/DialectBuilder.hpp"
@@ -49,15 +49,15 @@ namespace onnx_mlir {
 // operation.
 //===----------------------------------------------------------------------===//
 template <typename ONNXOp>
-struct StableHloDialectOp {
+struct StablehloDialectOp {
   using Op = void;
 };

 template <typename ONNXOp>
-using StableHloOp = typename StableHloDialectOp<ONNXOp>::Op;
+using StablehloOp = typename StablehloDialectOp<ONNXOp>::Op;

 //===----------------------------------------------------------------------===//
-// Common functions used when lowering the ONNX frontend dialect to StableHlo.
+// Common functions used when lowering the ONNX frontend dialect to Stablehlo.
 //===----------------------------------------------------------------------===//

 // Get shaped constant zero for the given input Type. If the input type
@@ -127,91 +127,91 @@ DenseIntElementsAttr GetI64ElementsAttr(

 /// Emit an ONNXSqueezeOp. If the input is constant, do const propagation, and
 /// return a constant.
-mlir::Value foldOrEmitONNXSqueezeOpStableHlo(
+mlir::Value foldOrEmitONNXSqueezeOpStablehlo(
     mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
     mlir::Type resultType, mlir::Value input, int64_t axis);

 /// Emit an ONNXUnsqueezeOp. If the input is constant, do const propagation, and
 /// return a constant.
-mlir::Value foldOrEmitONNXUnsqueezeOpStableHlo(
+mlir::Value foldOrEmitONNXUnsqueezeOpStablehlo(
     mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
     mlir::Type resultType, mlir::Value input, int64_t axis);

 /// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
 /// return constants.
 /// Only support evenly splitting.
-std::vector<mlir::Value> foldOrEmitONNXSplitOpStableHlo(
+std::vector<mlir::Value> foldOrEmitONNXSplitOpStablehlo(
     mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
     llvm::ArrayRef<mlir::Type> resultTypes, mlir::Value input, int64_t axis);

 /// Emit an ONNXTransposeOp. If the input is constant, do const propagation, and
 /// return a constant.
-mlir::Value foldOrEmitONNXTransposeOpStableHlo(
+mlir::Value foldOrEmitONNXTransposeOpStablehlo(
     mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
     mlir::Type resultType, mlir::Value input, mlir::ArrayAttr permAttr);

 // `Math` directory methods:
-void populateLoweringONNXClipOpToStableHloPattern(
+void populateLoweringONNXClipOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXElementwiseOpToStableHloPattern(
+void populateLoweringONNXElementwiseOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXGemmOpToStableHloPattern(
+void populateLoweringONNXGemmOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXMatMulOpToStableHloPattern(
+void populateLoweringONNXMatMulOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXReductionOpToStableHloPattern(
+void populateLoweringONNXReductionOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
 // `NN` directory methods:
-void populateLoweringONNXConvOpToStableHloPattern(
+void populateLoweringONNXConvOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXConvTransposeOpToStableHloPattern(
+void populateLoweringONNXConvTransposeOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXNormalizationOpToStableHloPattern(
+void populateLoweringONNXNormalizationOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXPoolingOpToStableHloPattern(
+void populateLoweringONNXPoolingOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
 // `RNN` directory methods:
-void populateLoweringONNXLSTMOpToStableHloPattern(
+void populateLoweringONNXLSTMOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *, bool);
 // `Tensor` directory methods:
-void populateLoweringONNXArgMaxOpToStableHloPattern(
+void populateLoweringONNXArgMaxOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXConcatOpToStableHloPattern(
+void populateLoweringONNXConcatOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXConstantOpToStableHloPattern(
+void populateLoweringONNXConstantOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXDepthToSpaceOpToStableHloPattern(
+void populateLoweringONNXDepthToSpaceOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXExpandOpToStableHloPattern(
+void populateLoweringONNXExpandOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXFlattenOpToStableHloPattern(
+void populateLoweringONNXFlattenOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXGatherOpToStableHloPattern(
+void populateLoweringONNXGatherOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXGatherElementsOpToStableHloPattern(
+void populateLoweringONNXGatherElementsOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXIdentityOpToStableHloPattern(
+void populateLoweringONNXIdentityOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXOneHotOpToStableHloPattern(
+void populateLoweringONNXOneHotOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXPadOpToStableHloPattern(
+void populateLoweringONNXPadOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXReshapeOpToStableHloPattern(
+void populateLoweringONNXReshapeOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXScatterNDOpToStableHloPattern(
+void populateLoweringONNXScatterNDOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXShapeOpToStableHloPattern(
+void populateLoweringONNXShapeOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXSliceOpToStableHloPattern(
+void populateLoweringONNXSliceOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXSplitOpToStableHloPattern(
+void populateLoweringONNXSplitOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXSqueezeOpToStableHloPattern(
+void populateLoweringONNXSqueezeOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXTileOpToStableHloPattern(
+void populateLoweringONNXTileOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXTransposeOpToStableHloPattern(
+void populateLoweringONNXTransposeOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
-void populateLoweringONNXUnsqueezeOpToStableHloPattern(
+void populateLoweringONNXUnsqueezeOpToStablehloPattern(
     RewritePatternSet &, MLIRContext *);
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp b/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp
similarity index 94%
rename from src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp
rename to src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp
index 17c9e5dae8..41203a3e4e 100644
--- a/src/Conversion/ONNXToStableHlo/RNN/LSTM.cpp
+++ b/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX LSTM Operators to StableHlo dialect.
+// This file lowers the ONNX LSTM Operators to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp"
 #include "src/Dialect/Mlir/DialectBuilder.hpp"

 #include "llvm/Support/Debug.h"
@@ -240,25 +240,25 @@ getWeightPack<ONNXLSTMOp, LstmWeightPack>(
   // Squeeze the direction axis from W and R.
   Value fW, bW, fR, bR;
   if (direction == FORWARD) {
-    fW = foldOrEmitONNXSqueezeOpStableHlo(rewriter, loc, w2DTy, W, /*axis=*/0);
-    fR = foldOrEmitONNXSqueezeOpStableHlo(rewriter, loc, r2DTy, R, /*axis=*/0);
+    fW = foldOrEmitONNXSqueezeOpStablehlo(rewriter, loc, w2DTy, W, /*axis=*/0);
+    fR = foldOrEmitONNXSqueezeOpStablehlo(rewriter, loc, r2DTy, R, /*axis=*/0);
   } else if (direction == REVERSE) {
-    bW = foldOrEmitONNXSqueezeOpStableHlo(rewriter, loc, w2DTy, W, /*axis=*/0);
-    bR = foldOrEmitONNXSqueezeOpStableHlo(rewriter, loc, r2DTy, R, /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeOpStablehlo(rewriter, loc, w2DTy, W, /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeOpStablehlo(rewriter, loc, r2DTy, R, /*axis=*/0);
   } else { // BIDIRECTIONAL
     // W
     std::vector<Value> vals =
-        foldOrEmitONNXSplitOpStableHlo(rewriter, loc, w3D2Ty, W, 0);
-    fW = foldOrEmitONNXSqueezeOpStableHlo(
+        foldOrEmitONNXSplitOpStablehlo(rewriter, loc, w3D2Ty, W, 0);
+    fW = foldOrEmitONNXSqueezeOpStablehlo(
         rewriter, loc, w2DTy, vals[0], /*axis=*/0);
-    bW = foldOrEmitONNXSqueezeOpStableHlo(
+    bW = foldOrEmitONNXSqueezeOpStablehlo(
         rewriter, loc, w2DTy, vals[1], /*axis=*/0);
     // R
     vals.clear();
-    vals = foldOrEmitONNXSplitOpStableHlo(rewriter, loc, r3D2Ty, R, 0);
-    fR = foldOrEmitONNXSqueezeOpStableHlo(
+    vals = foldOrEmitONNXSplitOpStablehlo(rewriter, loc, r3D2Ty, R, 0);
+    fR = foldOrEmitONNXSqueezeOpStablehlo(
         rewriter, loc, r2DTy, vals[0], /*axis=*/0);
-    bR = foldOrEmitONNXSqueezeOpStableHlo(
+    bR = foldOrEmitONNXSqueezeOpStablehlo(
         rewriter, loc, r2DTy, vals[1], /*axis=*/0);
   }

@@ -266,18 +266,18 @@ getWeightPack<ONNXLSTMOp, LstmWeightPack>(
   ArrayAttr permAttr = rewriter.getI64ArrayAttr({1, 0});
   if (direction == FORWARD || direction == BIDIRECTIONAL) {
     // W
-    weightForward.WT = foldOrEmitONNXTransposeOpStableHlo(
+    weightForward.WT = foldOrEmitONNXTransposeOpStablehlo(
         rewriter, loc, wTranspose2DTy, fW, permAttr);
     // R
-    weightForward.RT = foldOrEmitONNXTransposeOpStableHlo(
+    weightForward.RT = foldOrEmitONNXTransposeOpStablehlo(
         rewriter, loc, rTranspose2DTy, fR, permAttr);
   }
   if (direction == REVERSE || direction == BIDIRECTIONAL) {
     // W
-    weightReverse.WT = foldOrEmitONNXTransposeOpStableHlo(
+    weightReverse.WT = foldOrEmitONNXTransposeOpStablehlo(
         rewriter, loc, wTranspose2DTy, bW, permAttr);
     // R
-    weightReverse.RT = foldOrEmitONNXTransposeOpStableHlo(
+    weightReverse.RT = foldOrEmitONNXTransposeOpStablehlo(
         rewriter, loc, rTranspose2DTy, bR, permAttr);
   }
   return std::make_tuple(weightForward, weightReverse);
@@ -313,24 +313,24 @@ std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
     // Squeeze the direction axis from B.
     Value fB, bB;
     if (direction == FORWARD) {
-      fB = foldOrEmitONNXSqueezeOpStableHlo(
+      fB = foldOrEmitONNXSqueezeOpStablehlo(
           rewriter, loc, bType1D, B, /*axis=*/0);
     } else if (direction == REVERSE) {
-      bB = foldOrEmitONNXSqueezeOpStableHlo(
+      bB = foldOrEmitONNXSqueezeOpStablehlo(
           rewriter, loc, bType1D, B, /*axis=*/0);
     } else { // BIDIRECTIONAL
       std::vector<Value> vals;
-      vals = foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split2D2Ty, B, 0);
-      fB = foldOrEmitONNXSqueezeOpStableHlo(
+      vals = foldOrEmitONNXSplitOpStablehlo(rewriter, loc, split2D2Ty, B, 0);
+      fB = foldOrEmitONNXSqueezeOpStablehlo(
           rewriter, loc, bType1D, vals[0], /*axis=*/0);
-      bB = foldOrEmitONNXSqueezeOpStableHlo(
+      bB = foldOrEmitONNXSqueezeOpStablehlo(
           rewriter, loc, bType1D, vals[1], /*axis=*/0);
     }

     // Split B into individual bias tensors.
     if (direction == FORWARD || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split1D8Ty, fB, 0);
+          foldOrEmitONNXSplitOpStablehlo(rewriter, loc, split1D8Ty, fB, 0);
       biasForward.Wbi = vals[0];
       biasForward.Wbo = vals[1];
       biasForward.Wbf = vals[2];
@@ -343,7 +343,7 @@ std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
     }
     if (direction == REVERSE || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split1D8Ty, bB, 0);
+          foldOrEmitONNXSplitOpStablehlo(rewriter, loc, split1D8Ty, bB, 0);
       biasReverse.Wbi = vals[0];
       biasReverse.Wbo = vals[1];
       biasReverse.Wbf = vals[2];
@@ -372,24 +372,24 @@ std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
     // Squeeze the direction axis from P.
     Value fP, bP;
     if (direction == FORWARD) {
-      fP = foldOrEmitONNXSqueezeOpStableHlo(
+      fP = foldOrEmitONNXSqueezeOpStablehlo(
           rewriter, loc, pType1D, P, /*axis=*/0);
     } else if (direction == REVERSE) {
-      bP = foldOrEmitONNXSqueezeOpStableHlo(
+      bP = foldOrEmitONNXSqueezeOpStablehlo(
           rewriter, loc, pType1D, P, /*axis=*/0);
     } else { // BIDIRECTIONAL
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split2D2Ty, P, 0);
-      fP = foldOrEmitONNXSqueezeOpStableHlo(
+          foldOrEmitONNXSplitOpStablehlo(rewriter, loc, split2D2Ty, P, 0);
+      fP = foldOrEmitONNXSqueezeOpStablehlo(
           rewriter, loc, pType1D, vals[0], /*axis=*/0);
-      bP = foldOrEmitONNXSqueezeOpStableHlo(
+      bP = foldOrEmitONNXSqueezeOpStablehlo(
           rewriter, loc, pType1D, vals[1], /*axis=*/0);
     }

     // Split P into individual tensors.
     if (direction == FORWARD || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split1D3Ty, fP, 0);
+          foldOrEmitONNXSplitOpStablehlo(rewriter, loc, split1D3Ty, fP, 0);
       biasForward.Pi = vals[0];
       biasForward.Po = vals[1];
       biasForward.Pf = vals[2];
@@ -397,7 +397,7 @@ std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
     }
     if (direction == REVERSE || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOpStableHlo(rewriter, loc, split1D3Ty, bP, 0);
+          foldOrEmitONNXSplitOpStablehlo(rewriter, loc, split1D3Ty, bP, 0);
       biasReverse.Pi = vals[0];
       biasReverse.Po = vals[1];
       biasReverse.Pf = vals[2];
@@ -820,7 +820,7 @@ void calculateStateWithLoop<ONNXLSTMOp, LstmState, LstmActivationPack,

 } // namespace stablehlo

-void populateLoweringONNXLSTMOpToStableHloPattern(
+void populateLoweringONNXLSTMOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx, bool enableUnroll) {
   patterns.insert<onnx_mlir::stablehlo::ONNXRNNOpLowering<ONNXLSTMOp,
       onnx_mlir::stablehlo::LstmState, onnx_mlir::stablehlo::LstmActivationPack,
diff --git a/src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp
similarity index 98%
rename from src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp
rename to src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp
index 6fb0d7aeb9..f81eaff2d7 100644
--- a/src/Conversion/ONNXToStableHlo/RNN/RNNBase.cpp
+++ b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp
@@ -12,8 +12,8 @@
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"

 using namespace mlir;

diff --git a/src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp
similarity index 99%
rename from src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp
rename to src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp
index a0dbde620a..4e6f7336eb 100644
--- a/src/Conversion/ONNXToStableHlo/RNN/RNNBase.hpp
+++ b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp
@@ -14,7 +14,7 @@

 #pragma once

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"

 static constexpr llvm::StringRef FORWARD = "forward";
 static constexpr llvm::StringRef REVERSE = "reverse";
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/ArgMax.cpp b/src/Conversion/ONNXToStablehlo/Tensor/ArgMax.cpp
similarity index 91%
rename from src/Conversion/ONNXToStableHlo/Tensor/ArgMax.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/ArgMax.cpp
index 57be589e12..45a3cd32ba 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/ArgMax.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/ArgMax.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX ArgMax Operator to StableHlo dialect.
+// This file lowers the ONNX ArgMax Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -55,10 +55,10 @@ static void BuildArgmaxReductionBody(
   b.create<stablehlo::ReturnOp>(returnValues);
 }

-// ONNXArgMaxOp is mainly implemented using StableHlo DynamicIotaOp and
+// ONNXArgMaxOp is mainly implemented using Stablehlo DynamicIotaOp and
 // ReduceOp.
-struct ONNXArgMaxOpLoweringToStableHlo : public ConversionPattern {
-  ONNXArgMaxOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXArgMaxOpLoweringToStablehlo : public ConversionPattern {
+  ONNXArgMaxOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXArgMaxOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -69,7 +69,7 @@ struct ONNXArgMaxOpLoweringToStableHlo : public ConversionPattern {
     ONNXArgMaxOp argMaxOp = llvm::cast<ONNXArgMaxOp>(op);

     // Shape helper (not really used).
-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXArgMaxOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -150,9 +150,9 @@ struct ONNXArgMaxOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXArgMaxOpToStableHloPattern(
+void populateLoweringONNXArgMaxOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXArgMaxOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXArgMaxOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Concat.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Concat.cpp
similarity index 82%
rename from src/Conversion/ONNXToStableHlo/Tensor/Concat.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Concat.cpp
index ce32c4e701..25158021f0 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Concat.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Concat.cpp
@@ -8,13 +8,13 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Concat Operator to StableHlo dialect.
+// This file lowers the ONNX Concat Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

 #include "llvm/Support/Debug.h"

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Support/TypeUtilities.hpp"

 #define DEBUG_TYPE "onnx_to_stablehlo"
@@ -25,8 +25,8 @@ namespace onnx_mlir {

 namespace {

-struct ONNXConcatOpLoweringToStableHlo : public ConversionPattern {
-  ONNXConcatOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXConcatOpLoweringToStablehlo : public ConversionPattern {
+  ONNXConcatOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXConcatOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -57,9 +57,9 @@ struct ONNXConcatOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXConcatOpToStableHloPattern(
+void populateLoweringONNXConcatOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXConcatOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXConcatOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Constant.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Constant.cpp
similarity index 78%
rename from src/Conversion/ONNXToStableHlo/Tensor/Constant.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Constant.cpp
index c12f20951d..b19e2885fb 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Constant.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Constant.cpp
@@ -8,11 +8,11 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Constant Operator to StableHlo dialect.
+// This file lowers the ONNX Constant Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ElementsAttr/DisposableElementsAttr.hpp"

 using namespace mlir;
@@ -21,8 +21,8 @@ namespace onnx_mlir {

 namespace {

-struct ONNXConstantOpLoweringToStableHlo : public ConversionPattern {
-  ONNXConstantOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXConstantOpLoweringToStablehlo : public ConversionPattern {
+  ONNXConstantOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXConstantOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -43,9 +43,9 @@ struct ONNXConstantOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXConstantOpToStableHloPattern(
+void populateLoweringONNXConstantOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXConstantOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXConstantOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/DepthToSpace.cpp b/src/Conversion/ONNXToStablehlo/Tensor/DepthToSpace.cpp
similarity index 92%
rename from src/Conversion/ONNXToStableHlo/Tensor/DepthToSpace.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/DepthToSpace.cpp
index f1819ec37f..b73e5d9bf5 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/DepthToSpace.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/DepthToSpace.cpp
@@ -12,8 +12,8 @@
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"

 using namespace mlir;
@@ -35,7 +35,7 @@ struct ONNXDepthToSpaceOpLoweringToStablehlo
     ValueRange operands = adaptor.getOperands();
     Value input = adaptor.getInput();

-    MultiDialectBuilder<IndexExprBuilderForStableHlo, OnnxToStablehloBuilder>
+    MultiDialectBuilder<IndexExprBuilderForStablehlo, OnnxToStablehloBuilder>
         create(rewriter, loc);
     ONNXDepthToSpaceOpShapeHelper shapeHelper(
         op, operands, &create.stableHloIE);
@@ -91,7 +91,7 @@ struct ONNXDepthToSpaceOpLoweringToStablehlo

 } // namespace

-void populateLoweringONNXDepthToSpaceOpToStableHloPattern(
+void populateLoweringONNXDepthToSpaceOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
   patterns.insert<ONNXDepthToSpaceOpLoweringToStablehlo>(ctx);
 }
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Expand.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Expand.cpp
similarity index 86%
rename from src/Conversion/ONNXToStableHlo/Tensor/Expand.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Expand.cpp
index d0cdab8419..70d19c93a0 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Expand.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Expand.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Expand Operator to StableHlo dialect.
+// This file lowers the ONNX Expand Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -21,9 +21,9 @@ using namespace mlir;

 namespace onnx_mlir {

-// ONNXExpandOp(A) is mainly implemented using StableHlo mulOp(A, 1s)
-struct ONNXExpandOpLoweringToStableHlo : public ConversionPattern {
-  ONNXExpandOpLoweringToStableHlo(MLIRContext *ctx)
+// ONNXExpandOp(A) is mainly implemented using Stablehlo mulOp(A, 1s)
+struct ONNXExpandOpLoweringToStablehlo : public ConversionPattern {
+  ONNXExpandOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXExpandOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -36,9 +36,9 @@ struct ONNXExpandOpLoweringToStableHlo : public ConversionPattern {
     Location loc = op->getLoc();

     // Cannot be used because ExpandOp Shape helper scans for onnx ops in the
-    // inputs, and StableHlo conversion has already removed them.
+    // inputs, and Stablehlo conversion has already removed them.

-    // IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    // IndexExprBuilderForStablehlo createIE(rewriter, loc);
     // ONNXExpandOpShapeHelper shapeHelper(op, operands, &createIE);
     // LogicalResult shapeComputed = shapeHelper.computeShape();
     // assert(succeeded(shapeComputed) && "Failed to compute shape");
@@ -95,9 +95,9 @@ struct ONNXExpandOpLoweringToStableHlo : public ConversionPattern {
   }
 };

-void populateLoweringONNXExpandOpToStableHloPattern(
+void populateLoweringONNXExpandOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXExpandOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXExpandOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Flatten.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Flatten.cpp
similarity index 85%
rename from src/Conversion/ONNXToStableHlo/Tensor/Flatten.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Flatten.cpp
index c426a410c2..3ff0b244c4 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Flatten.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Flatten.cpp
@@ -8,11 +8,11 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Flatten Operator to StableHlo dialect.
+// This file lowers the ONNX Flatten Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Support/TypeUtilities.hpp"

 using namespace mlir;
@@ -21,9 +21,9 @@ namespace onnx_mlir {

 namespace {

-// ONNXFlattenOp(A) is mainly implemented using StableHlo reshapeOp
-struct ONNXFlattenOpLoweringToStableHlo : public ConversionPattern {
-  ONNXFlattenOpLoweringToStableHlo(MLIRContext *ctx)
+// ONNXFlattenOp(A) is mainly implemented using Stablehlo reshapeOp
+struct ONNXFlattenOpLoweringToStablehlo : public ConversionPattern {
+  ONNXFlattenOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXFlattenOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -68,9 +68,9 @@ struct ONNXFlattenOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXFlattenOpToStableHloPattern(
+void populateLoweringONNXFlattenOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXFlattenOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXFlattenOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Gather.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Gather.cpp
similarity index 85%
rename from src/Conversion/ONNXToStableHlo/Tensor/Gather.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Gather.cpp
index f7deced9a7..ecc116148d 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Gather.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Gather.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Gather Operator to StableHlo dialect.
+// This file lowers the ONNX Gather Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -23,9 +23,9 @@ namespace onnx_mlir {

 namespace {

-// ONNXGatherOp is mainly implemented using StableHlo TorchIndexSelectOp
-struct ONNXGatherOpLoweringToStableHlo : public ConversionPattern {
-  ONNXGatherOpLoweringToStableHlo(MLIRContext *ctx)
+// ONNXGatherOp is mainly implemented using Stablehlo TorchIndexSelectOp
+struct ONNXGatherOpLoweringToStablehlo : public ConversionPattern {
+  ONNXGatherOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXGatherOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -35,7 +35,7 @@ struct ONNXGatherOpLoweringToStableHlo : public ConversionPattern {
     Location loc = op->getLoc();

     // Is it unused?
-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXGatherOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -87,9 +87,9 @@ struct ONNXGatherOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXGatherOpToStableHloPattern(
+void populateLoweringONNXGatherOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXGatherOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXGatherOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/GatherElements.cpp b/src/Conversion/ONNXToStablehlo/Tensor/GatherElements.cpp
similarity index 92%
rename from src/Conversion/ONNXToStableHlo/Tensor/GatherElements.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/GatherElements.cpp
index 1b320421bd..0f84a26586 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/GatherElements.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/GatherElements.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX GatherElements Operator to StableHlo dialect.
+// This file lowers the ONNX GatherElements Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -23,8 +23,8 @@ namespace onnx_mlir {

 namespace {

-struct ONNXGatherElementsOpLoweringToStableHlo : public ConversionPattern {
-  ONNXGatherElementsOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXGatherElementsOpLoweringToStablehlo : public ConversionPattern {
+  ONNXGatherElementsOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(
             mlir::ONNXGatherElementsOp::getOperationName(), 1, ctx) {}

@@ -34,7 +34,7 @@ struct ONNXGatherElementsOpLoweringToStableHlo : public ConversionPattern {
     ONNXGatherElementsOp gatherOp = cast<ONNXGatherElementsOp>(op);
     Location loc = op->getLoc();

-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXGatherElementsOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -146,9 +146,9 @@ struct ONNXGatherElementsOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXGatherElementsOpToStableHloPattern(
+void populateLoweringONNXGatherElementsOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXGatherElementsOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXGatherElementsOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Identity.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Identity.cpp
similarity index 70%
rename from src/Conversion/ONNXToStableHlo/Tensor/Identity.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Identity.cpp
index 519490db91..197eb60883 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Identity.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Identity.cpp
@@ -8,11 +8,11 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNXIdentity operator to the StableHlo dialect.
+// This file lowers the ONNXIdentity operator to the Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"

 using namespace mlir;

@@ -20,8 +20,8 @@ namespace onnx_mlir {

 namespace {

-struct ONNXIdentityOpLoweringToStableHlo : public ConversionPattern {
-  ONNXIdentityOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXIdentityOpLoweringToStablehlo : public ConversionPattern {
+  ONNXIdentityOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXIdentityOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -34,9 +34,9 @@ struct ONNXIdentityOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXIdentityOpToStableHloPattern(
+void populateLoweringONNXIdentityOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXIdentityOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXIdentityOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp b/src/Conversion/ONNXToStablehlo/Tensor/OneHot.cpp
similarity index 91%
rename from src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/OneHot.cpp
index 411f3db4b5..71b3ca50a5 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/OneHot.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/OneHot.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX OneHot Operator to StableHlo dialect.
+// This file lowers the ONNX OneHot Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"

 #include <numeric>
@@ -24,9 +24,9 @@ namespace onnx_mlir {

 namespace {

-struct ONNXOneHotOpLoweringToStableHlo
+struct ONNXOneHotOpLoweringToStablehlo
     : public OpConversionPattern<ONNXOneHotOp> {
-  ONNXOneHotOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXOneHotOpLoweringToStablehlo(MLIRContext *ctx)
       : OpConversionPattern(ctx) {}

   LogicalResult matchAndRewrite(ONNXOneHotOp onehotOp,
@@ -41,7 +41,7 @@ struct ONNXOneHotOpLoweringToStableHlo
     Value values = adaptor.getValues();
     Type outputType = *op->result_type_begin();

-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXOneHotOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();
     int64_t axis = shapeHelper.axis;
@@ -119,9 +119,9 @@ struct ONNXOneHotOpLoweringToStableHlo

 } // namespace

-void populateLoweringONNXOneHotOpToStableHloPattern(
+void populateLoweringONNXOneHotOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXOneHotOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXOneHotOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Pad.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Pad.cpp
similarity index 96%
rename from src/Conversion/ONNXToStableHlo/Tensor/Pad.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Pad.cpp
index 55f279719a..44d7303c39 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Pad.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Pad.cpp
@@ -12,7 +12,7 @@
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ElementsAttr/DisposableElementsAttr.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"
@@ -93,7 +93,7 @@ struct ONNXPadOpLoweringToStablehlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXPadOpToStableHloPattern(
+void populateLoweringONNXPadOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
   patterns.insert<ONNXPadOpLoweringToStablehlo>(ctx);
 }
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Reshape.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Reshape.cpp
similarity index 77%
rename from src/Conversion/ONNXToStableHlo/Tensor/Reshape.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Reshape.cpp
index 061ff3f7e2..b3f9e28b75 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Reshape.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Reshape.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Reshape Operator to StableHlo dialect.
+// This file lowers the ONNX Reshape Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"

 using namespace mlir;
@@ -22,8 +22,8 @@ namespace onnx_mlir {

 namespace {

-struct ONNXReshapeOpLoweringToStableHlo : public ConversionPattern {
-  ONNXReshapeOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXReshapeOpLoweringToStablehlo : public ConversionPattern {
+  ONNXReshapeOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXReshapeOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -33,7 +33,7 @@ struct ONNXReshapeOpLoweringToStableHlo : public ConversionPattern {
     Value data = operandAdaptor.getData();
     Type outputType = *op->result_type_begin();

-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXReshapeOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();
     DimsExpr outputDims = shapeHelper.getOutputDims();
@@ -54,9 +54,9 @@ struct ONNXReshapeOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXReshapeOpToStableHloPattern(
+void populateLoweringONNXReshapeOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXReshapeOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXReshapeOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp b/src/Conversion/ONNXToStablehlo/Tensor/ScatterND.cpp
similarity index 90%
rename from src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/ScatterND.cpp
index 1dd2569264..80d5950948 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/ScatterND.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/ScatterND.cpp
@@ -8,11 +8,11 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX ScatterND Operator to StableHlo dialect.
+// This file lowers the ONNX ScatterND Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"

 using namespace mlir;
@@ -21,9 +21,9 @@ namespace onnx_mlir {

 namespace {

-struct ONNXScatterNDOpLoweringToStableHlo
+struct ONNXScatterNDOpLoweringToStablehlo
     : public OpConversionPattern<ONNXScatterNDOp> {
-  ONNXScatterNDOpLoweringToStableHlo(MLIRContext *ctx)
+  ONNXScatterNDOpLoweringToStablehlo(MLIRContext *ctx)
       : OpConversionPattern(ctx) {}

   LogicalResult matchAndRewrite(ONNXScatterNDOp scatterNDOp,
@@ -88,9 +88,9 @@ struct ONNXScatterNDOpLoweringToStableHlo

 } // namespace

-void populateLoweringONNXScatterNDOpToStableHloPattern(
+void populateLoweringONNXScatterNDOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXScatterNDOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXScatterNDOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Shape.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Shape.cpp
similarity index 78%
rename from src/Conversion/ONNXToStableHlo/Tensor/Shape.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Shape.cpp
index e079fa2576..5d219dd348 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Shape.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Shape.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Shape Operator to StableHlo dialect.
+// This file lowers the ONNX Shape Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"

 using namespace mlir;
@@ -22,8 +22,8 @@ namespace onnx_mlir {

 namespace {

-struct ONNXShapeOpLoweringToStableHlo : public ConversionPattern {
-  ONNXShapeOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXShapeOpLoweringToStablehlo : public ConversionPattern {
+  ONNXShapeOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXShapeOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -32,7 +32,7 @@ struct ONNXShapeOpLoweringToStableHlo : public ConversionPattern {
     ONNXShapeOpAdaptor operandAdaptor(operands, op->getAttrDictionary());
     ONNXShapeOp shapeOp = cast<ONNXShapeOp>(op);
     Location loc = op->getLoc();
-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXShapeOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -54,9 +54,9 @@ struct ONNXShapeOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXShapeOpToStableHloPattern(
+void populateLoweringONNXShapeOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXShapeOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXShapeOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Slice.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Slice.cpp
similarity index 95%
rename from src/Conversion/ONNXToStableHlo/Tensor/Slice.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Slice.cpp
index 3255f15209..8c68c9097c 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Slice.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Slice.cpp
@@ -8,11 +8,11 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Slice Operator to StableHlo dialect.
+// This file lowers the ONNX Slice Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/Krnl/KrnlHelper.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"
@@ -23,10 +23,10 @@ namespace onnx_mlir {

 namespace {

-// ONNXSliceOp(A) is mainly implemented using StableHlo sliceOp, and follows the
+// ONNXSliceOp(A) is mainly implemented using Stablehlo sliceOp, and follows the
 // onnx definition of slice.
-struct ONNXSliceOpLoweringToStableHlo : public ConversionPattern {
-  ONNXSliceOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXSliceOpLoweringToStablehlo : public ConversionPattern {
+  ONNXSliceOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXSliceOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -189,9 +189,9 @@ struct ONNXSliceOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXSliceOpToStableHloPattern(
+void populateLoweringONNXSliceOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXSliceOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXSliceOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Split.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Split.cpp
similarity index 86%
rename from src/Conversion/ONNXToStableHlo/Tensor/Split.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Split.cpp
index 0e5d2b2db3..6a3cad9684 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Split.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Split.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Split Operator to StableHlo dialect.
+// This file lowers the ONNX Split Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -23,9 +23,9 @@ namespace onnx_mlir {

 namespace {

-// ONNXSplitOp(A) is implemented using StableHlo sliceOp
-struct ONNXSplitOpLoweringToStableHlo : public ConversionPattern {
-  ONNXSplitOpLoweringToStableHlo(MLIRContext *ctx)
+// ONNXSplitOp(A) is implemented using Stablehlo sliceOp
+struct ONNXSplitOpLoweringToStablehlo : public ConversionPattern {
+  ONNXSplitOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXSplitOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -47,7 +47,7 @@ struct ONNXSplitOpLoweringToStableHlo : public ConversionPattern {
     int64_t inputDimSize = inputType.getDimSize(dimIndex);

     // Get a shape helper (not used?)
-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXSplitOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -97,9 +97,9 @@ struct ONNXSplitOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXSplitOpToStableHloPattern(
+void populateLoweringONNXSplitOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXSplitOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXSplitOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Squeeze.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Squeeze.cpp
similarity index 83%
rename from src/Conversion/ONNXToStableHlo/Tensor/Squeeze.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Squeeze.cpp
index 8a2320677f..f6d3545523 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Squeeze.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Squeeze.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Squeeze Operator to StableHlo dialect.
+// This file lowers the ONNX Squeeze Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -23,9 +23,9 @@ namespace onnx_mlir {

 namespace {

-// ONNXSqueezeOp(A) is implemented using StableHlo reshapeOp
-struct ONNXSqueezeOpLoweringToStableHlo : public ConversionPattern {
-  ONNXSqueezeOpLoweringToStableHlo(MLIRContext *ctx)
+// ONNXSqueezeOp(A) is implemented using Stablehlo reshapeOp
+struct ONNXSqueezeOpLoweringToStablehlo : public ConversionPattern {
+  ONNXSqueezeOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXSqueezeOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -41,7 +41,7 @@ struct ONNXSqueezeOpLoweringToStableHlo : public ConversionPattern {
     int64_t rank = dataType.getRank();

     // Shape helper is unused
-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXSqueezeOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -83,9 +83,9 @@ struct ONNXSqueezeOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXSqueezeOpToStableHloPattern(
+void populateLoweringONNXSqueezeOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXSqueezeOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXSqueezeOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Tile.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Tile.cpp
similarity index 91%
rename from src/Conversion/ONNXToStableHlo/Tensor/Tile.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Tile.cpp
index 2a76f24a30..778144b720 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Tile.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Tile.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Tile Operator to StableHlo dialect.
+// This file lowers the ONNX Tile Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -21,9 +21,9 @@ using namespace mlir;

 namespace onnx_mlir {

-// ONNXTileOp(A) is mainly implemented using StableHlo broadcastOp & reshapeOp
-struct ONNXTileOpLoweringToStableHlo : public ConversionPattern {
-  ONNXTileOpLoweringToStableHlo(MLIRContext *ctx)
+// ONNXTileOp(A) is mainly implemented using Stablehlo broadcastOp & reshapeOp
+struct ONNXTileOpLoweringToStablehlo : public ConversionPattern {
+  ONNXTileOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXTileOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -127,9 +127,9 @@ struct ONNXTileOpLoweringToStableHlo : public ConversionPattern {
   }
 };

-void populateLoweringONNXTileOpToStableHloPattern(
+void populateLoweringONNXTileOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXTileOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXTileOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Transpose.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Transpose.cpp
similarity index 84%
rename from src/Conversion/ONNXToStableHlo/Tensor/Transpose.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Transpose.cpp
index 8a750b347a..d2f0151f09 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Transpose.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Transpose.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Transpose Operator to StableHlo dialect.
+// This file lowers the ONNX Transpose Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -23,8 +23,8 @@ namespace onnx_mlir {

 namespace {

-struct ONNXTransposeOpLoweringToStableHlo : public ConversionPattern {
-  ONNXTransposeOpLoweringToStableHlo(MLIRContext *ctx)
+struct ONNXTransposeOpLoweringToStablehlo : public ConversionPattern {
+  ONNXTransposeOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXTransposeOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -72,9 +72,9 @@ struct ONNXTransposeOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXTransposeOpToStableHloPattern(
+void populateLoweringONNXTransposeOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXTransposeOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXTransposeOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStableHlo/Tensor/Unsqueeze.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Unsqueeze.cpp
similarity index 84%
rename from src/Conversion/ONNXToStableHlo/Tensor/Unsqueeze.cpp
rename to src/Conversion/ONNXToStablehlo/Tensor/Unsqueeze.cpp
index bcc5297297..44a43460e4 100644
--- a/src/Conversion/ONNXToStableHlo/Tensor/Unsqueeze.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Unsqueeze.cpp
@@ -8,12 +8,12 @@
 //
 // =============================================================================
 //
-// This file lowers the ONNX Unsqueeze Operator to StableHlo dialect.
+// This file lowers the ONNX Unsqueeze Operator to Stablehlo dialect.
 //
 //===----------------------------------------------------------------------===//

-#include "src/Conversion/ONNXToStableHlo/DialectBuilder.hpp"
-#include "src/Conversion/ONNXToStableHlo/ONNXToStableHloCommon.hpp"
+#include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"
 #include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
 #include "src/Support/TypeUtilities.hpp"

@@ -23,9 +23,9 @@ namespace onnx_mlir {

 namespace {

-// ONNXUnsqueezeOp(A) is implemented using StableHlo reshapeOp
-struct ONNXUnsqueezeOpLoweringToStableHlo : public ConversionPattern {
-  ONNXUnsqueezeOpLoweringToStableHlo(MLIRContext *ctx)
+// ONNXUnsqueezeOp(A) is implemented using Stablehlo reshapeOp
+struct ONNXUnsqueezeOpLoweringToStablehlo : public ConversionPattern {
+  ONNXUnsqueezeOpLoweringToStablehlo(MLIRContext *ctx)
       : ConversionPattern(mlir::ONNXUnsqueezeOp::getOperationName(), 1, ctx) {}

   LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
@@ -42,7 +42,7 @@ struct ONNXUnsqueezeOpLoweringToStableHlo : public ConversionPattern {

     // Unused; for example, axles can be read from it.
     // Code below does not seems to handle >v11 where axles are inputs.
-    IndexExprBuilderForStableHlo createIE(rewriter, loc);
+    IndexExprBuilderForStablehlo createIE(rewriter, loc);
     ONNXUnsqueezeOpShapeHelper shapeHelper(op, operands, &createIE);
     shapeHelper.computeShapeAndAssertOnFailure();

@@ -88,9 +88,9 @@ struct ONNXUnsqueezeOpLoweringToStableHlo : public ConversionPattern {

 } // namespace

-void populateLoweringONNXUnsqueezeOpToStableHloPattern(
+void populateLoweringONNXUnsqueezeOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
-  patterns.insert<ONNXUnsqueezeOpLoweringToStableHlo>(ctx);
+  patterns.insert<ONNXUnsqueezeOpLoweringToStablehlo>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToTOSA/DialectBuilder.cpp b/src/Conversion/ONNXToTOSA/DialectBuilder.cpp
index 73f13bac54..8488be4b5e 100644
--- a/src/Conversion/ONNXToTOSA/DialectBuilder.cpp
+++ b/src/Conversion/ONNXToTOSA/DialectBuilder.cpp
@@ -9,7 +9,7 @@
 // =============================================================================
 //
 // This file contains the dialect build for the TOSA dialect. Uses the same
-// implementation as ONNXToStableHlo with minor differences.
+// implementation as ONNXToStablehlo with minor differences.
 //
 //===----------------------------------------------------------------------===//

diff --git a/src/Conversion/ONNXToTOSA/DialectBuilder.hpp b/src/Conversion/ONNXToTOSA/DialectBuilder.hpp
index a4e07c6575..3087dc5e28 100644
--- a/src/Conversion/ONNXToTOSA/DialectBuilder.hpp
+++ b/src/Conversion/ONNXToTOSA/DialectBuilder.hpp
@@ -9,7 +9,7 @@
 // =============================================================================
 //
 // This file contains the dialect build for the TOSA dialect. Uses the same
-// implementation as ONNXToStableHlo with minor differences.
+// implementation as ONNXToStablehlo with minor differences.
 //
 //===----------------------------------------------------------------------===//

diff --git a/src/Dialect/Mlir/IndexExprBuilder.hpp b/src/Dialect/Mlir/IndexExprBuilder.hpp
index b6aa478938..3d14303612 100644
--- a/src/Dialect/Mlir/IndexExprBuilder.hpp
+++ b/src/Dialect/Mlir/IndexExprBuilder.hpp
@@ -15,10 +15,10 @@
 // The IndexExprBuilder class has virtual functions that needs to be
 // instantiated by specialized sub-classes, defined to work in a specific
 // dialect. There are currently 3 sub-classes, one for ONNX, KRNL, and
-// StableHlo.
+// Stablehlo.
 //
 // Namely: IndexExprBuilderForAnalysis, IndexExprBuilderForKrnl, and
-// IndexExprBuilderForStableHlo
+// IndexExprBuilderForStablehlo
 //
 //===----------------------------------------------------------------------===//

@@ -63,8 +63,8 @@ namespace onnx_mlir {
   analysis phase; runtime values are described by questionmark index
   expressions.

-  Other subclasses (e.g. IndexExprBuilderForKrnl/IndexExprBuilderForStableHlo )
-  generate dialect operations (e.g. Krnl/StableHlo ops) to generate code that
+  Other subclasses (e.g. IndexExprBuilderForKrnl/IndexExprBuilderForStablehlo )
+  generate dialect operations (e.g. Krnl/Stablehlo ops) to generate code that
   compute runtime values.
 */

diff --git a/src/Interface/ShapeHelperOpInterface.hpp b/src/Interface/ShapeHelperOpInterface.hpp
index 5bc6d280c1..bbe3398da9 100644
--- a/src/Interface/ShapeHelperOpInterface.hpp
+++ b/src/Interface/ShapeHelperOpInterface.hpp
@@ -66,8 +66,8 @@ struct ONNXOpShapeHelper {
    for values unknown at compile time. Example of such subclasses are
    IndexExprBuilderForKrnl (generates Krnl ops, in
    src/Dialect/Krnl/DialectBuilder.hpp, ) or IndexExprBuilderForStableHhlo
-   (generates Shape/StableHlo ops, in
-   src/Conversion/ONNXToStableHlo/DialectBuilder.hpp).
+   (generates Shape/Stablehlo ops, in
+   src/Conversion/ONNXToStablehlo/DialectBuilder.hpp).

    @param scope Index expression scope to be used. If none is provided, a new
    scope is created and stored internally. This scope will then be destructed
diff --git a/src/Pass/Passes.hpp b/src/Pass/Passes.hpp
index 3f8906fa82..c2fd68a231 100644
--- a/src/Pass/Passes.hpp
+++ b/src/Pass/Passes.hpp
@@ -90,9 +90,9 @@ void configureOnnxToKrnlLoweringPass(bool reportOnParallel,
 std::unique_ptr<mlir::Pass> createProcessAffineParallelPrivatePass();

 #ifdef ONNX_MLIR_ENABLE_STABLEHLO
-/// Add pass for lowering to StableHlo IR.
-std::unique_ptr<mlir::Pass> createLowerToStableHloPass();
-std::unique_ptr<mlir::Pass> createLowerToStableHloPass(bool enableUnroll);
+/// Add pass for lowering to Stablehlo IR.
+std::unique_ptr<mlir::Pass> createLowerToStablehloPass();
+std::unique_ptr<mlir::Pass> createLowerToStablehloPass(bool enableUnroll);
 #endif

 /// Pass for lowering krnl.dim operations to standard dialect.
diff --git a/src/Tools/onnx-mlir-opt/RegisterPasses.cpp b/src/Tools/onnx-mlir-opt/RegisterPasses.cpp
index a38469c649..e8d05853e3 100644
--- a/src/Tools/onnx-mlir-opt/RegisterPasses.cpp
+++ b/src/Tools/onnx-mlir-opt/RegisterPasses.cpp
@@ -138,7 +138,7 @@ void registerOMPasses(int optLevel) {

 #ifdef ONNX_MLIR_ENABLE_STABLEHLO
   mlir::registerPass([]() -> std::unique_ptr<mlir::Pass> {
-    return createLowerToStableHloPass();
+    return createLowerToStablehloPass();
   });
 #endif
 }
diff --git a/src/Transform/ONNX/Decompose.cpp b/src/Transform/ONNX/Decompose.cpp
index eefc97c818..584a161643 100644
--- a/src/Transform/ONNX/Decompose.cpp
+++ b/src/Transform/ONNX/Decompose.cpp
@@ -522,7 +522,7 @@ struct SoftmaxPattern : public OpRewritePattern<ONNXSoftmaxOp> {
   }
 };

-void populateDecomposingONNXBeforeStableHloPatterns(
+void populateDecomposingONNXBeforeStablehloPatterns(
     RewritePatternSet &patterns, MLIRContext *ctx) {
   patterns.add<SoftmaxPattern>(ctx);
 }
@@ -975,7 +975,7 @@ void DecomposeONNXToONNXPass::runOnOperation() {

 #ifdef ONNX_MLIR_DECOMP_ONNX_CONVTRANSPOSE
 #ifdef ONNX_MLIR_ENABLE_STABLEHLO
-  // ONNXtoStableHlo pass has own rewriting for ConvTranspose Op using
+  // ONNXtoStablehlo pass has own rewriting for ConvTranspose Op using
   // stablehlo ops. To avoid conflict with it, decomposing for ConvTranspose
   // is disabled when the target is stablehlo.
   if (this->target != "stablehlo") {
@@ -993,7 +993,7 @@ void DecomposeONNXToONNXPass::runOnOperation() {
   onnx_mlir::getDecomposeONNXToONNXPatterns(patterns);
 #ifdef ONNX_MLIR_ENABLE_STABLEHLO
   if (this->target == "stablehlo") {
-    populateDecomposingONNXBeforeStableHloPatterns(patterns, context);
+    populateDecomposingONNXBeforeStablehloPatterns(patterns, context);
     target.addIllegalOp<ONNXSoftmaxOp>();
   }
 #endif

From 449cd7e99b86a53532cb16d2a8b4cdc50b82886f Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Thu, 11 Jan 2024 06:52:50 -0500
Subject: [PATCH 09/11] move foldorEmitOnnx* to OnnxBuilder, update license
 info

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 .../ONNXToKrnl/ONNXToKrnlCommon.cpp           | 100 ++-------
 .../ONNXToKrnl/ONNXToKrnlCommon.hpp           |  12 +-
 src/Conversion/ONNXToKrnl/RNN/GRU.cpp         |  72 ++++---
 src/Conversion/ONNXToKrnl/RNN/LSTM.cpp        |  72 ++++---
 src/Conversion/ONNXToKrnl/RNN/RNN.cpp         |  56 ++---
 .../ConvertONNXToStablehlo.cpp                |   5 +-
 .../ONNXToStablehlo/DialectBuilder.cpp        |   5 +-
 .../ONNXToStablehlo/DialectBuilder.hpp        |   5 +-
 src/Conversion/ONNXToStablehlo/Math/Clip.cpp  |   2 +-
 .../ONNXToStablehlo/Math/Elementwise.cpp      |   2 +-
 src/Conversion/ONNXToStablehlo/Math/Gemm.cpp  |   2 +-
 .../ONNXToStablehlo/Math/MatMul.cpp           |   2 +-
 .../ONNXToStablehlo/Math/Reduction.cpp        |   2 +-
 src/Conversion/ONNXToStablehlo/NN/Conv.cpp    |   2 +-
 .../ONNXToStablehlo/NN/ConvTranspose.cpp      |   2 +-
 .../ONNXToStablehlo/NN/Normalization.cpp      |   2 +-
 src/Conversion/ONNXToStablehlo/NN/Pooling.cpp |   2 +-
 .../ONNXToStablehlo/ONNXToStablehloCommon.cpp |  93 +-------
 .../ONNXToStablehlo/ONNXToStablehloCommon.hpp |   5 +-
 src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp   |   2 +-
 .../ONNXToStablehlo/RNN/RNNBase.cpp           |   2 +-
 .../ONNXToStablehlo/RNN/RNNBase.hpp           |   2 +-
 .../ONNXToStablehlo/Tensor/ArgMax.cpp         |   2 +-
 .../ONNXToStablehlo/Tensor/Concat.cpp         |   2 +-
 .../ONNXToStablehlo/Tensor/Constant.cpp       |   2 +-
 .../ONNXToStablehlo/Tensor/DepthToSpace.cpp   |   2 +-
 .../ONNXToStablehlo/Tensor/Expand.cpp         |   2 +-
 .../ONNXToStablehlo/Tensor/Flatten.cpp        |   2 +-
 .../ONNXToStablehlo/Tensor/Gather.cpp         |   2 +-
 .../ONNXToStablehlo/Tensor/GatherElements.cpp |   2 +-
 .../ONNXToStablehlo/Tensor/Identity.cpp       |   2 +-
 .../ONNXToStablehlo/Tensor/OneHot.cpp         |   2 +-
 src/Conversion/ONNXToStablehlo/Tensor/Pad.cpp |   2 +-
 .../ONNXToStablehlo/Tensor/Reshape.cpp        |   2 +-
 .../ONNXToStablehlo/Tensor/ScatterND.cpp      |   2 +-
 .../ONNXToStablehlo/Tensor/Shape.cpp          |   2 +-
 .../ONNXToStablehlo/Tensor/Slice.cpp          |   2 +-
 .../ONNXToStablehlo/Tensor/Split.cpp          |   2 +-
 .../ONNXToStablehlo/Tensor/Squeeze.cpp        |   2 +-
 .../ONNXToStablehlo/Tensor/Tile.cpp           |   2 +-
 .../ONNXToStablehlo/Tensor/Transpose.cpp      |   2 +-
 .../ONNXToStablehlo/Tensor/Unsqueeze.cpp      |   2 +-
 src/Dialect/ONNX/DialectBuilder.cpp           | 198 ++++++++++++++++++
 src/Dialect/ONNX/DialectBuilder.hpp           |  66 +++++-
 44 files changed, 445 insertions(+), 308 deletions(-)

diff --git a/src/Conversion/ONNXToKrnl/ONNXToKrnlCommon.cpp b/src/Conversion/ONNXToKrnl/ONNXToKrnlCommon.cpp
index 429c45c6c8..1fb5cf3441 100644
--- a/src/Conversion/ONNXToKrnl/ONNXToKrnlCommon.cpp
+++ b/src/Conversion/ONNXToKrnl/ONNXToKrnlCommon.cpp
@@ -286,112 +286,44 @@ DenseElementsAttr getDenseElementAttrFromConstValue(mlir::Value value) {

 /// Emit an ONNXSqueezeV11Op. If the input is constant, do const propagation,
 /// and return a constant.
-Value foldOrEmitONNXSqueezeV11Op(ConversionPatternRewriter &rewriter,
+Value foldOrEmitONNXSqueezeV11OpKrnl(ConversionPatternRewriter &rewriter,
     Location loc, Type resultType, Value input, int64_t axis) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
-  TensorType tensorType = create.onnx.toTensor(resultType);
-  if (DenseElementsAttr inputElements =
-          getDenseElementAttrFromConstValue(input)) {
-    DenseElementsAttr squeezedElements = inputElements.reshape(tensorType);
-    Value constVal = create.onnx.constant(squeezedElements);
-    return create.onnx.toMemref(constVal);
-  } else {
-    return create.onnx.toMemref(
-        rewriter
-            .create<ONNXSqueezeV11Op>(loc, tensorType,
-                create.onnx.toTensor(input), rewriter.getI64ArrayAttr(axis))
-            .getResult());
-  }
+  return create.onnx.toMemref(create.onnx.foldOrEmitONNXSqueezeV11Op(rewriter,
+      loc, resultType, input, axis, getDenseElementAttrFromConstValue));
 }

 /// Emit an ONNXUnsqueezeV11Op. If the input is constant, do const
 /// propagation, and return a constant.
-Value foldOrEmitONNXUnsqueezeV11Op(ConversionPatternRewriter &rewriter,
+Value foldOrEmitONNXUnsqueezeV11OpKrnl(ConversionPatternRewriter &rewriter,
     Location loc, Type resultType, Value input, int64_t axis) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
-  TensorType tensorType = create.onnx.toTensor(resultType);
-  if (DenseElementsAttr inputElements =
-          getDenseElementAttrFromConstValue(input)) {
-    DenseElementsAttr unsqueezedElements = inputElements.reshape(tensorType);
-    Value constVal = create.onnx.constant(unsqueezedElements);
-    return create.onnx.toMemref(constVal);
-  } else {
-    return create.onnx.toMemref(
-        rewriter
-            .create<ONNXUnsqueezeV11Op>(loc, tensorType,
-                create.onnx.toTensor(input), rewriter.getI64ArrayAttr(axis))
-            .getResult());
-  }
+  return create.onnx.toMemref(create.onnx.foldOrEmitONNXUnsqueezeV11Op(rewriter,
+      loc, resultType, input, axis, getDenseElementAttrFromConstValue));
 }

 /// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
 /// return constants.
 /// Only support evenly splitting.
-std::vector<Value> foldOrEmitONNXSplitOp(ConversionPatternRewriter &rewriter,
-    Location loc, ArrayRef<Type> resultTypes, Value input, int64_t axis) {
-
+std::vector<Value> foldOrEmitONNXSplitV11OpKrnl(
+    ConversionPatternRewriter &rewriter, Location loc,
+    ArrayRef<Type> resultTypes, Value input, int64_t axis) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
-
+  std::vector<Value> slices = create.onnx.foldOrEmitONNXSplitV11Op(rewriter,
+      loc, resultTypes, input, axis, getDenseElementAttrFromConstValue);
   std::vector<Value> resVals;
-  int outputNum = resultTypes.size();
-
-  if (DenseElementsAttr inputElements =
-          getDenseElementAttrFromConstValue(input)) {
-    auto inputShape = inputElements.getType().getShape();
-    assert(outputNum == 0 || inputShape[axis] % outputNum == 0);
-    int64_t sizeOfEachSplit = outputNum != 0 ? inputShape[axis] / outputNum : 0;
-    SmallVector<int64_t, 4> sizes(outputNum, sizeOfEachSplit);
-
-    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
-    std::vector<ElementsAttr> splits =
-        elementsBuilder.split(inputElements, axis, sizes);
-    for (ElementsAttr splitElements : splits) {
-      // Avoid DisposableElementsAttr during conversion.
-      DenseElementsAttr denseSplitElements =
-          elementsBuilder.toDenseElementsAttr(splitElements);
-      Value constVal = create.onnx.constant(denseSplitElements);
-      resVals.emplace_back(create.onnx.toMemref(constVal));
-    }
-  } else {
-    SmallVector<Type, 4> convertedTypes;
-    for (auto t : resultTypes) {
-      convertedTypes.emplace_back(create.onnx.toTensor(t));
-    }
-    ONNXSplitV11Op split = rewriter.create<ONNXSplitV11Op>(loc, convertedTypes,
-        create.onnx.toTensor(input),
-        /*axis=*/axis, nullptr);
-    for (int i = 0; i < outputNum; ++i)
-      resVals.emplace_back(create.onnx.toMemref(split.getOutputs()[i]));
-  }
+  for (Value slice : slices)
+    resVals.emplace_back(create.onnx.toMemref(slice));
   return resVals;
 }

 /// Emit an ONNXTransposeOp. If the input is constant, do const propagation,
 /// and return a constant.
-Value foldOrEmitONNXTransposeOp(ConversionPatternRewriter &rewriter,
+Value foldOrEmitONNXTransposeOpKrnl(ConversionPatternRewriter &rewriter,
     Location loc, Type resultType, Value input, ArrayAttr permAttr) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
-  if (DenseElementsAttr inputElements =
-          getDenseElementAttrFromConstValue(input)) {
-    SmallVector<uint64_t, 4> perm;
-    for (auto permVal : permAttr.getValue())
-      perm.emplace_back(permVal.cast<IntegerAttr>().getInt());
-
-    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
-    ElementsAttr transposedElements =
-        elementsBuilder.transpose(inputElements, perm);
-    // Avoid DisposableElementsAttr during conversion.
-    DenseElementsAttr denseTransposedElements =
-        elementsBuilder.toDenseElementsAttr(transposedElements);
-    Value constVal = create.onnx.constant(denseTransposedElements);
-    return create.onnx.toMemref(constVal);
-  } else {
-    return create.onnx.toMemref(
-        rewriter
-            .create<ONNXTransposeOp>(loc, create.onnx.toTensor(resultType),
-                create.onnx.toTensor(input), permAttr)
-            .getResult());
-  }
+  return create.onnx.toMemref(create.onnx.foldOrEmitONNXTransposeOp(rewriter,
+      loc, resultType, input, permAttr, getDenseElementAttrFromConstValue));
 }

 /// Emit MemRef ReinterpretCastOp to create a new view for 'data'.
diff --git a/src/Conversion/ONNXToKrnl/ONNXToKrnlCommon.hpp b/src/Conversion/ONNXToKrnl/ONNXToKrnlCommon.hpp
index 5ea14039d9..0ebc605466 100644
--- a/src/Conversion/ONNXToKrnl/ONNXToKrnlCommon.hpp
+++ b/src/Conversion/ONNXToKrnl/ONNXToKrnlCommon.hpp
@@ -133,28 +133,28 @@ bool checkOpToCall(mlir::Operation *op, std::string opsForCall);

 /// Emit an ONNXSqueezeOp. If the input is constant, do const propagation, and
 /// return a constant.
-mlir::Value foldOrEmitONNXSqueezeV11Op(
+mlir::Value foldOrEmitONNXSqueezeV11OpKrnl(
     mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
     mlir::Type resultType, mlir::Value input, int64_t axis);

 /// Emit an ONNXUnsqueezeOp. If the input is constant, do const propagation, and
 /// return a constant.
-mlir::Value foldOrEmitONNXUnsqueezeV11Op(
+mlir::Value foldOrEmitONNXUnsqueezeV11OpKrnl(
     mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
     mlir::Type resultType, mlir::Value input, int64_t axis);

 /// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
 /// return constants.
 /// Only support evenly splitting.
-std::vector<mlir::Value> foldOrEmitONNXSplitOp(
+std::vector<mlir::Value> foldOrEmitONNXSplitV11OpKrnl(
     mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
     llvm::ArrayRef<mlir::Type> resultTypes, mlir::Value input, int64_t axis);

 /// Emit an ONNXTransposeOp. If the input is constant, do const propagation, and
 /// return a constant.
-mlir::Value foldOrEmitONNXTransposeOp(mlir::ConversionPatternRewriter &rewriter,
-    mlir::Location loc, mlir::Type resultType, mlir::Value input,
-    mlir::ArrayAttr permAttr);
+mlir::Value foldOrEmitONNXTransposeOpKrnl(
+    mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+    mlir::Type resultType, mlir::Value input, mlir::ArrayAttr permAttr);

 /// Emit MemRef ReinterpretCastOp to create a new view for 'data'.
 /// The new view is created using the given 'outputDims'.
diff --git a/src/Conversion/ONNXToKrnl/RNN/GRU.cpp b/src/Conversion/ONNXToKrnl/RNN/GRU.cpp
index 2818bbcc5f..c6000a6b87 100644
--- a/src/Conversion/ONNXToKrnl/RNN/GRU.cpp
+++ b/src/Conversion/ONNXToKrnl/RNN/GRU.cpp
@@ -207,60 +207,64 @@ getWeightPack<ONNXGRUOp, GruWeightPack>(
   // Squeeze the direction axis from W and R.
   Value fW, bW, fR, bR;
   if (direction == FORWARD) {
-    fW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, W, /*axis=*/0);
-    fR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, R, /*axis=*/0);
+    fW = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, w2DTy, W, /*axis=*/0);
+    fR = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, r2DTy, R, /*axis=*/0);
   } else if (direction == REVERSE) {
-    bW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, W, /*axis=*/0);
-    bR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, R, /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, w2DTy, W, /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, r2DTy, R, /*axis=*/0);
   } else { // BIDIRECTIONAL
     // W
     std::vector<Value> vals =
-        foldOrEmitONNXSplitOp(rewriter, loc, w3D2Ty, W, 0);
-    fW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, vals[0], /*axis=*/0);
-    bW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, vals[1], /*axis=*/0);
+        foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, w3D2Ty, W, 0);
+    fW = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, w2DTy, vals[0], /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, w2DTy, vals[1], /*axis=*/0);
     // R
     vals.clear();
-    vals = foldOrEmitONNXSplitOp(rewriter, loc, r3D2Ty, R, 0);
-    fR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, vals[0], /*axis=*/0);
-    bR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, vals[1], /*axis=*/0);
+    vals = foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, r3D2Ty, R, 0);
+    fR = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, r2DTy, vals[0], /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, r2DTy, vals[1], /*axis=*/0);
   }

   // Split W and R into individual weight tensors, and transpose them.
   if (direction == FORWARD || direction == BIDIRECTIONAL) {
     // W
-    weightForward.WT =
-        foldOrEmitONNXTransposeOp(rewriter, loc, wTransposeTy, fW, permAttr);
+    weightForward.WT = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, wTransposeTy, fW, permAttr);
     // R
     if (linearBeforeReset) {
-      weightForward.RT =
-          foldOrEmitONNXTransposeOp(rewriter, loc, rTransposeTy, fR, permAttr);
+      weightForward.RT = foldOrEmitONNXTransposeOpKrnl(
+          rewriter, loc, rTransposeTy, fR, permAttr);
     } else {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, rSplit2D3Ty, fR, 0);
-      weightForward.Rz = foldOrEmitONNXTransposeOp(
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, rSplit2D3Ty, fR, 0);
+      weightForward.Rz = foldOrEmitONNXTransposeOpKrnl(
           rewriter, loc, rTranspose2DTy, vals[0], permAttr);
-      weightForward.Rr = foldOrEmitONNXTransposeOp(
+      weightForward.Rr = foldOrEmitONNXTransposeOpKrnl(
           rewriter, loc, rTranspose2DTy, vals[1], permAttr);
-      weightForward.Rh = foldOrEmitONNXTransposeOp(
+      weightForward.Rh = foldOrEmitONNXTransposeOpKrnl(
           rewriter, loc, rTranspose2DTy, vals[2], permAttr);
     }
   }
   if (direction == REVERSE || direction == BIDIRECTIONAL) {
     // W
-    weightReverse.WT =
-        foldOrEmitONNXTransposeOp(rewriter, loc, wTransposeTy, bW, permAttr);
+    weightReverse.WT = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, wTransposeTy, bW, permAttr);
     // R
     if (linearBeforeReset) {
-      weightReverse.RT =
-          foldOrEmitONNXTransposeOp(rewriter, loc, rTransposeTy, bR, permAttr);
+      weightReverse.RT = foldOrEmitONNXTransposeOpKrnl(
+          rewriter, loc, rTransposeTy, bR, permAttr);
     } else {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, rSplit2D3Ty, bR, 0);
-      weightReverse.Rz = foldOrEmitONNXTransposeOp(
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, rSplit2D3Ty, bR, 0);
+      weightReverse.Rz = foldOrEmitONNXTransposeOpKrnl(
           rewriter, loc, rTranspose2DTy, vals[0], permAttr);
-      weightReverse.Rr = foldOrEmitONNXTransposeOp(
+      weightReverse.Rr = foldOrEmitONNXTransposeOpKrnl(
           rewriter, loc, rTranspose2DTy, vals[1], permAttr);
-      weightReverse.Rh = foldOrEmitONNXTransposeOp(
+      weightReverse.Rh = foldOrEmitONNXTransposeOpKrnl(
           rewriter, loc, rTranspose2DTy, vals[2], permAttr);
     }
   }
@@ -297,22 +301,24 @@ std::tuple<GruBiasPack, GruBiasPack> getBiasPack<ONNXGRUOp, GruBiasPack>(
     // Squeeze the direction axis from B.
     Value fB, bB;
     if (direction == FORWARD) {
-      fB = foldOrEmitONNXSqueezeV11Op(rewriter, loc, bType1D, B, /*axis=*/0);
+      fB =
+          foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, bType1D, B, /*axis=*/0);
     } else if (direction == REVERSE) {
-      bB = foldOrEmitONNXSqueezeV11Op(rewriter, loc, bType1D, B, /*axis=*/0);
+      bB =
+          foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, bType1D, B, /*axis=*/0);
     } else { // BIDIRECTIONAL
       std::vector<Value> vals;
-      vals = foldOrEmitONNXSplitOp(rewriter, loc, split2D2Ty, B, 0);
-      fB = foldOrEmitONNXSqueezeV11Op(
+      vals = foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split2D2Ty, B, 0);
+      fB = foldOrEmitONNXSqueezeV11OpKrnl(
           rewriter, loc, bType1D, vals[0], /*axis=*/0);
-      bB = foldOrEmitONNXSqueezeV11Op(
+      bB = foldOrEmitONNXSqueezeV11OpKrnl(
           rewriter, loc, bType1D, vals[1], /*axis=*/0);
     }

     // Split B into individual bias tensors.
     if (direction == FORWARD || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, split1D6Ty, fB, 0);
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split1D6Ty, fB, 0);
       biasForward.Wbz = vals[0];
       biasForward.Wbr = vals[1];
       biasForward.Wbh = vals[2];
@@ -323,7 +329,7 @@ std::tuple<GruBiasPack, GruBiasPack> getBiasPack<ONNXGRUOp, GruBiasPack>(
     }
     if (direction == REVERSE || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, split1D6Ty, bB, 0);
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split1D6Ty, bB, 0);
       biasReverse.Wbz = vals[0];
       biasReverse.Wbr = vals[1];
       biasReverse.Wbh = vals[2];
diff --git a/src/Conversion/ONNXToKrnl/RNN/LSTM.cpp b/src/Conversion/ONNXToKrnl/RNN/LSTM.cpp
index 72d7d685b6..f1855c829a 100644
--- a/src/Conversion/ONNXToKrnl/RNN/LSTM.cpp
+++ b/src/Conversion/ONNXToKrnl/RNN/LSTM.cpp
@@ -227,41 +227,45 @@ getWeightPack<ONNXLSTMOp, LstmWeightPack>(
   // Squeeze the direction axis from W and R.
   Value fW, bW, fR, bR;
   if (direction == FORWARD) {
-    fW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, W, /*axis=*/0);
-    fR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, R, /*axis=*/0);
+    fW = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, w2DTy, W, /*axis=*/0);
+    fR = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, r2DTy, R, /*axis=*/0);
   } else if (direction == REVERSE) {
-    bW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, W, /*axis=*/0);
-    bR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, R, /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, w2DTy, W, /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, r2DTy, R, /*axis=*/0);
   } else { // BIDIRECTIONAL
     // W
     std::vector<Value> vals =
-        foldOrEmitONNXSplitOp(rewriter, loc, w3D2Ty, W, 0);
-    fW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, vals[0], /*axis=*/0);
-    bW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, vals[1], /*axis=*/0);
+        foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, w3D2Ty, W, 0);
+    fW = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, w2DTy, vals[0], /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, w2DTy, vals[1], /*axis=*/0);
     // R
     vals.clear();
-    vals = foldOrEmitONNXSplitOp(rewriter, loc, r3D2Ty, R, 0);
-    fR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, vals[0], /*axis=*/0);
-    bR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, vals[1], /*axis=*/0);
+    vals = foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, r3D2Ty, R, 0);
+    fR = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, r2DTy, vals[0], /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, r2DTy, vals[1], /*axis=*/0);
   }

   // Transpose W and R.
   ArrayAttr permAttr = rewriter.getI64ArrayAttr({1, 0});
   if (direction == FORWARD || direction == BIDIRECTIONAL) {
     // W
-    weightForward.WT =
-        foldOrEmitONNXTransposeOp(rewriter, loc, wTranspose2DTy, fW, permAttr);
+    weightForward.WT = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, wTranspose2DTy, fW, permAttr);
     // R
-    weightForward.RT =
-        foldOrEmitONNXTransposeOp(rewriter, loc, rTranspose2DTy, fR, permAttr);
+    weightForward.RT = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, rTranspose2DTy, fR, permAttr);
   }
   if (direction == REVERSE || direction == BIDIRECTIONAL) {
     // W
-    weightReverse.WT =
-        foldOrEmitONNXTransposeOp(rewriter, loc, wTranspose2DTy, bW, permAttr);
+    weightReverse.WT = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, wTranspose2DTy, bW, permAttr);
     // R
-    weightReverse.RT =
-        foldOrEmitONNXTransposeOp(rewriter, loc, rTranspose2DTy, bR, permAttr);
+    weightReverse.RT = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, rTranspose2DTy, bR, permAttr);
   }
   return std::make_tuple(weightForward, weightReverse);
 }
@@ -296,22 +300,24 @@ std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
     // Squeeze the direction axis from B.
     Value fB, bB;
     if (direction == FORWARD) {
-      fB = foldOrEmitONNXSqueezeV11Op(rewriter, loc, bType1D, B, /*axis=*/0);
+      fB =
+          foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, bType1D, B, /*axis=*/0);
     } else if (direction == REVERSE) {
-      bB = foldOrEmitONNXSqueezeV11Op(rewriter, loc, bType1D, B, /*axis=*/0);
+      bB =
+          foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, bType1D, B, /*axis=*/0);
     } else { // BIDIRECTIONAL
       std::vector<Value> vals;
-      vals = foldOrEmitONNXSplitOp(rewriter, loc, split2D2Ty, B, 0);
-      fB = foldOrEmitONNXSqueezeV11Op(
+      vals = foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split2D2Ty, B, 0);
+      fB = foldOrEmitONNXSqueezeV11OpKrnl(
           rewriter, loc, bType1D, vals[0], /*axis=*/0);
-      bB = foldOrEmitONNXSqueezeV11Op(
+      bB = foldOrEmitONNXSqueezeV11OpKrnl(
           rewriter, loc, bType1D, vals[1], /*axis=*/0);
     }

     // Split B into individual bias tensors.
     if (direction == FORWARD || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, split1D8Ty, fB, 0);
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split1D8Ty, fB, 0);
       biasForward.Wbi = vals[0];
       biasForward.Wbo = vals[1];
       biasForward.Wbf = vals[2];
@@ -324,7 +330,7 @@ std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
     }
     if (direction == REVERSE || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, split1D8Ty, bB, 0);
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split1D8Ty, bB, 0);
       biasReverse.Wbi = vals[0];
       biasReverse.Wbo = vals[1];
       biasReverse.Wbf = vals[2];
@@ -353,22 +359,24 @@ std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
     // Squeeze the direction axis from P.
     Value fP, bP;
     if (direction == FORWARD) {
-      fP = foldOrEmitONNXSqueezeV11Op(rewriter, loc, pType1D, P, /*axis=*/0);
+      fP =
+          foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, pType1D, P, /*axis=*/0);
     } else if (direction == REVERSE) {
-      bP = foldOrEmitONNXSqueezeV11Op(rewriter, loc, pType1D, P, /*axis=*/0);
+      bP =
+          foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, pType1D, P, /*axis=*/0);
     } else { // BIDIRECTIONAL
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, split2D2Ty, P, 0);
-      fP = foldOrEmitONNXSqueezeV11Op(
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split2D2Ty, P, 0);
+      fP = foldOrEmitONNXSqueezeV11OpKrnl(
           rewriter, loc, pType1D, vals[0], /*axis=*/0);
-      bP = foldOrEmitONNXSqueezeV11Op(
+      bP = foldOrEmitONNXSqueezeV11OpKrnl(
           rewriter, loc, pType1D, vals[1], /*axis=*/0);
     }

     // Split P into individual tensors.
     if (direction == FORWARD || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, split1D3Ty, fP, 0);
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split1D3Ty, fP, 0);
       biasForward.Pi = vals[0];
       biasForward.Po = vals[1];
       biasForward.Pf = vals[2];
@@ -376,7 +384,7 @@ std::tuple<LstmBiasPack, LstmBiasPack> getBiasPack<ONNXLSTMOp, LstmBiasPack>(
     }
     if (direction == REVERSE || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, split1D3Ty, bP, 0);
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split1D3Ty, bP, 0);
       biasReverse.Pi = vals[0];
       biasReverse.Po = vals[1];
       biasReverse.Pf = vals[2];
diff --git a/src/Conversion/ONNXToKrnl/RNN/RNN.cpp b/src/Conversion/ONNXToKrnl/RNN/RNN.cpp
index bbe3940431..1c43a064e1 100644
--- a/src/Conversion/ONNXToKrnl/RNN/RNN.cpp
+++ b/src/Conversion/ONNXToKrnl/RNN/RNN.cpp
@@ -162,36 +162,40 @@ getWeightPack<ONNXRNNOp, RnnWeightPack>(
   // Unsqueeze the direction axis from W and R.
   Value fW, bW, fR, bR;
   if (direction == FORWARD) {
-    fW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, W, /*axis=*/0);
-    fR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, R, /*axis=*/0);
+    fW = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, w2DTy, W, /*axis=*/0);
+    fR = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, r2DTy, R, /*axis=*/0);
   } else if (direction == REVERSE) {
-    bW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, W, /*axis=*/0);
-    bR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, R, /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, w2DTy, W, /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, r2DTy, R, /*axis=*/0);
   } else { // BIDIRECTIONAL
     // W
     std::vector<Value> vals =
-        foldOrEmitONNXSplitOp(rewriter, loc, w3D2Ty, W, 0);
-    fW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, vals[0], /*axis=*/0);
-    bW = foldOrEmitONNXSqueezeV11Op(rewriter, loc, w2DTy, vals[1], /*axis=*/0);
+        foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, w3D2Ty, W, 0);
+    fW = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, w2DTy, vals[0], /*axis=*/0);
+    bW = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, w2DTy, vals[1], /*axis=*/0);
     // R
     vals.clear();
-    vals = foldOrEmitONNXSplitOp(rewriter, loc, r3D2Ty, R, 0);
-    fR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, vals[0], /*axis=*/0);
-    bR = foldOrEmitONNXSqueezeV11Op(rewriter, loc, r2DTy, vals[1], /*axis=*/0);
+    vals = foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, r3D2Ty, R, 0);
+    fR = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, r2DTy, vals[0], /*axis=*/0);
+    bR = foldOrEmitONNXSqueezeV11OpKrnl(
+        rewriter, loc, r2DTy, vals[1], /*axis=*/0);
   }

   // Split W and R into individual weight tensors, and transpose them.
   if (direction == FORWARD || direction == BIDIRECTIONAL) {
-    weightForward.Wi =
-        foldOrEmitONNXTransposeOp(rewriter, loc, wTranspose2DTy, fW, permAttr);
-    weightForward.Ri =
-        foldOrEmitONNXTransposeOp(rewriter, loc, rTranspose2DTy, fR, permAttr);
+    weightForward.Wi = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, wTranspose2DTy, fW, permAttr);
+    weightForward.Ri = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, rTranspose2DTy, fR, permAttr);
   }
   if (direction == REVERSE || direction == BIDIRECTIONAL) {
-    weightReverse.Wi =
-        foldOrEmitONNXTransposeOp(rewriter, loc, wTranspose2DTy, bW, permAttr);
-    weightReverse.Ri =
-        foldOrEmitONNXTransposeOp(rewriter, loc, rTranspose2DTy, bR, permAttr);
+    weightReverse.Wi = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, wTranspose2DTy, bW, permAttr);
+    weightReverse.Ri = foldOrEmitONNXTransposeOpKrnl(
+        rewriter, loc, rTranspose2DTy, bR, permAttr);
   }
   return std::make_tuple(weightForward, weightReverse);
 }
@@ -224,29 +228,31 @@ std::tuple<RnnBiasPack, RnnBiasPack> getBiasPack<ONNXRNNOp, RnnBiasPack>(
     // Unsqueeze the direction axis from B.
     Value fB, bB;
     if (direction == FORWARD) {
-      fB = foldOrEmitONNXSqueezeV11Op(rewriter, loc, bType1D, B, /*axis=*/0);
+      fB =
+          foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, bType1D, B, /*axis=*/0);
     } else if (direction == REVERSE) {
-      bB = foldOrEmitONNXSqueezeV11Op(rewriter, loc, bType1D, B, /*axis=*/0);
+      bB =
+          foldOrEmitONNXSqueezeV11OpKrnl(rewriter, loc, bType1D, B, /*axis=*/0);
     } else { // BIDIRECTIONAL
       std::vector<Value> vals;
-      vals = foldOrEmitONNXSplitOp(rewriter, loc, split2D2Ty, B, 0);
-      fB = foldOrEmitONNXSqueezeV11Op(
+      vals = foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split2D2Ty, B, 0);
+      fB = foldOrEmitONNXSqueezeV11OpKrnl(
           rewriter, loc, bType1D, vals[0], /*axis=*/0);
-      bB = foldOrEmitONNXSqueezeV11Op(
+      bB = foldOrEmitONNXSqueezeV11OpKrnl(
           rewriter, loc, bType1D, vals[1], /*axis=*/0);
     }

     // Split B into individual bias tensors.
     if (direction == FORWARD || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, split1D2Ty, fB, 0);
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split1D2Ty, fB, 0);
       biasForward.Wbi = vals[0];
       biasForward.Rbi = vals[1];
       biasForward.hasBias = true;
     }
     if (direction == REVERSE || direction == BIDIRECTIONAL) {
       std::vector<Value> vals =
-          foldOrEmitONNXSplitOp(rewriter, loc, split1D2Ty, bB, 0);
+          foldOrEmitONNXSplitV11OpKrnl(rewriter, loc, split1D2Ty, bB, 0);
       biasReverse.Wbi = vals[0];
       biasReverse.Rbi = vals[1];
       biasReverse.hasBias = true;
diff --git a/src/Conversion/ONNXToStablehlo/ConvertONNXToStablehlo.cpp b/src/Conversion/ONNXToStablehlo/ConvertONNXToStablehlo.cpp
index 25b34ffeba..74ea09a3dc 100644
--- a/src/Conversion/ONNXToStablehlo/ConvertONNXToStablehlo.cpp
+++ b/src/Conversion/ONNXToStablehlo/ConvertONNXToStablehlo.cpp
@@ -2,10 +2,9 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====------ ConvertONNXToStablehlo.cpp - ONNX dialects to Stablehlo lowering
-//-------===//
+//====- ConvertONNXToStablehlo.cpp - ONNX dialects to Stablehlo lowering --===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/DialectBuilder.cpp b/src/Conversion/ONNXToStablehlo/DialectBuilder.cpp
index e85c2c1643..85f1ec69ae 100644
--- a/src/Conversion/ONNXToStablehlo/DialectBuilder.cpp
+++ b/src/Conversion/ONNXToStablehlo/DialectBuilder.cpp
@@ -2,10 +2,9 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====------ DialectBuilder.cpp - Stablehlo dialect builder
-//--------------------===//
+//====---------- DialectBuilder.cpp - Stablehlo dialect builder -----------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/DialectBuilder.hpp b/src/Conversion/ONNXToStablehlo/DialectBuilder.hpp
index 0c32b6ea63..65e8d55675 100644
--- a/src/Conversion/ONNXToStablehlo/DialectBuilder.hpp
+++ b/src/Conversion/ONNXToStablehlo/DialectBuilder.hpp
@@ -2,10 +2,9 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====------ DialectBuilder.hpp - Stablehlo dialect builder
-//--------------------===//
+//====---------- DialectBuilder.hpp - Stablehlo dialect builder -----------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Math/Clip.cpp b/src/Conversion/ONNXToStablehlo/Math/Clip.cpp
index f36a5bce9c..b98360105e 100644
--- a/src/Conversion/ONNXToStablehlo/Math/Clip.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/Clip.cpp
@@ -4,7 +4,7 @@

 //===----------------- Clip.cpp - Lowering Clip Op ------------------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Math/Elementwise.cpp b/src/Conversion/ONNXToStablehlo/Math/Elementwise.cpp
index 71eaac6c9d..b5b58f2bd6 100644
--- a/src/Conversion/ONNXToStablehlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/Elementwise.cpp
@@ -4,7 +4,7 @@

 //===---------------- Elementwise.cpp - Elementwise Ops -------------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Math/Gemm.cpp b/src/Conversion/ONNXToStablehlo/Math/Gemm.cpp
index b2128bbd5e..075125d86d 100644
--- a/src/Conversion/ONNXToStablehlo/Math/Gemm.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/Gemm.cpp
@@ -4,7 +4,7 @@

 //===----------------- Gemm.cpp - Lowering Gemm Op ------------------------===//
 //
-// Copyright 2022-2023
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Math/MatMul.cpp b/src/Conversion/ONNXToStablehlo/Math/MatMul.cpp
index 8365152c35..667860b721 100644
--- a/src/Conversion/ONNXToStablehlo/Math/MatMul.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/MatMul.cpp
@@ -4,7 +4,7 @@

 //===----------------- Matmul.cpp - Lowering Matmul Op --------------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Math/Reduction.cpp b/src/Conversion/ONNXToStablehlo/Math/Reduction.cpp
index b2e5a5bd43..9e67d50285 100644
--- a/src/Conversion/ONNXToStablehlo/Math/Reduction.cpp
+++ b/src/Conversion/ONNXToStablehlo/Math/Reduction.cpp
@@ -4,7 +4,7 @@

 //===-------------- Reduction.cpp - Lowering Reduction Ops ----------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/NN/Conv.cpp b/src/Conversion/ONNXToStablehlo/NN/Conv.cpp
index ff06ba5375..c3b966db29 100644
--- a/src/Conversion/ONNXToStablehlo/NN/Conv.cpp
+++ b/src/Conversion/ONNXToStablehlo/NN/Conv.cpp
@@ -4,7 +4,7 @@

 //===----------- Normalization.cpp - Lowering Normalization Ops -----------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/NN/ConvTranspose.cpp b/src/Conversion/ONNXToStablehlo/NN/ConvTranspose.cpp
index cf8fe9bd53..fbc8b7446a 100644
--- a/src/Conversion/ONNXToStablehlo/NN/ConvTranspose.cpp
+++ b/src/Conversion/ONNXToStablehlo/NN/ConvTranspose.cpp
@@ -4,7 +4,7 @@

 //===----------- ConvTranspose.cpp - Lowering ConvTranspose Op ------------===//
 //
-// Copyright 2022-2023
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/NN/Normalization.cpp b/src/Conversion/ONNXToStablehlo/NN/Normalization.cpp
index 59ccbf20ec..6fc9a3023a 100644
--- a/src/Conversion/ONNXToStablehlo/NN/Normalization.cpp
+++ b/src/Conversion/ONNXToStablehlo/NN/Normalization.cpp
@@ -4,7 +4,7 @@

 //===----------- Normalization.cpp - Lowering Normalization Ops -----------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp b/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp
index abbcc62c56..3dc78f8861 100644
--- a/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp
+++ b/src/Conversion/ONNXToStablehlo/NN/Pooling.cpp
@@ -4,7 +4,7 @@

 //===---------------- Pooling.cpp - Lowering Pooling Ops ------------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.cpp b/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.cpp
index 4a5e09bf49..3123d413b8 100644
--- a/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.cpp
+++ b/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.cpp
@@ -2,10 +2,9 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====----- ONNXToStablehloCommon.cpp - ONNX dialects to Stablehlo lowering
-//---------===//
+//====-- ONNXToStablehloCommon.cpp - ONNX dialects to Stablehlo lowering--===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
@@ -131,23 +130,13 @@ DenseElementsAttr getDenseElementAttrFromConstValue(mlir::Value value) {
 }
 } // namespace

-// Emit an ONNXSqueezeOp. If the input is constant, do const propagation,
+/// Emit an ONNXSqueezeOp. If the input is constant, do const propagation,
 /// and return a constant.
 Value foldOrEmitONNXSqueezeOpStablehlo(ConversionPatternRewriter &rewriter,
     Location loc, Type resultType, Value input, int64_t axis) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
-  TensorType tensorType = create.onnx.toTensor(resultType);
-  if (DenseElementsAttr inputElements =
-          getDenseElementAttrFromConstValue(input)) {
-    DenseElementsAttr squeezedElements = inputElements.reshape(tensorType);
-    Value constVal = create.onnx.constant(squeezedElements);
-    return constVal;
-  } else {
-    return rewriter
-        .create<ONNXSqueezeOp>(loc, tensorType, create.onnx.toTensor(input),
-            create.onnx.constantInt64({axis}))
-        .getResult();
-  }
+  return create.onnx.foldOrEmitONNXSqueezeOp(rewriter, loc, resultType, input,
+      axis, getDenseElementAttrFromConstValue);
 }

 /// Emit an ONNXUnsqueezeOp. If the input is constant, do const
@@ -155,18 +144,8 @@ Value foldOrEmitONNXSqueezeOpStablehlo(ConversionPatternRewriter &rewriter,
 Value foldOrEmitONNXUnsqueezeOpStablehlo(ConversionPatternRewriter &rewriter,
     Location loc, Type resultType, Value input, int64_t axis) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
-  TensorType tensorType = create.onnx.toTensor(resultType);
-  if (DenseElementsAttr inputElements =
-          getDenseElementAttrFromConstValue(input)) {
-    DenseElementsAttr unsqueezedElements = inputElements.reshape(tensorType);
-    Value constVal = create.onnx.constant(unsqueezedElements);
-    return constVal;
-  } else {
-    return rewriter
-        .create<ONNXUnsqueezeOp>(loc, tensorType, create.onnx.toTensor(input),
-            create.onnx.constantInt64({axis}))
-        .getResult();
-  }
+  return create.onnx.foldOrEmitONNXUnsqueezeOp(rewriter, loc, resultType, input,
+      axis, getDenseElementAttrFromConstValue);
 }

 /// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
@@ -176,40 +155,8 @@ std::vector<Value> foldOrEmitONNXSplitOpStablehlo(
     ConversionPatternRewriter &rewriter, Location loc,
     ArrayRef<Type> resultTypes, Value input, int64_t axis) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
-  std::vector<Value> resVals;
-  int outputNum = resultTypes.size();
-  if (DenseElementsAttr inputElements =
-          getDenseElementAttrFromConstValue(input)) {
-    auto inputShape = inputElements.getType().getShape();
-    assert(outputNum == 0 || inputShape[axis] % outputNum == 0);
-    int64_t sizeOfEachSplit = outputNum != 0 ? inputShape[axis] / outputNum : 0;
-    SmallVector<int64_t, 4> sizes(outputNum, sizeOfEachSplit);
-
-    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
-    std::vector<ElementsAttr> splits =
-        elementsBuilder.split(inputElements, axis, sizes);
-    for (ElementsAttr splitElements : splits) {
-      // Avoid DisposableElementsAttr during conversion.
-      DenseElementsAttr denseSplitElements =
-          elementsBuilder.toDenseElementsAttr(splitElements);
-      Value constVal = create.onnx.constant(denseSplitElements);
-      resVals.emplace_back(constVal);
-    }
-  } else {
-    SmallVector<Type, 4> convertedTypes;
-    SmallVector<int64_t> splitSizesI64;
-    for (auto t : resultTypes) {
-      convertedTypes.emplace_back(create.onnx.toTensor(t));
-      splitSizesI64.emplace_back(t.cast<ShapedType>().getShape()[axis]);
-    }
-    Value splitSizes = create.onnx.constantInt64(splitSizesI64);
-    ONNXSplitOp split = rewriter.create<ONNXSplitOp>(loc, convertedTypes,
-        create.onnx.toTensor(input), splitSizes,
-        /*axis=*/axis, nullptr);
-    for (int i = 0; i < outputNum; ++i)
-      resVals.emplace_back(split.getOutputs()[i]);
-  }
-  return resVals;
+  return create.onnx.foldOrEmitONNXSplitOp(rewriter, loc, resultTypes, input,
+      axis, getDenseElementAttrFromConstValue);
 }

 /// Emit an ONNXTransposeOp. If the input is constant, do const propagation,
@@ -217,26 +164,8 @@ std::vector<Value> foldOrEmitONNXSplitOpStablehlo(
 Value foldOrEmitONNXTransposeOpStablehlo(ConversionPatternRewriter &rewriter,
     Location loc, Type resultType, Value input, ArrayAttr permAttr) {
   MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
-  if (DenseElementsAttr inputElements =
-          getDenseElementAttrFromConstValue(input)) {
-    SmallVector<uint64_t, 4> perm;
-    for (auto permVal : permAttr.getValue())
-      perm.emplace_back(permVal.cast<IntegerAttr>().getInt());
-
-    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
-    ElementsAttr transposedElements =
-        elementsBuilder.transpose(inputElements, perm);
-    // Avoid DisposableElementsAttr during conversion.
-    DenseElementsAttr denseTransposedElements =
-        elementsBuilder.toDenseElementsAttr(transposedElements);
-    Value constVal = create.onnx.constant(denseTransposedElements);
-    return constVal;
-  } else {
-    return rewriter
-        .create<ONNXTransposeOp>(loc, create.onnx.toTensor(resultType),
-            create.onnx.toTensor(input), permAttr)
-        .getResult();
-  }
+  return create.onnx.foldOrEmitONNXTransposeOp(rewriter, loc, resultType, input,
+      permAttr, getDenseElementAttrFromConstValue);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp b/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp
index bc77fc217f..832e72b973 100644
--- a/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp
+++ b/src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp
@@ -2,10 +2,9 @@
  * SPDX-License-Identifier: Apache-2.0
  */

-//====------ ONNXToStablehloCommon.hpp - ONNX dialects to Stablehlo lowering
-//--------===//
+//====-- ONNXToStablehloCommon.hpp - ONNX dialects to Stablehlo lowering--===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp b/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp
index 41203a3e4e..d2fd54cd70 100644
--- a/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp
+++ b/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp
@@ -4,7 +4,7 @@

 //===--------------- LSTM.cpp - Lowering LSTM Op --------------------------===//
 //
-// Copyright 2023
+// Copyright 2023-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp
index f81eaff2d7..ab110b2e61 100644
--- a/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp
+++ b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp
@@ -4,7 +4,7 @@

 //===--------------- RNNBase.cpp - Lowering RNN Ops -----------------------===//
 //
-// Copyright 2023
+// Copyright 2023-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp
index 4e6f7336eb..a635f3ec9c 100644
--- a/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp
+++ b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp
@@ -4,7 +4,7 @@

 //===--------------- RNNBase.hpp - Lowering RNN Ops -----------------------===//
 //
-// Copyright 2023
+// Copyright 2023-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/ArgMax.cpp b/src/Conversion/ONNXToStablehlo/Tensor/ArgMax.cpp
index 45a3cd32ba..6d5eeb556d 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/ArgMax.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/ArgMax.cpp
@@ -4,7 +4,7 @@

 //===---------------- ArgMax.cpp - Lowering ArgMax Op -------------------===//
 //
-// Copyright 2021-2022 The IBM Research Authors.
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Concat.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Concat.cpp
index 25158021f0..b387b446d9 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Concat.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Concat.cpp
@@ -4,7 +4,7 @@

 //===---------------- Concat.cpp - Lowering Concat Op -------------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Constant.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Constant.cpp
index b19e2885fb..226cfa1433 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Constant.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Constant.cpp
@@ -4,7 +4,7 @@

 //===---------------- Constant.cpp - Lowering Constant Op -----------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/DepthToSpace.cpp b/src/Conversion/ONNXToStablehlo/Tensor/DepthToSpace.cpp
index b73e5d9bf5..9a9aafc61a 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/DepthToSpace.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/DepthToSpace.cpp
@@ -4,7 +4,7 @@

 //===------------ DepthToSpace.cpp - Lowering DepthToSpace Op -------------===//
 //
-// Copyright 2023
+// Copyright 2023-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Expand.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Expand.cpp
index 70d19c93a0..cfbed51a18 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Expand.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Expand.cpp
@@ -4,7 +4,7 @@

 //===----------------Expand.cpp - Lowering Expand Op----------------------=== //
 //
-// Copyright 2020-2022 The IBM Research Authors.
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Flatten.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Flatten.cpp
index 3ff0b244c4..bf98ee332f 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Flatten.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Flatten.cpp
@@ -4,7 +4,7 @@

 //===---------------- Concat.cpp - Lowering Concat Op -------------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Gather.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Gather.cpp
index ecc116148d..46937a9f7d 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Gather.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Gather.cpp
@@ -4,7 +4,7 @@

 //===---------------- Gather.cpp - Lowering Gather Op ---------------------===//
 //
-// Copyright 2020-2022 The IBM Research Authors.
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/GatherElements.cpp b/src/Conversion/ONNXToStablehlo/Tensor/GatherElements.cpp
index 0f84a26586..4916504496 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/GatherElements.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/GatherElements.cpp
@@ -4,7 +4,7 @@

 //===-------- GatherElements.cpp - Lowering GatherElements Op -------------===//
 //
-// Copyright 2023
+// Copyright 2023-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Identity.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Identity.cpp
index 197eb60883..995b6f4e0f 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Identity.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Identity.cpp
@@ -4,7 +4,7 @@

 //===----------------- Identity.cpp - Lowering Identity Op ----------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/OneHot.cpp b/src/Conversion/ONNXToStablehlo/Tensor/OneHot.cpp
index 71b3ca50a5..3ea38b01ec 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/OneHot.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/OneHot.cpp
@@ -4,7 +4,7 @@

 //===---------------- OneHot.cpp - Lowering OneHot Op -------------------===//
 //
-// Copyright 2023
+// Copyright 2023-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Pad.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Pad.cpp
index 44d7303c39..14b72a7f92 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Pad.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Pad.cpp
@@ -4,7 +4,7 @@

 //===----------- Pad.cpp - Lowering Pad Op ------------===//
 //
-// Copyright 2022-2023
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Reshape.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Reshape.cpp
index b3f9e28b75..ce73e9bdda 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Reshape.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Reshape.cpp
@@ -4,7 +4,7 @@

 //===---------------- Reshape.cpp - Lowering Reshape Op -------------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/ScatterND.cpp b/src/Conversion/ONNXToStablehlo/Tensor/ScatterND.cpp
index 80d5950948..8b929924ab 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/ScatterND.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/ScatterND.cpp
@@ -4,7 +4,7 @@

 //===--------------- ScatterND.cpp - Lowering ScatterND Op ----------------===//
 //
-// Copyright 2023
+// Copyright 2023-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Shape.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Shape.cpp
index 5d219dd348..7e0955d8f2 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Shape.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Shape.cpp
@@ -4,7 +4,7 @@

 //===----------------- Shape.cpp - Lowering Shape Op ----------------------===//
 //
-// Copyright 2020-2022 The IBM Research Authors.
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Slice.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Slice.cpp
index 8c68c9097c..3b6ae7fb4a 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Slice.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Slice.cpp
@@ -4,7 +4,7 @@

 //===----------------Slice.cpp - Lowering Slice Op----------------------=== //
 //
-// Copyright 2020-2023 The IBM Research Authors.
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Split.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Split.cpp
index 6a3cad9684..8eab7eca59 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Split.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Split.cpp
@@ -4,7 +4,7 @@

 //===---------------- Split.cpp - Lowering Split Op -----------------------===//
 //
-// Copyright 2022-2023
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Squeeze.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Squeeze.cpp
index f6d3545523..5cc64cd40e 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Squeeze.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Squeeze.cpp
@@ -4,7 +4,7 @@

 //===--------------- Squeeze.cpp - Lowering Squeeze Op ----------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Tile.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Tile.cpp
index 778144b720..b74d8c9e1b 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Tile.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Tile.cpp
@@ -4,7 +4,7 @@

 //===----------------Tile.cpp - Lowering Tile Op----------------------=== //
 //
-// Copyright 2020-2023 The IBM Research Authors.
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Transpose.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Transpose.cpp
index d2f0151f09..350f8f5de1 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Transpose.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Transpose.cpp
@@ -4,7 +4,7 @@

 //===---------------- Transpose.cpp - Lowering Transpose Op ---------------===//
 //
-// Copyright 2022-2023
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToStablehlo/Tensor/Unsqueeze.cpp b/src/Conversion/ONNXToStablehlo/Tensor/Unsqueeze.cpp
index 44a43460e4..e517b89d37 100644
--- a/src/Conversion/ONNXToStablehlo/Tensor/Unsqueeze.cpp
+++ b/src/Conversion/ONNXToStablehlo/Tensor/Unsqueeze.cpp
@@ -4,7 +4,7 @@

 //===--------------- Unsqueeze.cpp - Lowering Unsqueeze Op ----------------===//
 //
-// Copyright 2022
+// Copyright 2022-2024
 //
 // =============================================================================
 //
diff --git a/src/Dialect/ONNX/DialectBuilder.cpp b/src/Dialect/ONNX/DialectBuilder.cpp
index ffd5131888..e7b547a919 100644
--- a/src/Dialect/ONNX/DialectBuilder.cpp
+++ b/src/Dialect/ONNX/DialectBuilder.cpp
@@ -501,6 +501,204 @@ Value OnnxBuilder::reshapeToNDim(
   return reshape(outputType, val, newShapeVals);
 }

+// =============================================================================
+// Fold and emit support.
+// =============================================================================
+
+/// Emit an ONNXSqueezeOp. If the input is constant, do const propagation,
+/// and return a constant.
+Value OnnxBuilder::foldOrEmitONNXSqueezeOp(ConversionPatternRewriter &rewriter,
+    Location loc, Type resultType, Value input, int64_t axis,
+    DenseElementsAttrGetter getDenseElementAttrFromConstValue) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr squeezedElements = inputElements.reshape(tensorType);
+    return create.onnx.constant(squeezedElements);
+  } else {
+    return rewriter
+        .create<ONNXSqueezeOp>(loc, tensorType, create.onnx.toTensor(input),
+            create.onnx.constantInt64({axis}))
+        .getResult();
+  }
+}
+
+/// Emit an ONNXSqueezeV11Op. If the input is constant, do const propagation,
+/// and return a constant.
+Value OnnxBuilder::foldOrEmitONNXSqueezeV11Op(
+    ConversionPatternRewriter &rewriter, Location loc, Type resultType,
+    Value input, int64_t axis,
+    DenseElementsAttrGetter getDenseElementAttrFromConstValue) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr squeezedElements = inputElements.reshape(tensorType);
+    return create.onnx.constant(squeezedElements);
+  } else {
+    return rewriter
+        .create<ONNXSqueezeV11Op>(loc, tensorType, create.onnx.toTensor(input),
+            rewriter.getI64ArrayAttr(axis))
+        .getResult();
+  }
+}
+
+/// Emit an ONNXUnsqueezeOp. If the input is constant, do const
+/// propagation, and return a constant.
+Value OnnxBuilder::foldOrEmitONNXUnsqueezeOp(
+    ConversionPatternRewriter &rewriter, Location loc, Type resultType,
+    Value input, int64_t axis,
+    DenseElementsAttrGetter getDenseElementAttrFromConstValue) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr unsqueezedElements = inputElements.reshape(tensorType);
+    return create.onnx.constant(unsqueezedElements);
+  } else {
+    return rewriter
+        .create<ONNXUnsqueezeOp>(loc, tensorType, create.onnx.toTensor(input),
+            create.onnx.constantInt64({axis}))
+        .getResult();
+  }
+}
+
+/// Emit an ONNXUnsqueezeV11Op. If the input is constant, do const
+/// propagation, and return a constant.
+Value OnnxBuilder::foldOrEmitONNXUnsqueezeV11Op(
+    ConversionPatternRewriter &rewriter, Location loc, Type resultType,
+    Value input, int64_t axis,
+    DenseElementsAttrGetter getDenseElementAttrFromConstValue) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  TensorType tensorType = create.onnx.toTensor(resultType);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    DenseElementsAttr unsqueezedElements = inputElements.reshape(tensorType);
+    return create.onnx.constant(unsqueezedElements);
+  } else {
+    return rewriter
+        .create<ONNXUnsqueezeV11Op>(loc, tensorType,
+            create.onnx.toTensor(input), rewriter.getI64ArrayAttr(axis))
+        .getResult();
+  }
+}
+
+/// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
+/// return constants.
+/// Only support evenly splitting.
+std::vector<Value> OnnxBuilder::foldOrEmitONNXSplitOp(
+    ConversionPatternRewriter &rewriter, Location loc,
+    ArrayRef<Type> resultTypes, Value input, int64_t axis,
+    DenseElementsAttrGetter getDenseElementAttrFromConstValue) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  std::vector<Value> resVals;
+  int outputNum = resultTypes.size();
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    auto inputShape = inputElements.getType().getShape();
+    assert(outputNum == 0 || inputShape[axis] % outputNum == 0);
+    int64_t sizeOfEachSplit = outputNum != 0 ? inputShape[axis] / outputNum : 0;
+    SmallVector<int64_t, 4> sizes(outputNum, sizeOfEachSplit);
+
+    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
+    std::vector<ElementsAttr> splits =
+        elementsBuilder.split(inputElements, axis, sizes);
+    for (ElementsAttr splitElements : splits) {
+      // Avoid DisposableElementsAttr during conversion.
+      DenseElementsAttr denseSplitElements =
+          elementsBuilder.toDenseElementsAttr(splitElements);
+      Value constVal = create.onnx.constant(denseSplitElements);
+      resVals.emplace_back(constVal);
+    }
+  } else {
+    SmallVector<Type, 4> convertedTypes;
+    SmallVector<int64_t> splitSizesI64;
+    for (auto t : resultTypes) {
+      convertedTypes.emplace_back(create.onnx.toTensor(t));
+      splitSizesI64.emplace_back(t.cast<ShapedType>().getShape()[axis]);
+    }
+    Value splitSizes = create.onnx.constantInt64(splitSizesI64);
+    ONNXSplitOp split = rewriter.create<ONNXSplitOp>(loc, convertedTypes,
+        create.onnx.toTensor(input), splitSizes,
+        /*axis=*/axis, nullptr);
+    for (int i = 0; i < outputNum; ++i)
+      resVals.emplace_back(split.getOutputs()[i]);
+  }
+  return resVals;
+}
+
+/// Emit an ONNXSplitV11Op. If the input is constant, do const propagation, and
+/// return constants.
+/// Only support evenly splitting.
+std::vector<Value> OnnxBuilder::foldOrEmitONNXSplitV11Op(
+    ConversionPatternRewriter &rewriter, Location loc,
+    ArrayRef<Type> resultTypes, Value input, int64_t axis,
+    DenseElementsAttrGetter getDenseElementAttrFromConstValue) {
+
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+
+  std::vector<Value> resVals;
+  int outputNum = resultTypes.size();
+
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    auto inputShape = inputElements.getType().getShape();
+    assert(outputNum == 0 || inputShape[axis] % outputNum == 0);
+    int64_t sizeOfEachSplit = outputNum != 0 ? inputShape[axis] / outputNum : 0;
+    SmallVector<int64_t, 4> sizes(outputNum, sizeOfEachSplit);
+
+    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
+    std::vector<ElementsAttr> splits =
+        elementsBuilder.split(inputElements, axis, sizes);
+    for (ElementsAttr splitElements : splits) {
+      // Avoid DisposableElementsAttr during conversion.
+      DenseElementsAttr denseSplitElements =
+          elementsBuilder.toDenseElementsAttr(splitElements);
+      resVals.emplace_back(create.onnx.constant(denseSplitElements));
+    }
+  } else {
+    SmallVector<Type, 4> convertedTypes;
+    for (auto t : resultTypes) {
+      convertedTypes.emplace_back(create.onnx.toTensor(t));
+    }
+    ONNXSplitV11Op split = rewriter.create<ONNXSplitV11Op>(loc, convertedTypes,
+        create.onnx.toTensor(input),
+        /*axis=*/axis, nullptr);
+    for (int i = 0; i < outputNum; ++i)
+      resVals.emplace_back(split.getOutputs()[i]);
+  }
+  return resVals;
+}
+
+/// Emit an ONNXTransposeOp. If the input is constant, do const propagation,
+/// and return a constant.
+Value OnnxBuilder::foldOrEmitONNXTransposeOp(
+    ConversionPatternRewriter &rewriter, Location loc, Type resultType,
+    Value input, ArrayAttr permAttr,
+    DenseElementsAttrGetter getDenseElementAttrFromConstValue) {
+  MultiDialectBuilder<OnnxBuilder> create(rewriter, loc);
+  if (DenseElementsAttr inputElements =
+          getDenseElementAttrFromConstValue(input)) {
+    SmallVector<uint64_t, 4> perm;
+    for (auto permVal : permAttr.getValue())
+      perm.emplace_back(permVal.cast<IntegerAttr>().getInt());
+
+    OnnxElementsAttrBuilder elementsBuilder(rewriter.getContext());
+    ElementsAttr transposedElements =
+        elementsBuilder.transpose(inputElements, perm);
+    // Avoid DisposableElementsAttr during conversion.
+    DenseElementsAttr denseTransposedElements =
+        elementsBuilder.toDenseElementsAttr(transposedElements);
+    return create.onnx.constant(denseTransposedElements);
+  } else {
+    return rewriter
+        .create<ONNXTransposeOp>(loc, create.onnx.toTensor(resultType),
+            create.onnx.toTensor(input), permAttr)
+        .getResult();
+  }
+}
+
 // =============================================================================
 // IndexExpr Builder for Analysis
 // =============================================================================
diff --git a/src/Dialect/ONNX/DialectBuilder.hpp b/src/Dialect/ONNX/DialectBuilder.hpp
index 95aa233979..e7712c19c7 100644
--- a/src/Dialect/ONNX/DialectBuilder.hpp
+++ b/src/Dialect/ONNX/DialectBuilder.hpp
@@ -34,11 +34,11 @@ struct OnnxBuilder : DialectBuilder {

   // Create operation and infer shape.
   template <typename OnnxOpType, typename... Args>
-  OnnxOpType createOpAndInferShapes(Args &&... args) const;
+  OnnxOpType createOpAndInferShapes(Args &&...args) const;

   template <typename OnnxOpType, typename... Args>
   OnnxOpType createTypedOpAndInferShapes(
-      mlir::Type result_ty, Args &&... args) const;
+      mlir::Type result_ty, Args &&...args) const;

   // ONNXAddOp
   mlir::Value add(mlir::Value A, mlir::Value B) const;
@@ -212,6 +212,68 @@ struct OnnxBuilder : DialectBuilder {
   mlir::Value where(mlir::Type outputType, mlir::Value condition, mlir::Value X,
       mlir::Value Y) const;

+  // =============================================================================
+  // Fold and emit support.
+  // =============================================================================
+
+  // These utilities emit an ONNXOp and try to fold it if possible. If the input
+  // is constant, do const propagation, and return a constant. The funcion needs
+  // to have std::function<DenseElementsAttr(mlir::Value value)> as the last
+  // argument. It is used to get the DenseElementsAttr from the value if the
+  // value is a constant.
+
+  using DenseElementsAttrGetter =
+      std::function<mlir::DenseElementsAttr(mlir::Value)>;
+
+  /// Emit an ONNXSqueezeOp. If the input is constant, do const propagation, and
+  /// return a constant.
+  mlir::Value foldOrEmitONNXSqueezeOp(mlir::ConversionPatternRewriter &rewriter,
+      mlir::Location loc, mlir::Type resultType, mlir::Value input,
+      int64_t axis, DenseElementsAttrGetter getDenseElementAttrFromConstValue);
+
+  /// Emit an ONNXSqueezeV11Op. If the input is constant, do const propagation,
+  /// and return a constant.
+  mlir::Value foldOrEmitONNXSqueezeV11Op(
+      mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+      mlir::Type resultType, mlir::Value input, int64_t axis,
+      DenseElementsAttrGetter getDenseElementAttrFromConstValue);
+
+  /// Emit an ONNXUnsqueezeOp. If the input is constant, do const propagation,
+  /// and return a constant.
+  mlir::Value foldOrEmitONNXUnsqueezeOp(
+      mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+      mlir::Type resultType, mlir::Value input, int64_t axis,
+      DenseElementsAttrGetter getDenseElementAttrFromConstValue);
+
+  /// Emit an ONNXUnsqueezeV11Op. If the input is constant, do const
+  /// propagation, and return a constant.
+  mlir::Value foldOrEmitONNXUnsqueezeV11Op(
+      mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+      mlir::Type resultType, mlir::Value input, int64_t axis,
+      DenseElementsAttrGetter getDenseElementAttrFromConstValue);
+
+  /// Emit an ONNXSplitOp. If the input is constant, do const propagation, and
+  /// return constants.
+  /// Only support evenly splitting.
+  std::vector<mlir::Value> foldOrEmitONNXSplitOp(
+      mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+      llvm::ArrayRef<mlir::Type> resultTypes, mlir::Value input, int64_t axis,
+      DenseElementsAttrGetter getDenseElementAttrFromConstValue);
+
+  /// Emit an ONNXSplitV11Op. If the input is constant, do const propagation,
+  /// and return constants. Only support evenly splitting.
+  std::vector<mlir::Value> foldOrEmitONNXSplitV11Op(
+      mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+      llvm::ArrayRef<mlir::Type> resultTypes, mlir::Value input, int64_t axis,
+      DenseElementsAttrGetter getDenseElementAttrFromConstValue);
+
+  /// Emit an ONNXTransposeOp. If the input is constant, do const propagation,
+  /// and return a constant.
+  mlir::Value foldOrEmitONNXTransposeOp(
+      mlir::ConversionPatternRewriter &rewriter, mlir::Location loc,
+      mlir::Type resultType, mlir::Value input, mlir::ArrayAttr permAttr,
+      DenseElementsAttrGetter getDenseElementAttrFromConstValue);
+
 private:
   mlir::IntegerAttr getSignedInt64Attr(int64_t n) const;
 };

From 3f4797d82e8e4d81bb2adc2c2aca1c86c94eb7db Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Thu, 11 Jan 2024 06:56:26 -0500
Subject: [PATCH 10/11] fix clang format

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 src/Dialect/ONNX/DialectBuilder.hpp | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/Dialect/ONNX/DialectBuilder.hpp b/src/Dialect/ONNX/DialectBuilder.hpp
index e7712c19c7..2df39afd98 100644
--- a/src/Dialect/ONNX/DialectBuilder.hpp
+++ b/src/Dialect/ONNX/DialectBuilder.hpp
@@ -34,11 +34,11 @@ struct OnnxBuilder : DialectBuilder {

   // Create operation and infer shape.
   template <typename OnnxOpType, typename... Args>
-  OnnxOpType createOpAndInferShapes(Args &&...args) const;
+  OnnxOpType createOpAndInferShapes(Args &&... args) const;

   template <typename OnnxOpType, typename... Args>
   OnnxOpType createTypedOpAndInferShapes(
-      mlir::Type result_ty, Args &&...args) const;
+      mlir::Type result_ty, Args &&... args) const;

   // ONNXAddOp
   mlir::Value add(mlir::Value A, mlir::Value B) const;

From 9ec2829fa0907f4460a8aaeabf7c0deac05605c2 Mon Sep 17 00:00:00 2001
From: Yan Xu <yan.xu0210@bytedance.com>
Date: Tue, 16 Jan 2024 01:38:50 -0500
Subject: [PATCH 11/11] move common RNN utilities into
 src/Conversion/ONNXConversionCommon

Signed-off-by: Yan Xu <yan.xu0210@bytedance.com>
---
 src/Conversion/CMakeLists.txt                 |   1 +
 .../ONNXConversionCommon/CMakeLists.txt       |  12 ++
 .../ONNXConversionCommon/RNN/LSTM.cpp         | 154 ++++++++++++++++
 .../ONNXConversionCommon/RNN/LSTM.hpp         |  54 ++++++
 .../ONNXConversionCommon/RNN/RNNBase.cpp      |  73 ++++++++
 .../ONNXConversionCommon/RNN/RNNBase.hpp      |  55 ++++++
 src/Conversion/ONNXToKrnl/CMakeLists.txt      |   9 +-
 src/Conversion/ONNXToKrnl/RNN/LSTM.cpp        | 161 +----------------
 src/Conversion/ONNXToKrnl/RNN/RNNBase.cpp     |  51 ------
 src/Conversion/ONNXToKrnl/RNN/RNNBase.hpp     |  25 +--
 src/Conversion/ONNXToStablehlo/CMakeLists.txt |   1 +
 src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp   | 170 +-----------------
 .../ONNXToStablehlo/RNN/RNNBase.cpp           |  51 ------
 .../ONNXToStablehlo/RNN/RNNBase.hpp           |  26 +--
 14 files changed, 361 insertions(+), 482 deletions(-)
 create mode 100644 src/Conversion/ONNXConversionCommon/CMakeLists.txt
 create mode 100644 src/Conversion/ONNXConversionCommon/RNN/LSTM.cpp
 create mode 100644 src/Conversion/ONNXConversionCommon/RNN/LSTM.hpp
 create mode 100644 src/Conversion/ONNXConversionCommon/RNN/RNNBase.cpp
 create mode 100644 src/Conversion/ONNXConversionCommon/RNN/RNNBase.hpp

diff --git a/src/Conversion/CMakeLists.txt b/src/Conversion/CMakeLists.txt
index 19c7b4f046..c99192f261 100644
--- a/src/Conversion/CMakeLists.txt
+++ b/src/Conversion/CMakeLists.txt
@@ -1,5 +1,6 @@
 # SPDX-License-Identifier: Apache-2.0

+add_subdirectory(ONNXConversionCommon)
 add_subdirectory(ONNXToKrnl)
 add_subdirectory(KrnlToLLVM)
 add_subdirectory(KrnlToAffine)
diff --git a/src/Conversion/ONNXConversionCommon/CMakeLists.txt b/src/Conversion/ONNXConversionCommon/CMakeLists.txt
new file mode 100644
index 0000000000..bb707912c1
--- /dev/null
+++ b/src/Conversion/ONNXConversionCommon/CMakeLists.txt
@@ -0,0 +1,12 @@
+# SPDX-License-Identifier: Apache-2.0
+
+add_onnx_mlir_library(OMONNXConversionCommon
+  RNN/RNNBase.cpp
+  RNN/LSTM.cpp
+
+  LINK_LIBS PUBLIC
+
+  OMONNXOps
+  OMSupport
+  MLIRTransforms
+)
diff --git a/src/Conversion/ONNXConversionCommon/RNN/LSTM.cpp b/src/Conversion/ONNXConversionCommon/RNN/LSTM.cpp
new file mode 100644
index 0000000000..3de32358f6
--- /dev/null
+++ b/src/Conversion/ONNXConversionCommon/RNN/LSTM.cpp
@@ -0,0 +1,154 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- LSTM.cpp - Lowering LSTM Op --------------------------===//
+//
+// Copyright 2019-2024 The IBM Research Authors.
+// Modifications Copyright 2023-2024
+//
+// =============================================================================
+//
+// This file includes utilities for lowering the ONNX LSTM Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXConversionCommon/RNN/LSTM.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+template <>
+bool hasAllNoneOutput<ONNXLSTMOp>(ONNXLSTMOp *op) {
+  return (isNoneValue(op->getY()) && isNoneValue(op->getYH()) &&
+          isNoneValue(op->getYC()));
+}
+
+template <>
+std::tuple<LstmActivationPack, LstmActivationPack>
+getActivationPack<ONNXLSTMOp, LstmActivationPack>(ONNXLSTMOp *op) {
+  auto direction = op->getDirection();
+  auto activations = op->getActivations();
+  auto activationAlpha = op->getActivationAlpha();
+  auto activationBeta = op->getActivationBeta();
+
+  LstmActivationPack activationForward, activationReverse;
+
+  // Get activation function name.
+  // Default forward functions
+  activationForward.f.name = "sigmoid";
+  activationForward.g.name = "tanh";
+  activationForward.h.name = "tanh";
+  // Default backward functions
+  activationReverse.f.name = "sigmoid";
+  activationReverse.g.name = "tanh";
+  activationReverse.h.name = "tanh";
+  if (activations) {
+    ArrayAttr activationArrAttr = activations.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.name =
+            activationArrAttr[0].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.name =
+            activationArrAttr[1].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.name =
+            activationArrAttr[2].cast<StringAttr>().getValue();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.name =
+            activationArrAttr[startIndex].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.name =
+            activationArrAttr[startIndex + 1].cast<StringAttr>().getValue();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.name =
+            activationArrAttr[startIndex + 2].cast<StringAttr>().getValue();
+      }
+    }
+  }
+
+  // Get alpha attributes.
+  if (activationAlpha) {
+    ArrayAttr activationArrAttr = activationAlpha.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.alpha = activationArrAttr[0].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.alpha = activationArrAttr[1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.alpha = activationArrAttr[2].cast<FloatAttr>();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.alpha =
+            activationArrAttr[startIndex].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.alpha =
+            activationArrAttr[startIndex + 1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.alpha =
+            activationArrAttr[startIndex + 2].cast<FloatAttr>();
+      }
+    }
+  }
+
+  // Get beta attributes.
+  if (activationBeta) {
+    ArrayAttr activationArrAttr = activationBeta.value();
+    if (direction == FORWARD || direction == BIDIRECTIONAL) {
+      // Forward activations.
+      if (activationArrAttr.size() > 0) {
+        activationForward.f.beta = activationArrAttr[0].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 1) {
+        activationForward.g.beta = activationArrAttr[1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > 2) {
+        activationForward.h.beta = activationArrAttr[2].cast<FloatAttr>();
+      }
+    }
+
+    // Reverse activations.
+    if (direction == REVERSE || direction == BIDIRECTIONAL) {
+      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
+      if (activationArrAttr.size() > startIndex) {
+        activationReverse.f.beta =
+            activationArrAttr[startIndex].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 1) {
+        activationReverse.g.beta =
+            activationArrAttr[startIndex + 1].cast<FloatAttr>();
+      }
+      if (activationArrAttr.size() > startIndex + 2) {
+        activationReverse.h.beta =
+            activationArrAttr[startIndex + 2].cast<FloatAttr>();
+      }
+    }
+  }
+
+  return std::make_tuple(activationForward, activationReverse);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXConversionCommon/RNN/LSTM.hpp b/src/Conversion/ONNXConversionCommon/RNN/LSTM.hpp
new file mode 100644
index 0000000000..913b10cea5
--- /dev/null
+++ b/src/Conversion/ONNXConversionCommon/RNN/LSTM.hpp
@@ -0,0 +1,54 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- LSTM.hpp - Lowering LSTM Op --------------------------===//
+//
+// Copyright 2024
+//
+// =============================================================================
+//
+// This file includes utilities for lowering the ONNX LSTM Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXConversionCommon/RNN/RNNBase.hpp"
+
+namespace onnx_mlir {
+
+struct LstmActivationPack {
+  RNNActivation f;
+  RNNActivation g;
+  RNNActivation h;
+};
+
+struct LstmWeightPack {
+  mlir::Value WT;
+  mlir::Value RT;
+};
+
+struct LstmBiasPack {
+  bool hasBias = false;
+  mlir::Value Wbi;
+  mlir::Value Wbo;
+  mlir::Value Wbf;
+  mlir::Value Wbc;
+  mlir::Value Rbi;
+  mlir::Value Rbo;
+  mlir::Value Rbf;
+  mlir::Value Rbc;
+  // Put peephole here.
+  bool hasPeephole = false;
+  mlir::Value Pi;
+  mlir::Value Po;
+  mlir::Value Pf;
+};
+
+template <>
+bool hasAllNoneOutput<mlir::ONNXLSTMOp>(mlir::ONNXLSTMOp *op);
+
+template <>
+std::tuple<LstmActivationPack, LstmActivationPack>
+getActivationPack<mlir::ONNXLSTMOp, LstmActivationPack>(mlir::ONNXLSTMOp *op);
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXConversionCommon/RNN/RNNBase.cpp b/src/Conversion/ONNXConversionCommon/RNN/RNNBase.cpp
new file mode 100644
index 0000000000..5ab59dd830
--- /dev/null
+++ b/src/Conversion/ONNXConversionCommon/RNN/RNNBase.cpp
@@ -0,0 +1,73 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- RNNBase.cpp - Lowering RNN Ops -----------------------===//
+//
+// Copyright 2019-2024 The IBM Research Authors.
+// Modifications Copyright 2023-2024
+//
+// =============================================================================
+//
+// This file defines common base utilities for lowering the ONNX RNN Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXConversionCommon/RNN/RNNBase.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+// Get a dimension of the tensor's shape.
+int64_t dimAt(Value val, int index) {
+  return val.getType().cast<ShapedType>().getShape()[index];
+}
+
+// Apply an activation function on a given scalar operand.
+Value applyActivation(OpBuilder &rewriter, Location loc,
+    RNNActivation activation, Value operand) {
+  Value res;
+
+  std::vector<mlir::NamedAttribute> attributes;
+  if (activation.alpha) {
+    attributes.emplace_back(
+        rewriter.getNamedAttr("alpha", activation.alpha.value()));
+  }
+  if (activation.beta) {
+    attributes.emplace_back(
+        rewriter.getNamedAttr("beta", activation.beta.value()));
+  }
+  Type resType = operand.getType();
+
+  // Change equality to be case insensitive.
+  if (activation.name.equals_insensitive("relu"))
+    res = rewriter.create<ONNXReluOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("tanh"))
+    res = rewriter.create<ONNXTanhOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("sigmoid"))
+    res = rewriter.create<ONNXSigmoidOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("affine"))
+    llvm_unreachable("Unsupported activation");
+  else if (activation.name.equals_insensitive("leakyrelu"))
+    res = rewriter.create<ONNXLeakyReluOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("thresholdedrelu"))
+    res = rewriter.create<ONNXThresholdedReluOp>(
+        loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("scaledtanh"))
+    llvm_unreachable("Unsupported activation");
+  else if (activation.name.equals_insensitive("hardsigmoid"))
+    res = rewriter.create<ONNXHardSigmoidOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("elu"))
+    res = rewriter.create<ONNXEluOp>(loc, resType, operand, attributes);
+  else if (activation.name.equals_insensitive("softsign"))
+    res = rewriter.create<ONNXSoftsignOp>(loc, resType, operand);
+  else if (activation.name.equals_insensitive("softplus"))
+    res = rewriter.create<ONNXSoftplusOp>(loc, resType, operand);
+  else
+    llvm_unreachable("Unsupported activation");
+
+  return res;
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXConversionCommon/RNN/RNNBase.hpp b/src/Conversion/ONNXConversionCommon/RNN/RNNBase.hpp
new file mode 100644
index 0000000000..c33dde1555
--- /dev/null
+++ b/src/Conversion/ONNXConversionCommon/RNN/RNNBase.hpp
@@ -0,0 +1,55 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- RNNBase.hpp - Lowering RNN Ops -----------------------===//
+//
+// Copyright 2019-2024 The IBM Research Authors.
+// Modifications Copyright 2023-2024
+//
+// =============================================================================
+//
+// This file defines common base utilities for lowering the ONNX RNN Operators.
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+#include "src/Dialect/ONNX/ONNXOps.hpp"
+#include "src/Dialect/ONNX/ONNXOps/OpHelper.hpp"
+
+static constexpr llvm::StringRef FORWARD = "forward";
+static constexpr llvm::StringRef REVERSE = "reverse";
+static constexpr llvm::StringRef BIDIRECTIONAL = "bidirectional";
+
+namespace onnx_mlir {
+
+struct RNNActivation {
+  llvm::StringRef name;
+  std::optional<mlir::FloatAttr> alpha;
+  std::optional<mlir::FloatAttr> beta;
+};
+
+/// Get a dimension of the tensor's shape.
+int64_t dimAt(mlir::Value val, int index);
+
+/// Apply an activation function on a given operand.
+mlir::Value applyActivation(mlir::OpBuilder &rewriter, mlir::Location loc,
+    RNNActivation activation, mlir::Value operand);
+
+// Override the following methods when lowering an RNN operation:
+// - hasAllNoneOutput
+// - getActivationPack
+
+// Check whether all outputs have NoneType or not.
+template <typename RNNOp>
+bool hasAllNoneOutput(RNNOp *op);
+
+// Obtain activations functions for a specific operation.
+template <typename RNNOp, typename A>
+std::tuple<A, A> getActivationPack(RNNOp *op);
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToKrnl/CMakeLists.txt b/src/Conversion/ONNXToKrnl/CMakeLists.txt
index 6b5f473260..df968f8444 100644
--- a/src/Conversion/ONNXToKrnl/CMakeLists.txt
+++ b/src/Conversion/ONNXToKrnl/CMakeLists.txt
@@ -43,7 +43,7 @@ add_onnx_mlir_library(OMONNXToKrnl
   Sequence/SequenceInsert.cpp
   Sequence/SequenceLength.cpp
   Tensor/ArgMinMax.cpp
-  Tensor/Compress.cpp
+  Tensor/Compress.cpp
   Tensor/Concat.cpp
   Tensor/ConcatShapeTranspose.cpp
   Tensor/Constant.cpp
@@ -57,7 +57,7 @@ add_onnx_mlir_library(OMONNXToKrnl
   Tensor/GatherND.cpp
   Tensor/Identity.cpp
   Tensor/NonZero.cpp
-  Tensor/OneHot.cpp
+  Tensor/OneHot.cpp
   Tensor/Pad.cpp
   Tensor/PrintSignature.cpp
   Tensor/Range.cpp
@@ -65,9 +65,9 @@ add_onnx_mlir_library(OMONNXToKrnl
   Tensor/Resize.cpp
   Tensor/ReverseSequence.cpp
   Tensor/ScatterElements.cpp
-  Tensor/ScatterND.cpp
+  Tensor/ScatterND.cpp
   Tensor/Shape.cpp
-  Tensor/Size.cpp
+  Tensor/Size.cpp
   Tensor/Slice.cpp
   Tensor/SpaceToDepth.cpp
   Tensor/Split.cpp
@@ -79,6 +79,7 @@ add_onnx_mlir_library(OMONNXToKrnl

   LINK_LIBS PUBLIC
   OMAccelerator
+  OMONNXConversionCommon
   OMONNXOps
   OMSupport
   MLIRFuncDialect
diff --git a/src/Conversion/ONNXToKrnl/RNN/LSTM.cpp b/src/Conversion/ONNXToKrnl/RNN/LSTM.cpp
index f1855c829a..f8744681c9 100644
--- a/src/Conversion/ONNXToKrnl/RNN/LSTM.cpp
+++ b/src/Conversion/ONNXToKrnl/RNN/LSTM.cpp
@@ -12,6 +12,7 @@
 //
 //===----------------------------------------------------------------------===//

+#include "src/Conversion/ONNXConversionCommon/RNN/LSTM.hpp"
 #include "src/Conversion/ONNXToKrnl/RNN/RNNBase.hpp"
 #include "src/Dialect/Mlir/DialectBuilder.hpp"

@@ -31,166 +32,6 @@ struct LstmState {
   Value reverseCt;
 };

-struct LstmActivationPack {
-  RNNActivation f;
-  RNNActivation g;
-  RNNActivation h;
-};
-
-struct LstmWeightPack {
-  Value WT;
-  Value RT;
-};
-
-struct LstmBiasPack {
-  bool hasBias = false;
-  Value Wbi;
-  Value Wbo;
-  Value Wbf;
-  Value Wbc;
-  Value Rbi;
-  Value Rbo;
-  Value Rbf;
-  Value Rbc;
-  // Put peephole here.
-  bool hasPeephole = false;
-  Value Pi;
-  Value Po;
-  Value Pf;
-};
-
-template <>
-bool hasAllNoneOutput<ONNXLSTMOp>(ONNXLSTMOp *op) {
-  return (isNoneValue(op->getY()) && isNoneValue(op->getYH()) &&
-          isNoneValue(op->getYC()));
-}
-
-template <>
-std::tuple<LstmActivationPack, LstmActivationPack>
-getActivationPack<ONNXLSTMOp, LstmActivationPack>(ONNXLSTMOp *op) {
-  auto direction = op->getDirection();
-  auto activations = op->getActivations();
-  auto activationAlpha = op->getActivationAlpha();
-  auto activationBeta = op->getActivationBeta();
-
-  LstmActivationPack activationForward, activationReverse;
-
-  // Get activation function name.
-  // Default forward functions
-  activationForward.f.name = "sigmoid";
-  activationForward.g.name = "tanh";
-  activationForward.h.name = "tanh";
-  // Default backward functions
-  activationReverse.f.name = "sigmoid";
-  activationReverse.g.name = "tanh";
-  activationReverse.h.name = "tanh";
-  if (activations) {
-    ArrayAttr activationArrAttr = activations.value();
-    if (direction == FORWARD || direction == BIDIRECTIONAL) {
-      // Forward activations.
-      if (activationArrAttr.size() > 0) {
-        activationForward.f.name =
-            activationArrAttr[0].cast<StringAttr>().getValue();
-      }
-      if (activationArrAttr.size() > 1) {
-        activationForward.g.name =
-            activationArrAttr[1].cast<StringAttr>().getValue();
-      }
-      if (activationArrAttr.size() > 2) {
-        activationForward.h.name =
-            activationArrAttr[2].cast<StringAttr>().getValue();
-      }
-    }
-
-    // Reverse activations.
-    if (direction == REVERSE || direction == BIDIRECTIONAL) {
-      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
-      if (activationArrAttr.size() > startIndex) {
-        activationReverse.f.name =
-            activationArrAttr[startIndex].cast<StringAttr>().getValue();
-      }
-      if (activationArrAttr.size() > startIndex + 1) {
-        activationReverse.g.name =
-            activationArrAttr[startIndex + 1].cast<StringAttr>().getValue();
-      }
-      if (activationArrAttr.size() > startIndex + 2) {
-        activationReverse.h.name =
-            activationArrAttr[startIndex + 2].cast<StringAttr>().getValue();
-      }
-    }
-  }
-
-  // Get alpha attributes.
-  if (activationAlpha) {
-    ArrayAttr activationArrAttr = activationAlpha.value();
-    if (direction == FORWARD || direction == BIDIRECTIONAL) {
-      // Forward activations.
-      if (activationArrAttr.size() > 0) {
-        activationForward.f.alpha = activationArrAttr[0].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > 1) {
-        activationForward.g.alpha = activationArrAttr[1].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > 2) {
-        activationForward.h.alpha = activationArrAttr[2].cast<FloatAttr>();
-      }
-    }
-
-    // Reverse activations.
-    if (direction == REVERSE || direction == BIDIRECTIONAL) {
-      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
-      if (activationArrAttr.size() > startIndex) {
-        activationReverse.f.alpha =
-            activationArrAttr[startIndex].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > startIndex + 1) {
-        activationReverse.g.alpha =
-            activationArrAttr[startIndex + 1].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > startIndex + 2) {
-        activationReverse.h.alpha =
-            activationArrAttr[startIndex + 2].cast<FloatAttr>();
-      }
-    }
-  }
-
-  // Get beta attributes.
-  if (activationBeta) {
-    ArrayAttr activationArrAttr = activationBeta.value();
-    if (direction == FORWARD || direction == BIDIRECTIONAL) {
-      // Forward activations.
-      if (activationArrAttr.size() > 0) {
-        activationForward.f.beta = activationArrAttr[0].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > 1) {
-        activationForward.g.beta = activationArrAttr[1].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > 2) {
-        activationForward.h.beta = activationArrAttr[2].cast<FloatAttr>();
-      }
-    }
-
-    // Reverse activations.
-    if (direction == REVERSE || direction == BIDIRECTIONAL) {
-      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
-      if (activationArrAttr.size() > startIndex) {
-        activationReverse.f.beta =
-            activationArrAttr[startIndex].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > startIndex + 1) {
-        activationReverse.g.beta =
-            activationArrAttr[startIndex + 1].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > startIndex + 2) {
-        activationReverse.h.beta =
-            activationArrAttr[startIndex + 2].cast<FloatAttr>();
-      }
-    }
-  }
-
-  return std::make_tuple(activationForward, activationReverse);
-}
-
 template <>
 std::tuple<LstmWeightPack, LstmWeightPack>
 getWeightPack<ONNXLSTMOp, LstmWeightPack>(
diff --git a/src/Conversion/ONNXToKrnl/RNN/RNNBase.cpp b/src/Conversion/ONNXToKrnl/RNN/RNNBase.cpp
index b12747e174..8c680397a5 100644
--- a/src/Conversion/ONNXToKrnl/RNN/RNNBase.cpp
+++ b/src/Conversion/ONNXToKrnl/RNN/RNNBase.cpp
@@ -19,11 +19,6 @@ using namespace mlir;

 namespace onnx_mlir {

-// Get a dimension of the tensor's shape.
-int64_t dimAt(Value val, int index) {
-  return val.getType().cast<ShapedType>().getShape()[index];
-}
-
 /// Insert Allocate and Deallocate for the all hidden output.
 /// Shape :: [seq_length, num_directions, batch_size, hidden_size]
 Value allocAllHidden(ConversionPatternRewriter &rewriter, Location loc,
@@ -249,52 +244,6 @@ void stateToOutputForHiddenOrCell(ConversionPatternRewriter &rewriter,
   }
 }

-// Apply an activation function on a given scalar operand.
-Value applyActivation(OpBuilder &rewriter, Location loc,
-    RNNActivation activation, Value operand) {
-  Value res;
-
-  std::vector<mlir::NamedAttribute> attributes;
-  if (activation.alpha) {
-    attributes.emplace_back(
-        rewriter.getNamedAttr("alpha", activation.alpha.value()));
-  }
-  if (activation.beta) {
-    attributes.emplace_back(
-        rewriter.getNamedAttr("beta", activation.beta.value()));
-  }
-  Type resType = operand.getType();
-
-  // Change equality to be case insensitive.
-  if (activation.name.equals_insensitive("relu"))
-    res = rewriter.create<ONNXReluOp>(loc, resType, operand);
-  else if (activation.name.equals_insensitive("tanh"))
-    res = rewriter.create<ONNXTanhOp>(loc, resType, operand);
-  else if (activation.name.equals_insensitive("sigmoid"))
-    res = rewriter.create<ONNXSigmoidOp>(loc, resType, operand);
-  else if (activation.name.equals_insensitive("affine"))
-    llvm_unreachable("Unsupported activation");
-  else if (activation.name.equals_insensitive("leakyrelu"))
-    res = rewriter.create<ONNXLeakyReluOp>(loc, resType, operand, attributes);
-  else if (activation.name.equals_insensitive("thresholdedrelu"))
-    res = rewriter.create<ONNXThresholdedReluOp>(
-        loc, resType, operand, attributes);
-  else if (activation.name.equals_insensitive("scaledtanh"))
-    llvm_unreachable("Unsupported activation");
-  else if (activation.name.equals_insensitive("hardsigmoid"))
-    res = rewriter.create<ONNXHardSigmoidOp>(loc, resType, operand, attributes);
-  else if (activation.name.equals_insensitive("elu"))
-    res = rewriter.create<ONNXEluOp>(loc, resType, operand, attributes);
-  else if (activation.name.equals_insensitive("softsign"))
-    res = rewriter.create<ONNXSoftsignOp>(loc, resType, operand);
-  else if (activation.name.equals_insensitive("softplus"))
-    res = rewriter.create<ONNXSoftplusOp>(loc, resType, operand);
-  else
-    llvm_unreachable("Unsupported activation");
-
-  return res;
-}
-
 /// Create a copy of a slice of X at a specific timestep.
 /// This function is not able correctly to emit 'dealloc' for the copy since it
 /// does not have enough information about the parent context. Users must
diff --git a/src/Conversion/ONNXToKrnl/RNN/RNNBase.hpp b/src/Conversion/ONNXToKrnl/RNN/RNNBase.hpp
index ec0b28c8ec..4c5f179dbd 100644
--- a/src/Conversion/ONNXToKrnl/RNN/RNNBase.hpp
+++ b/src/Conversion/ONNXToKrnl/RNN/RNNBase.hpp
@@ -16,24 +16,13 @@

 #include "mlir/IR/AffineExpr.h"

+#include "src/Conversion/ONNXConversionCommon/RNN/RNNBase.hpp"
 #include "src/Conversion/ONNXToKrnl/ONNXToKrnlCommon.hpp"

 static constexpr int BUFFER_ALIGN = 128;
-static constexpr llvm::StringRef FORWARD = "forward";
-static constexpr llvm::StringRef REVERSE = "reverse";
-static constexpr llvm::StringRef BIDIRECTIONAL = "bidirectional";

 namespace onnx_mlir {

-struct RNNActivation {
-  llvm::StringRef name;
-  std::optional<mlir::FloatAttr> alpha;
-  std::optional<mlir::FloatAttr> beta;
-};
-
-/// Get a dimension of the tensor's shape.
-int64_t dimAt(mlir::Value val, int index);
-
 /// Insert Allocate and Deallocate for the all hidden output.
 mlir::Value allocAllHidden(mlir::ConversionPatternRewriter &rewriter,
     mlir::Location loc, const mlir::TypeConverter *typeConverter, mlir::Value X,
@@ -67,10 +56,6 @@ void stateToOutputForHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
     mlir::Location loc, mlir::Value forwardVal, mlir::Value reverseVal,
     llvm::StringRef direction, mlir::Value output);

-/// Apply an activation function on a given operand.
-mlir::Value applyActivation(mlir::OpBuilder &rewriter, mlir::Location loc,
-    RNNActivation activation, mlir::Value operand);
-
 /// Get a slice of X at a specific timestep.
 mlir::Value emitXSliceAt(mlir::ConversionPatternRewriter &rewriter,
     mlir::Location loc, mlir::Value X, mlir::Value timestep);
@@ -93,14 +78,6 @@ mlir::Value handleSequenceLens(KrnlBuilder &createKrnl, MathBuilder &createMath,
 // - calculateState
 // - stateToOutput

-// Check whether all outputs have NoneType or not.
-template <typename RNNOp>
-bool hasAllNoneOutput(RNNOp *op);
-
-// Obtain activations functions for a specific operation.
-template <typename RNNOp, typename A>
-std::tuple<A, A> getActivationPack(RNNOp *op);
-
 /// Obtain weight tensors in 2D for each gate.
 /// In ONNX, weights for gates and directions are combined in a single tensor.
 /// This function splits them into 2D tensors.
diff --git a/src/Conversion/ONNXToStablehlo/CMakeLists.txt b/src/Conversion/ONNXToStablehlo/CMakeLists.txt
index 8255395b36..690c58ef44 100644
--- a/src/Conversion/ONNXToStablehlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToStablehlo/CMakeLists.txt
@@ -78,6 +78,7 @@ add_onnx_mlir_library(OMONNXToStablehlo
   MLIRQuantDialect
   MLIRTransforms
   OMAccelerator
+  OMONNXConversionCommon
   OMONNXRewrite
   OMONNXOps
   OMSupport
diff --git a/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp b/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp
index d2fd54cd70..e3d4ee24a5 100644
--- a/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp
+++ b/src/Conversion/ONNXToStablehlo/RNN/LSTM.cpp
@@ -12,14 +12,11 @@
 //
 //===----------------------------------------------------------------------===//

+#include "src/Conversion/ONNXConversionCommon/RNN/LSTM.hpp"
 #include "src/Conversion/ONNXToStablehlo/DialectBuilder.hpp"
 #include "src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp"
 #include "src/Dialect/Mlir/DialectBuilder.hpp"

-#include "llvm/Support/Debug.h"
-
-#define DEBUG_TYPE "lstm"
-
 using namespace mlir;

 namespace onnx_mlir {
@@ -42,166 +39,6 @@ struct LstmState {
   Value reverseCt;
 };

-struct LstmActivationPack {
-  RNNActivation f;
-  RNNActivation g;
-  RNNActivation h;
-};
-
-struct LstmWeightPack {
-  Value WT;
-  Value RT;
-};
-
-struct LstmBiasPack {
-  bool hasBias = false;
-  Value Wbi;
-  Value Wbo;
-  Value Wbf;
-  Value Wbc;
-  Value Rbi;
-  Value Rbo;
-  Value Rbf;
-  Value Rbc;
-  // Put peephole here.
-  bool hasPeephole = false;
-  Value Pi;
-  Value Po;
-  Value Pf;
-};
-
-template <>
-bool hasAllNoneOutput<ONNXLSTMOp>(ONNXLSTMOp *op) {
-  return (isNoneValue(op->getY()) && isNoneValue(op->getYH()) &&
-          isNoneValue(op->getYC()));
-}
-
-template <>
-std::tuple<LstmActivationPack, LstmActivationPack>
-getActivationPack<ONNXLSTMOp, LstmActivationPack>(ONNXLSTMOp *op) {
-  auto direction = op->getDirection();
-  auto activations = op->getActivations();
-  auto activationAlpha = op->getActivationAlpha();
-  auto activationBeta = op->getActivationBeta();
-
-  LstmActivationPack activationForward, activationReverse;
-
-  // Get activation function name.
-  // Default forward functions
-  activationForward.f.name = "sigmoid";
-  activationForward.g.name = "tanh";
-  activationForward.h.name = "tanh";
-  // Default backward functions
-  activationReverse.f.name = "sigmoid";
-  activationReverse.g.name = "tanh";
-  activationReverse.h.name = "tanh";
-  if (activations) {
-    ArrayAttr activationArrAttr = activations.value();
-    if (direction == FORWARD || direction == BIDIRECTIONAL) {
-      // Forward activations.
-      if (activationArrAttr.size() > 0) {
-        activationForward.f.name =
-            activationArrAttr[0].cast<StringAttr>().getValue();
-      }
-      if (activationArrAttr.size() > 1) {
-        activationForward.g.name =
-            activationArrAttr[1].cast<StringAttr>().getValue();
-      }
-      if (activationArrAttr.size() > 2) {
-        activationForward.h.name =
-            activationArrAttr[2].cast<StringAttr>().getValue();
-      }
-    }
-
-    // Reverse activations.
-    if (direction == REVERSE || direction == BIDIRECTIONAL) {
-      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
-      if (activationArrAttr.size() > startIndex) {
-        activationReverse.f.name =
-            activationArrAttr[startIndex].cast<StringAttr>().getValue();
-      }
-      if (activationArrAttr.size() > startIndex + 1) {
-        activationReverse.g.name =
-            activationArrAttr[startIndex + 1].cast<StringAttr>().getValue();
-      }
-      if (activationArrAttr.size() > startIndex + 2) {
-        activationReverse.h.name =
-            activationArrAttr[startIndex + 2].cast<StringAttr>().getValue();
-      }
-    }
-  }
-
-  // Get alpha attributes.
-  if (activationAlpha) {
-    ArrayAttr activationArrAttr = activationAlpha.value();
-    if (direction == FORWARD || direction == BIDIRECTIONAL) {
-      // Forward activations.
-      if (activationArrAttr.size() > 0) {
-        activationForward.f.alpha = activationArrAttr[0].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > 1) {
-        activationForward.g.alpha = activationArrAttr[1].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > 2) {
-        activationForward.h.alpha = activationArrAttr[2].cast<FloatAttr>();
-      }
-    }
-
-    // Reverse activations.
-    if (direction == REVERSE || direction == BIDIRECTIONAL) {
-      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
-      if (activationArrAttr.size() > startIndex) {
-        activationReverse.f.alpha =
-            activationArrAttr[startIndex].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > startIndex + 1) {
-        activationReverse.g.alpha =
-            activationArrAttr[startIndex + 1].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > startIndex + 2) {
-        activationReverse.h.alpha =
-            activationArrAttr[startIndex + 2].cast<FloatAttr>();
-      }
-    }
-  }
-
-  // Get beta attributes.
-  if (activationBeta) {
-    ArrayAttr activationArrAttr = activationBeta.value();
-    if (direction == FORWARD || direction == BIDIRECTIONAL) {
-      // Forward activations.
-      if (activationArrAttr.size() > 0) {
-        activationForward.f.beta = activationArrAttr[0].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > 1) {
-        activationForward.g.beta = activationArrAttr[1].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > 2) {
-        activationForward.h.beta = activationArrAttr[2].cast<FloatAttr>();
-      }
-    }
-
-    // Reverse activations.
-    if (direction == REVERSE || direction == BIDIRECTIONAL) {
-      unsigned int startIndex = (direction == REVERSE) ? 0 : 3;
-      if (activationArrAttr.size() > startIndex) {
-        activationReverse.f.beta =
-            activationArrAttr[startIndex].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > startIndex + 1) {
-        activationReverse.g.beta =
-            activationArrAttr[startIndex + 1].cast<FloatAttr>();
-      }
-      if (activationArrAttr.size() > startIndex + 2) {
-        activationReverse.h.beta =
-            activationArrAttr[startIndex + 2].cast<FloatAttr>();
-      }
-    }
-  }
-
-  return std::make_tuple(activationForward, activationReverse);
-}
-
 template <>
 std::tuple<LstmWeightPack, LstmWeightPack>
 getWeightPack<ONNXLSTMOp, LstmWeightPack>(
@@ -823,9 +660,8 @@ void calculateStateWithLoop<ONNXLSTMOp, LstmState, LstmActivationPack,
 void populateLoweringONNXLSTMOpToStablehloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx, bool enableUnroll) {
   patterns.insert<onnx_mlir::stablehlo::ONNXRNNOpLowering<ONNXLSTMOp,
-      onnx_mlir::stablehlo::LstmState, onnx_mlir::stablehlo::LstmActivationPack,
-      onnx_mlir::stablehlo::LstmWeightPack,
-      onnx_mlir::stablehlo::LstmBiasPack>>(ctx, enableUnroll);
+      onnx_mlir::stablehlo::LstmState, onnx_mlir::LstmActivationPack,
+      onnx_mlir::LstmWeightPack, onnx_mlir::LstmBiasPack>>(ctx, enableUnroll);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp
index ab110b2e61..cb30905dcb 100644
--- a/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp
+++ b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.cpp
@@ -21,11 +21,6 @@ namespace onnx_mlir {

 namespace stablehlo {

-// Get a dimension of the tensor's shape.
-int64_t dimAt(Value val, int index) {
-  return val.getType().cast<ShapedType>().getShape()[index];
-}
-
 /// Allocate the all hidden output.
 /// Shape :: [seq_length, num_directions, batch_size, hidden_size]
 Value allocAllHidden(
@@ -157,52 +152,6 @@ void stateToOutputForHiddenOrCell(ConversionPatternRewriter &rewriter,
   }
 }

-// Apply an activation function on a given scalar operand.
-Value applyActivation(OpBuilder &rewriter, Location loc,
-    RNNActivation activation, Value operand) {
-  Value res;
-
-  std::vector<mlir::NamedAttribute> attributes;
-  if (activation.alpha) {
-    attributes.emplace_back(
-        rewriter.getNamedAttr("alpha", activation.alpha.value()));
-  }
-  if (activation.beta) {
-    attributes.emplace_back(
-        rewriter.getNamedAttr("beta", activation.beta.value()));
-  }
-  Type resType = operand.getType();
-
-  // Change equality to be case insensitive.
-  if (activation.name.equals_insensitive("relu"))
-    res = rewriter.create<ONNXReluOp>(loc, resType, operand);
-  else if (activation.name.equals_insensitive("tanh"))
-    res = rewriter.create<ONNXTanhOp>(loc, resType, operand);
-  else if (activation.name.equals_insensitive("sigmoid"))
-    res = rewriter.create<ONNXSigmoidOp>(loc, resType, operand);
-  else if (activation.name.equals_insensitive("affine"))
-    llvm_unreachable("Unsupported activation");
-  else if (activation.name.equals_insensitive("leakyrelu"))
-    res = rewriter.create<ONNXLeakyReluOp>(loc, resType, operand, attributes);
-  else if (activation.name.equals_insensitive("thresholdedrelu"))
-    res = rewriter.create<ONNXThresholdedReluOp>(
-        loc, resType, operand, attributes);
-  else if (activation.name.equals_insensitive("scaledtanh"))
-    llvm_unreachable("Unsupported activation");
-  else if (activation.name.equals_insensitive("hardsigmoid"))
-    res = rewriter.create<ONNXHardSigmoidOp>(loc, resType, operand, attributes);
-  else if (activation.name.equals_insensitive("elu"))
-    res = rewriter.create<ONNXEluOp>(loc, resType, operand, attributes);
-  else if (activation.name.equals_insensitive("softsign"))
-    res = rewriter.create<ONNXSoftsignOp>(loc, resType, operand);
-  else if (activation.name.equals_insensitive("softplus"))
-    res = rewriter.create<ONNXSoftplusOp>(loc, resType, operand);
-  else
-    llvm_unreachable("Unsupported activation");
-
-  return res;
-}
-
 /// Create a copy of a slice of X at a specific timestep.
 Value emitXSliceAt(ConversionPatternRewriter &rewriter, Location loc, Value X,
     Value timestepIV) {
diff --git a/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp
index a635f3ec9c..c065c77218 100644
--- a/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp
+++ b/src/Conversion/ONNXToStablehlo/RNN/RNNBase.hpp
@@ -14,25 +14,13 @@

 #pragma once

+#include "src/Conversion/ONNXConversionCommon/RNN/RNNBase.hpp"
 #include "src/Conversion/ONNXToStablehlo/ONNXToStablehloCommon.hpp"

-static constexpr llvm::StringRef FORWARD = "forward";
-static constexpr llvm::StringRef REVERSE = "reverse";
-static constexpr llvm::StringRef BIDIRECTIONAL = "bidirectional";
-
 namespace onnx_mlir {

 namespace stablehlo {

-struct RNNActivation {
-  llvm::StringRef name;
-  std::optional<mlir::FloatAttr> alpha;
-  std::optional<mlir::FloatAttr> beta;
-};
-
-/// Get a dimension of the tensor's shape.
-int64_t dimAt(mlir::Value val, int index);
-
 /// Allocate the all hidden output.
 mlir::Value allocAllHidden(mlir::ConversionPatternRewriter &rewriter,
     mlir::Location loc, mlir::Value X, mlir::Value R);
@@ -59,10 +47,6 @@ void stateToOutputForHiddenOrCell(mlir::ConversionPatternRewriter &rewriter,
     mlir::Location loc, mlir::Value forwardVal, mlir::Value reverseVal,
     llvm::StringRef direction, mlir::Value &output);

-/// Apply an activation function on a given operand.
-mlir::Value applyActivation(mlir::OpBuilder &rewriter, mlir::Location loc,
-    RNNActivation activation, mlir::Value operand);
-
 /// Get a slice of X at a specific timestep.
 mlir::Value emitXSliceAt(mlir::ConversionPatternRewriter &rewriter,
     mlir::Location loc, mlir::Value X, mlir::Value timestep);
@@ -76,14 +60,6 @@ mlir::Value emitXSliceAt(mlir::ConversionPatternRewriter &rewriter,
 // - calculateState
 // - stateToOutput

-// Check whether all outputs have NoneType or not.
-template <typename RNNOp>
-bool hasAllNoneOutput(RNNOp *op);
-
-// Obtain activations functions for a specific operation.
-template <typename RNNOp, typename A>
-std::tuple<A, A> getActivationPack(RNNOp *op);
-
 /// Obtain weight tensors in 2D for each gate.
 /// In ONNX, weights for gates and directions are combined in a single tensor.
 /// This function splits them into 2D tensors.
