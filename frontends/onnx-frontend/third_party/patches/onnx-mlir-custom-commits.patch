From ba76064f75f7f794098450d67bb0c1c508983a42 Mon Sep 17 00:00:00 2001
From: "chongsong.chen" <chongsong.chen@bytedance.com>
Date: Mon, 31 Oct 2022 06:52:50 +0000
Subject: [PATCH 1/3] Clip, Log, Tanh, Max, LeakyRelu, ConvTranspose,
 AveragePool

Signed-off-by: chongsong.chen <chongsong.chen@bytedance.com>
---
 src/Conversion/ONNXToMhlo/CMakeLists.txt      |   2 +
 .../ONNXToMhlo/ConvertONNXToMhlo.cpp          |   2 +
 src/Conversion/ONNXToMhlo/Math/Clip.cpp       | 119 ++++++++++
 .../ONNXToMhlo/Math/Elementwise.cpp           |  47 ++++
 .../ONNXToMhlo/NN/ConvTranspose.cpp           | 216 ++++++++++++++++++
 src/Conversion/ONNXToMhlo/NN/Pooling.cpp      |  77 ++++++-
 .../ONNXToMhlo/ONNXToMhloCommon.hpp           |   4 +
 src/Conversion/ONNXToMhlo/Tensor/ArgMax.cpp   |   2 +-
 src/Conversion/ONNXToMhlo/Tensor/Expand.cpp   |   2 +-
 src/Conversion/ONNXToMhlo/Tensor/Gather.cpp   |   2 +-
 src/Conversion/ONNXToMhlo/Tensor/Shape.cpp    |   2 +-
 src/Conversion/ONNXToMhlo/Tensor/Slice.cpp    |   2 +-
 src/Conversion/ONNXToMhlo/Tensor/Tile.cpp     |   2 +-
 src/Dialect/ONNX/ONNXOps/NN/Conv.cpp          | 157 +++++++++++--
 src/Dialect/ONNX/ONNXOps/ShapeHelper.cpp      |   1 +
 src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp      |  21 ++
 src/Transform/ONNX/ConstProp.td               |   2 +-
 .../conversion/onnx_to_mhlo/Math/Clip.mlir    |  58 +++++
 .../onnx_to_mhlo/Math/Elementwise.mlir        |  45 +++-
 .../onnx_to_mhlo/NN/ConvTranspose.mlir        |  81 +++++++
 .../conversion/onnx_to_mhlo/NN/Pooling.mlir   |  98 +++++++-
 .../onnx_to_mhlo/Tensor/Concat.mlir           |   8 +-
 test/mlir/onnx/onnx_shape_inference.mlir      |  14 +-
 23 files changed, 911 insertions(+), 53 deletions(-)
 create mode 100644 src/Conversion/ONNXToMhlo/Math/Clip.cpp
 create mode 100644 src/Conversion/ONNXToMhlo/NN/ConvTranspose.cpp
 create mode 100644 test/mlir/conversion/onnx_to_mhlo/Math/Clip.mlir
 create mode 100644 test/mlir/conversion/onnx_to_mhlo/NN/ConvTranspose.mlir

diff --git a/src/Conversion/ONNXToMhlo/CMakeLists.txt b/src/Conversion/ONNXToMhlo/CMakeLists.txt
index 018a2904..5d9bcedb 100644
--- a/src/Conversion/ONNXToMhlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToMhlo/CMakeLists.txt
@@ -25,11 +25,13 @@ add_onnx_mlir_library(OMONNXToMhlo
   ConvertONNXToMhlo.cpp
   ONNXToMhloCommon.cpp
 
+  Math/Clip.cpp
   Math/Elementwise.cpp
   Math/Gemm.cpp
   Math/MatMul.cpp
   Math/Reduction.cpp
   NN/Conv.cpp
+  NN/ConvTranspose.cpp
   NN/Normalization.cpp
   NN/Pooling.cpp
   Tensor/ArgMax.cpp
diff --git a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
index 28140cfa..ad631a17 100644
--- a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
+++ b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
@@ -22,12 +22,14 @@ namespace onnx_mlir {
 void populateONNXToMhloConversionPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
   // Math
+  populateLoweringONNXClipOpToMhloPattern(patterns, ctx);
   populateLoweringONNXElementwiseOpToMhloPattern(patterns, ctx);
   populateLoweringONNXGemmOpToMhloPattern(patterns, ctx);
   populateLoweringONNXMatMulOpToMhloPattern(patterns, ctx);
   populateLoweringONNXReductionOpToMhloPattern(patterns, ctx);
   // Neural network
   populateLoweringONNXConvOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXConvTransposeOpToMhloPattern(patterns, ctx);
   populateLoweringONNXNormalizationOpToMhloPattern(patterns, ctx);
   populateLoweringONNXPoolingOpToMhloPattern(patterns, ctx);
   // Tensor
diff --git a/src/Conversion/ONNXToMhlo/Math/Clip.cpp b/src/Conversion/ONNXToMhlo/Math/Clip.cpp
new file mode 100644
index 00000000..0589c1eb
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Math/Clip.cpp
@@ -0,0 +1,119 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===----------------- Clip.cpp - Lowering Clip Op ------------------------===//
+//
+// Copyright 2022
+//
+// =============================================================================
+//
+// This file lowers the ONNX Clip Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+//===----------------------------------------------------------------------===//
+// Scalar unary ops for lowering ONNXClipOp
+//===----------------------------------------------------------------------===//
+
+struct ONNXClipOpLoweringToMhlo : public ConversionPattern {
+  ONNXClipOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXClipOp::getOperationName(), 1, ctx) {}
+
+  Attribute getLowestAttrByType(
+      ConversionPatternRewriter &rewriter, Type type) const {
+    Attribute constant = nullptr;
+    TypeSwitch<Type>(type)
+        .Case<FloatType>([&](FloatType type) {
+          auto &semantics = type.getFloatSemantics();
+          constant = rewriter.getFloatAttr(
+              type, APFloat::getLargest(semantics, /*negative=*/true));
+        })
+        .Case<IntegerType>([&](IntegerType type) {
+          unsigned width = type.getWidth();
+          bool isBool = (width == 1);
+          if (type.isUnsigned() || isBool)
+            constant = rewriter.getIntegerAttr(type, APInt::getMinValue(width));
+          else
+            constant =
+                rewriter.getIntegerAttr(type, APInt::getSignedMinValue(width));
+        })
+        .Default([](Type) { llvm_unreachable("unsupported element type"); });
+    assert(constant != nullptr && "Expecting valid constant value");
+    return constant;
+  }
+
+  Attribute getMaxAttrByType(
+      ConversionPatternRewriter &rewriter, Type type) const {
+    Attribute constant = nullptr;
+    TypeSwitch<Type>(type)
+        .Case<FloatType>([&](FloatType type) {
+          auto &semantics = type.getFloatSemantics();
+          constant = rewriter.getFloatAttr(
+              type, APFloat::getLargest(semantics, /*negative=*/false));
+        })
+        .Case<IntegerType>([&](IntegerType type) {
+          unsigned width = type.getWidth();
+          bool isBool = (width == 1);
+          if (type.isUnsigned() || isBool)
+            constant = rewriter.getIntegerAttr(type, APInt::getMaxValue(width));
+          else
+            constant =
+                rewriter.getIntegerAttr(type, APInt::getSignedMaxValue(width));
+        })
+        .Default([](Type) { llvm_unreachable("unsupported element type"); });
+    assert(constant != nullptr && "Expecting valid constant value");
+    return constant;
+  }
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    ONNXClipOp clipOp = cast<ONNXClipOp>(op);
+
+    ONNXClipOpAdaptor operandAdaptor(operands);
+    ONNXClipOpShapeHelper shapeHelper(&clipOp);
+    auto shapeComputed = shapeHelper.computeShape(operandAdaptor);
+    assert(succeeded(shapeComputed) && "Could not compute output shape");
+
+    Value input = operandAdaptor.input();
+    Value min = operandAdaptor.min();
+    Value max = operandAdaptor.max();
+
+    Type outputType = *op->result_type_begin();
+    assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+    ShapedType outputShapedType = outputType.cast<ShapedType>();
+    Type elemType = outputShapedType.getElementType();
+
+    if (min.getType().isa<NoneType>()) {
+      min = rewriter.create<mhlo::ConstantOp>(
+          loc, DenseElementsAttr::get(mlir::RankedTensorType::get({}, elemType),
+                   getLowestAttrByType(rewriter, elemType)));
+    }
+    if (max.getType().isa<NoneType>()) {
+      max = rewriter.create<mhlo::ConstantOp>(
+          loc, DenseElementsAttr::get(mlir::RankedTensorType::get({}, elemType),
+                   getMaxAttrByType(rewriter, elemType)));
+    }
+
+    Value result =
+        rewriter.create<mhlo::ClampOp>(loc, outputType, min, input, max);
+    rewriter.replaceOp(op, result);
+    return success();
+  }
+};
+
+void populateLoweringONNXClipOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXClipOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
index 1c3d275b..67497fd5 100644
--- a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
@@ -55,6 +55,16 @@ struct MhloDialectOp<ONNXExpOp> {
   using Op = mhlo::ExpOp;
 };
 
+template <>
+struct MhloDialectOp<ONNXLogOp> {
+  using Op = mhlo::LogOp;
+};
+
+template <>
+struct MhloDialectOp<ONNXMaxOp> {
+  using Op = mhlo::MaxOp;
+};
+
 template <>
 struct MhloDialectOp<ONNXMulOp> {
   using Op = mhlo::MulOp;
@@ -80,6 +90,11 @@ struct MhloDialectOp<ONNXSubOp> {
   using Op = mhlo::SubtractOp;
 };
 
+template <>
+struct MhloDialectOp<ONNXTanhOp> {
+  using Op = mhlo::TanhOp;
+};
+
 namespace {
 
 template <typename ONNXOp>
@@ -160,6 +175,34 @@ struct ONNXElementwiseUnaryOpLoweringToMhlo<ONNXReluOp>
   }
 };
 
+// ONNXLeakyReluOp(x) = alpha * x if x < 0 else x.
+template <>
+struct ONNXElementwiseUnaryOpLoweringToMhlo<ONNXLeakyReluOp>
+    : public ConversionPattern {
+  ONNXElementwiseUnaryOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXLeakyReluOp::getOperationName(), 1, ctx) {}
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    ONNXLeakyReluOpAdaptor adaptor(operands, op->getAttrDictionary());
+    Value inp = adaptor.X();
+    llvm::APFloat alpha = adaptor.alpha();
+    ShapedType inpType = inp.getType().dyn_cast_or_null<ShapedType>();
+    if (inpType == nullptr)
+      return failure();
+    Type resultType = *op->result_type_begin();
+    Value alphaVal = getShapedFloat(loc, rewriter, alpha, inp);
+    Value leakyActivationVal = rewriter.create<mhlo::MulOp>(loc, inp, alphaVal);
+    Value broadcastedZero = getShapedZero(loc, rewriter, inp);
+    Value compareGtZero = rewriter.create<mhlo::CompareOp>(
+        loc, inp, broadcastedZero, mhlo::ComparisonDirection::GT);
+    Value resultOp = rewriter.create<mhlo::SelectOp>(
+        loc, resultType, compareGtZero, inp, leakyActivationVal);
+    rewriter.replaceOp(op, resultOp);
+    return success();
+  }
+};
+
 template <>
 struct ONNXElementwiseUnaryOpLoweringToMhlo<ONNXCastOp>
     : public ConversionPattern {
@@ -268,9 +311,12 @@ void populateLoweringONNXElementwiseOpToMhloPattern(
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXCeilOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXCosOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXExpOp>,
+      ONNXElementwiseUnaryOpLoweringToMhlo<ONNXLeakyReluOp>,
+      ONNXElementwiseUnaryOpLoweringToMhlo<ONNXLogOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXSigmoidOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXSqrtOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXReluOp>,
+      ONNXElementwiseUnaryOpLoweringToMhlo<ONNXTanhOp>,
       ONNXElementwiseCompareBinaryOpLoweringToMhlo<ONNXEqualOp>,
       ONNXElementwiseCompareBinaryOpLoweringToMhlo<ONNXGreaterOp>,
       ONNXElementwiseCompareBinaryOpLoweringToMhlo<ONNXGreaterOrEqualOp>,
@@ -280,6 +326,7 @@ void populateLoweringONNXElementwiseOpToMhloPattern(
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXAddOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXAndOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXDivOp>,
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMaxOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMulOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXSubOp>>(ctx);
 }
diff --git a/src/Conversion/ONNXToMhlo/NN/ConvTranspose.cpp b/src/Conversion/ONNXToMhlo/NN/ConvTranspose.cpp
new file mode 100644
index 00000000..e83f45b5
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/NN/ConvTranspose.cpp
@@ -0,0 +1,216 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===----------- ConvTranspose.cpp - Lowering ConvTranspose Op ------------===//
+//
+// Copyright 2022
+//
+// =============================================================================
+//
+// This file lowers ONNX ConvTranspose Operators to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXConvTransposeOpLoweringToMhlo : public ConversionPattern {
+  ONNXConvTransposeOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(
+            mlir::ONNXConvTransposeOp::getOperationName(), 1, ctx) {}
+
+  Value reshapeFilter(ConversionPatternRewriter &rewriter, Location loc,
+      Value filterOperand, int64_t groupNum, int rank) const {
+    assert(isRankedShapedType(filterOperand.getType()) &&
+           "Expected Ranked ShapedType");
+    ShapedType filterType = filterOperand.getType().cast<ShapedType>();
+    assert(filterType.hasStaticShape() && "Expected static shape for filter");
+    ArrayRef<int64_t> filterShape = filterType.getShape();
+    Type elemType = filterType.getElementType();
+
+    // 1. [IC, OC//G, H, W, ...] => [G, IC//G, OC//G, H, W, ...]
+    SmallVector<int64_t> newFilterShape(filterShape.begin(), filterShape.end());
+    newFilterShape[0] /= groupNum;
+    newFilterShape.insert(newFilterShape.begin(), groupNum);
+    filterOperand = rewriter.create<mhlo::ReshapeOp>(
+        loc, RankedTensorType::get(newFilterShape, elemType), filterOperand);
+
+    // 2. [G, IC//G, OC//G, H, W, ...] => [G, OC//G, IC//G, H, W, ...]
+    llvm::SmallVector<int64_t> transposeDims(rank + 1);
+    for (int64_t i = 0; i <= rank; i++)
+      transposeDims[i] = i;
+    std::swap(transposeDims[1], transposeDims[2]);
+    filterOperand = rewriter.create<mhlo::TransposeOp>(
+        loc, filterOperand, rewriter.getI64TensorAttr(transposeDims));
+
+    // 3. [G, OC//G, IC//G, H, W, ...] => [OC, IC//G, H, W, ...]
+    std::swap(newFilterShape[1], newFilterShape[2]);
+    newFilterShape.erase(newFilterShape.begin());
+    newFilterShape[0] *= groupNum;
+    filterOperand = rewriter.create<mhlo::ReshapeOp>(
+        loc, RankedTensorType::get(newFilterShape, elemType), filterOperand);
+
+    return filterOperand;
+  }
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+
+    ONNXConvTransposeOpAdaptor operandAdaptor(
+        operands, op->getAttrDictionary());
+    ONNXConvTransposeOp convOp = llvm::dyn_cast<ONNXConvTransposeOp>(op);
+    Location loc = op->getLoc();
+
+    ONNXConvTransposeOpShapeHelper shapeHelper(&convOp);
+    LogicalResult shapecomputed = shapeHelper.computeShape(operandAdaptor);
+    assert(succeeded(shapecomputed) && "Could not compute output shape");
+
+    llvm::SmallVector<IndexExpr, 2> kernelShape = shapeHelper.kernelShape;
+    llvm::SmallVector<int64_t, 2> strides = shapeHelper.strides;
+    llvm::SmallVector<int64_t, 2> dilations = shapeHelper.dilations;
+    llvm::SmallVector<int64_t, 2> outputPadding = shapeHelper.outputPadding;
+    bool needHandleOutputPadding = std::any_of(outputPadding.begin(),
+        outputPadding.end(), [](int64_t i) { return i != 0; });
+    DimsExpr outputDims = shapeHelper.dimsForOutput();
+    int64_t outputRank = shapeHelper.dimsForOutput().size();
+
+    Value inputOperand = operandAdaptor.X();
+    Value filterOperand = operandAdaptor.W();
+    Value biasOperand = operandAdaptor.B();
+    bool hasBias = !biasOperand.getType().isa<NoneType>();
+    int64_t groupNum = convOp.group();
+
+    assert(isRankedShapedType(inputOperand.getType()) &&
+           "Expected Ranked ShapedType");
+    ShapedType inputType = inputOperand.getType().cast<ShapedType>();
+    Type elemType = inputType.getElementType();
+    // Onnx Input is NCHW
+    int64_t spatialOffset = 2;
+    int64_t rank = inputType.getRank();
+    assert(rank == outputRank && "Input/Output rank should be equal");
+    int64_t kernelSize = kernelShape.size();
+
+    Type outputType = *op->result_type_begin();
+    Type convOutputType = outputType;
+    if (needHandleOutputPadding) {
+      assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+      auto finalShape = outputType.cast<ShapedType>().getShape();
+      SmallVector<int64_t> convOutputShape(
+          finalShape.begin(), finalShape.end());
+      for (int i = spatialOffset; i < rank; ++i) {
+        if (finalShape[i] == ShapedType::kDynamicSize)
+          continue;
+        convOutputShape[i] = finalShape[i] - outputPadding[i - 2];
+      }
+      convOutputType = RankedTensorType::get(convOutputShape, elemType);
+    }
+
+    SmallVector<int64_t> spatialDimensions;
+    for (int64_t i = spatialOffset; i < rank; i++) {
+      spatialDimensions.push_back(i);
+    }
+    SmallVector<int64_t> kernelDimensions;
+    for (int64_t i = spatialOffset; i < spatialOffset + kernelSize; i++) {
+      kernelDimensions.push_back(i);
+    }
+
+    // paddings
+    DimsExpr pads = shapeHelper.pads;
+    int64_t spatialRank = rank - spatialOffset;
+    SmallVector<int64_t> flattenPaddings;
+    // currently only support static spatial dims
+    if (!IndexExpr::isLiteral(kernelShape) || !IndexExpr::isLiteral(pads))
+      return failure();
+    for (int64_t i = 0; i < spatialRank; i++) {
+      flattenPaddings.push_back(
+          dilations[i] * (kernelShape[i].getLiteral() - 1) -
+          pads[i].getLiteral());
+      flattenPaddings.push_back(
+          dilations[i] * (kernelShape[i].getLiteral() - 1) -
+          pads[i + spatialRank].getLiteral());
+    }
+
+    mhlo::ConvDimensionNumbersAttr dimension_numbers =
+        mhlo::ConvDimensionNumbersAttr::get(rewriter.getContext(), 0, 1,
+            spatialDimensions, 1, 0, kernelDimensions, 0, 1, spatialDimensions);
+
+    // Reverse and transpose filterOperand
+    filterOperand = rewriter.create<mhlo::ReverseOp>(
+        loc, filterOperand, rewriter.getI64TensorAttr(spatialDimensions));
+    if (groupNum > 1)
+      filterOperand =
+          reshapeFilter(rewriter, loc, filterOperand, groupNum, rank);
+    else {
+      // Transpose filterOperand from [i, o, ...] to [o, i, ...]
+      llvm::SmallVector<int64_t> transposeDims(rank);
+      for (int64_t i = 0; i < rank; i++)
+        transposeDims[i] = i;
+      std::swap(transposeDims[0], transposeDims[1]);
+      filterOperand = rewriter.create<mhlo::TransposeOp>(
+          loc, filterOperand, rewriter.getI64TensorAttr(transposeDims));
+    }
+
+    Value convResult = rewriter.create<mhlo::ConvolutionOp>(loc, convOutputType,
+        inputOperand, filterOperand,
+        rewriter.getI64VectorAttr(SmallVector<int64_t>(spatialRank, 1)),
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({spatialRank, 2}, rewriter.getI64Type()),
+            flattenPaddings),
+        rewriter.getI64VectorAttr(strides),
+        rewriter.getI64VectorAttr(dilations), nullptr, dimension_numbers,
+        groupNum, 1, nullptr);
+
+    Value padResult;
+    if (!needHandleOutputPadding) {
+      padResult = convResult;
+    } else {
+      SmallVector<int64_t> edgePaddingLowVec(rank, 0);
+      SmallVector<int64_t> edgePaddingHighVec(rank, 0);
+      SmallVector<int64_t> interiorPaddingVec(rank, 0);
+      std::copy(outputPadding.begin(), outputPadding.end(),
+          edgePaddingHighVec.begin() + 2);
+      Value zeroPaddingValue = rewriter.create<mhlo::ConstantOp>(
+          loc, DenseElementsAttr::get(mlir::RankedTensorType::get({}, elemType),
+                   rewriter.getZeroAttr(elemType)));
+      mlir::DenseIntElementsAttr edgePaddingLow =
+          rewriter.getI64VectorAttr(edgePaddingLowVec);
+      mlir::DenseIntElementsAttr edgePaddingHigh =
+          rewriter.getI64VectorAttr(edgePaddingHighVec);
+      mlir::DenseIntElementsAttr interiorPadding =
+          rewriter.getI64VectorAttr(interiorPaddingVec);
+      padResult = rewriter.create<mhlo::PadOp>(loc, outputType, convResult,
+          zeroPaddingValue, edgePaddingLow, edgePaddingHigh, interiorPadding);
+    }
+
+    Value addBiasResult;
+    if (!hasBias) {
+      addBiasResult = padResult;
+    } else {
+      Value finalB;
+      Value resultShape = rewriter.create<shape::ShapeOfOp>(loc, padResult);
+      finalB = rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc, outputType,
+          biasOperand, resultShape, rewriter.getI64TensorAttr({1}));
+      addBiasResult = rewriter.create<mhlo::AddOp>(loc, padResult, finalB);
+    }
+
+    rewriter.replaceOp(op, addBiasResult);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXConvTransposeOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXConvTransposeOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/NN/Pooling.cpp b/src/Conversion/ONNXToMhlo/NN/Pooling.cpp
index f57371a0..63d99a7b 100644
--- a/src/Conversion/ONNXToMhlo/NN/Pooling.cpp
+++ b/src/Conversion/ONNXToMhlo/NN/Pooling.cpp
@@ -21,6 +21,27 @@ namespace onnx_mlir {
 
 namespace {
 
+static Value createInitialValueForPoolingOp(
+    Operation *op, Type elemType, ConversionPatternRewriter &rewriter) {
+  Location loc = op->getLoc();
+  if (isa<ONNXMaxPoolSingleOutOp>(op)) {
+    // returns negative infinity
+    return rewriter.create<mhlo::ConstantOp>(
+        loc, rewriter.getFloatAttr(elemType,
+                 APFloat::getInf(elemType.cast<FloatType>().getFloatSemantics(),
+                     /*isNegative=*/true)));
+  }
+  if (isa<ONNXAveragePoolOp>(op)) {
+    // returns negative infinity
+    return rewriter.create<mhlo::ConstantOp>(loc,
+        rewriter.getFloatAttr(elemType,
+            APFloat::getZero(elemType.cast<FloatType>().getFloatSemantics(),
+                /*isNegative=*/false)));
+  }
+  op->emitError("unimplemented lowering for onnx pooling op\n");
+  return nullptr;
+}
+
 // Builds body for reduce op by using the template binary op as the
 // reducer op.
 template <typename Op>
@@ -37,7 +58,7 @@ void buildReduceBody(Type elementType, Region *body, OpBuilder *builder) {
 }
 
 template <typename Op>
-void buildReduceBodyFor(Type elementType, Region *body, OpBuilder *builder) {}
+void buildReduceBodyFor(Type elementType, Region *body, OpBuilder *builder);
 
 template <>
 void buildReduceBodyFor<ONNXMaxPoolSingleOutOp>(
@@ -45,6 +66,12 @@ void buildReduceBodyFor<ONNXMaxPoolSingleOutOp>(
   buildReduceBody<mhlo::MaxOp>(elementType, body, builder);
 }
 
+template <>
+void buildReduceBodyFor<ONNXAveragePoolOp>(
+    Type elementType, Region *body, OpBuilder *builder) {
+  buildReduceBody<mhlo::AddOp>(elementType, body, builder);
+}
+
 static DenseIntElementsAttr getDenseIntElementsAttr(
     SmallVectorImpl<int64_t> &values, Builder *builder) {
   return DenseIntElementsAttr::get(
@@ -107,10 +134,9 @@ struct ONNXPoolOpLoweringToMhlo : public ConversionPattern {
     int64_t rank = inputType.getRank();
     int64_t ceilMode = poolOp.ceil_mode();
 
-    Value negInfinity = rewriter.create<mhlo::ConstantOp>(
-        loc, rewriter.getFloatAttr(elemType,
-                 APFloat::getInf(elemType.cast<FloatType>().getFloatSemantics(),
-                     /*isNegative=*/true)));
+    Value initVal = createInitialValueForPoolingOp(op, elemType, rewriter);
+    if (initVal == nullptr)
+      return failure();
 
     // paddings
     llvm::SmallVector<IndexExpr, 4> pads = shapeHelper.pads;
@@ -139,7 +165,7 @@ struct ONNXPoolOpLoweringToMhlo : public ConversionPattern {
     padVector(dilations, spatialOffset, 1);
     mhlo::ReduceWindowOp reduce =
         rewriter.create<mhlo::ReduceWindowOp>(loc, outputType, inputOperand,
-            negInfinity, getKernelAttr(kernelShape, &rewriter, spatialOffset),
+            initVal, getKernelAttr(kernelShape, &rewriter, spatialOffset),
             getDenseIntElementsAttr(strides, &rewriter),
             /*base_dilations=*/DenseIntElementsAttr(),
             /*window_dilations=*/getDenseIntElementsAttr(dilations, &rewriter),
@@ -147,7 +173,42 @@ struct ONNXPoolOpLoweringToMhlo : public ConversionPattern {
                 RankedTensorType::get({rank, 2}, rewriter.getI64Type()),
                 flattenPaddings));
     buildReduceBodyFor<PoolOp>(elemType, &reduce.getBody(), &rewriter);
-    rewriter.replaceOp(op, reduce->getResults());
+
+    if (isa<ONNXAveragePoolOp>(op)) {
+      Value reduceResult = reduce.getResult(0);
+      int64_t countIncludePad =
+          llvm::cast<ONNXAveragePoolOp>(op).count_include_pad();
+      if (countIncludePad) {
+        // Use kernel size as the divisor
+        int64_t kernelSize = 1;
+        for (int64_t i = 0; i < spatialRank; i++) {
+          kernelSize *= kernelShape[i].getLiteral();
+        }
+        Value divisor = getShapedFloat(loc, rewriter, kernelSize, reduceResult);
+        Value divResult = rewriter.create<mhlo::DivOp>(
+            loc, outputType, reduceResult, divisor);
+        rewriter.replaceOp(op, divResult);
+      } else {
+        // Use another mhlo.ReduceWindowOp to get the divisor
+        Value one = getShapedFloat(loc, rewriter, 1.0, inputOperand);
+        mhlo::ReduceWindowOp reduceDivisor =
+            rewriter.create<mhlo::ReduceWindowOp>(loc, outputType, one, initVal,
+                getKernelAttr(kernelShape, &rewriter, spatialOffset),
+                getDenseIntElementsAttr(strides, &rewriter),
+                /*base_dilations=*/DenseIntElementsAttr(),
+                /*window_dilations=*/getDenseIntElementsAttr(dilations, &rewriter),
+                DenseIntElementsAttr::get(
+                    RankedTensorType::get({rank, 2}, rewriter.getI64Type()),
+                    flattenPaddings));
+        buildReduceBodyFor<ONNXAveragePoolOp>(
+            elemType, &reduceDivisor.getBody(), &rewriter);
+        Value divResult = rewriter.create<mhlo::DivOp>(
+            loc, outputType, reduceResult, reduceDivisor.getResult(0));
+        rewriter.replaceOp(op, divResult);
+      }
+    } else {
+      rewriter.replaceOp(op, reduce->getResults());
+    }
     return success();
   }
 };
@@ -158,6 +219,8 @@ void populateLoweringONNXPoolingOpToMhloPattern(
     RewritePatternSet &patterns, MLIRContext *ctx) {
   patterns.insert<ONNXPoolOpLoweringToMhlo<ONNXMaxPoolSingleOutOp,
       ONNXMaxPoolSingleOutOpAdaptor, ONNXMaxPoolSingleOutOpShapeHelper>>(ctx);
+  patterns.insert<ONNXPoolOpLoweringToMhlo<ONNXAveragePoolOp,
+      ONNXAveragePoolOpAdaptor, ONNXAveragePoolOpShapeHelper>>(ctx);
 }
 
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
index 2076323d..c27de18b 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
@@ -115,6 +115,8 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     ConversionPatternRewriter &rewriter, Location loc, int64_t outputRank);
 
 // `Math` directory methods:
+void populateLoweringONNXClipOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXElementwiseOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXGemmOpToMhloPattern(
@@ -126,6 +128,8 @@ void populateLoweringONNXReductionOpToMhloPattern(
 // `NN` directory methods:
 void populateLoweringONNXConvOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXConvTransposeOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXNormalizationOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXPoolingOpToMhloPattern(
diff --git a/src/Conversion/ONNXToMhlo/Tensor/ArgMax.cpp b/src/Conversion/ONNXToMhlo/Tensor/ArgMax.cpp
index bb6e2c20..b07cb843 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/ArgMax.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/ArgMax.cpp
@@ -4,7 +4,7 @@
 
 //===---------------- ArgMax.cpp - Lowering ArgMax Op -------------------===//
 //
-// Copyright 2021-2022 The IBM Research Authors.
+// Copyright 2022
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
index 8437aeef..dabc9076 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
@@ -4,7 +4,7 @@
 
 //===----------------Expand.cpp - Lowering Expand Op----------------------=== //
 //
-// Copyright 2020-2022 The IBM Research Authors.
+// Copyright 2022
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Gather.cpp b/src/Conversion/ONNXToMhlo/Tensor/Gather.cpp
index 3d5c8353..8670f4db 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Gather.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Gather.cpp
@@ -4,7 +4,7 @@
 
 //===---------------- Gather.cpp - Lowering Gather Op ---------------------===//
 //
-// Copyright 2020-2022 The IBM Research Authors.
+// Copyright 2022
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Shape.cpp b/src/Conversion/ONNXToMhlo/Tensor/Shape.cpp
index 1e4beaf5..7ae15654 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Shape.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Shape.cpp
@@ -4,7 +4,7 @@
 
 //===----------------- Shape.cpp - Lowering Shape Op ----------------------===//
 //
-// Copyright 2020-2022 The IBM Research Authors.
+// Copyright 2022
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Slice.cpp b/src/Conversion/ONNXToMhlo/Tensor/Slice.cpp
index 01184380..7228eb0d 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Slice.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Slice.cpp
@@ -4,7 +4,7 @@
 
 //===----------------Slice.cpp - Lowering Slice Op----------------------=== //
 //
-// Copyright 2020-2022 The IBM Research Authors.
+// Copyright 2022
 //
 // =============================================================================
 //
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Tile.cpp b/src/Conversion/ONNXToMhlo/Tensor/Tile.cpp
index 255b347b..4749ed49 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Tile.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Tile.cpp
@@ -4,7 +4,7 @@
 
 //===----------------Tile.cpp - Lowering Tile Op----------------------=== //
 //
-// Copyright 2020-2022 The IBM Research Authors.
+// Copyright 2022
 //
 // =============================================================================
 //
diff --git a/src/Dialect/ONNX/ONNXOps/NN/Conv.cpp b/src/Dialect/ONNX/ONNXOps/NN/Conv.cpp
index 2efca418..06a519f7 100644
--- a/src/Dialect/ONNX/ONNXOps/NN/Conv.cpp
+++ b/src/Dialect/ONNX/ONNXOps/NN/Conv.cpp
@@ -330,6 +330,132 @@ LogicalResult ONNXConvOpShapeHelper::computeShape(
       op->kernel_shape(), op->pads(), op->strides(), op->dilations());
 }
 
+ONNXConvTransposeOpShapeHelper::ONNXConvTransposeOpShapeHelper(
+    ONNXConvTransposeOp *newOp, IndexExprScope *inScope)
+    : ONNXOpShapeHelper<ONNXConvTransposeOp>(
+          newOp, newOp->getOperation()->getNumResults(), inScope) {}
+
+ONNXConvTransposeOpShapeHelper::ONNXConvTransposeOpShapeHelper(
+    ONNXConvTransposeOp *newOp, OpBuilder *rewriter,
+    ArrayValueIndexCapture::GetDenseVal fGetDenseVal,
+    ArrayValueIndexCapture::LoadVal fLoadVal, IndexExprScope *inScope)
+    : ONNXOpShapeHelper<ONNXConvTransposeOp>(newOp,
+          newOp->getOperation()->getNumResults(), rewriter, fGetDenseVal,
+          fLoadVal, inScope) {}
+
+LogicalResult ONNXConvTransposeOpShapeHelper::computeShape(
+    ONNXConvTransposeOpAdaptor operandAdaptor) {
+  Value filterValue = operandAdaptor.W();
+  Optional<ArrayAttr> kernelShapeOpt = op->kernel_shape();
+  Optional<ArrayAttr> padOpt = op->pads();
+  Optional<ArrayAttr> strideOpt = op->strides();
+  Optional<ArrayAttr> dilationOpt = op->dilations();
+  Optional<ArrayAttr> outputPaddingOpt = op->output_padding();
+  Optional<ArrayAttr> outputShapeOpt = op->output_shape();
+  int64_t groupNum = op->group();
+  llvm::StringRef autoPad = op->auto_pad();
+
+  // Shape inference indicated by passing a null rewriter pointer.
+  // Basic information.
+  Value xValue = (Value)operandAdaptor.X();
+  int64_t rank = xValue.getType().cast<ShapedType>().getRank();
+  int64_t spatialOffset = 2;
+  int64_t spatialRank = rank - spatialOffset;
+
+  MemRefBoundsIndexCapture XBounds(operandAdaptor.X());
+  MemRefBoundsIndexCapture WBounds(filterValue);
+
+  // Fill the stride, dilation, kernel.
+  for (int i = 0; i < spatialRank; ++i) {
+    // Strides, default 1.
+    strides.emplace_back(
+        strideOpt.has_value() ? ArrayAttrIntVal(strideOpt, i) : 1);
+    // Dilations, default 1.
+    dilations.emplace_back(
+        dilationOpt.has_value() ? ArrayAttrIntVal(dilationOpt, i) : 1);
+    // Kernel shape from attribute, default from Weight's spatial dims.
+    if (kernelShapeOpt.has_value()) {
+      kernelShape.emplace_back(
+          LiteralIndexExpr(ArrayAttrIntVal(kernelShapeOpt, i)));
+    } else {
+      kernelShape.emplace_back(WBounds.getSymbol(i + spatialOffset));
+    }
+    // Output Padding, default 0.
+    outputPadding.emplace_back(outputPaddingOpt.has_value()
+                                   ? ArrayAttrIntVal(outputPaddingOpt, i)
+                                   : 0);
+  }
+  // Pads, at this stage a given compile-time literal or default 0.
+  for (int i = 0; i < 2 * spatialRank; ++i) {
+    int64_t p = padOpt.has_value() ? ArrayAttrIntVal(padOpt, i) : 0;
+    pads.emplace_back(LiteralIndexExpr(p));
+  }
+
+  // Handle output size: start by inserting batch size and output channels.
+  DimsExpr outputDims;
+  outputDims.emplace_back(XBounds.getDim(0));
+  outputDims.emplace_back(
+      WBounds.getDim(1) *
+      LiteralIndexExpr(groupNum)); // CO may be different from CI.
+
+  LiteralIndexExpr zeroIE(0);
+  LiteralIndexExpr oneIE(1);
+  for (int i = 0; i < spatialRank; ++i) {
+    IndexExpr I = XBounds.getDim(i + spatialOffset);
+    IndexExpr K = kernelShape[i];
+    LiteralIndexExpr d(dilations[i]);
+    LiteralIndexExpr s(strides[i]);
+    LiteralIndexExpr outPad(outputPadding[i]);
+
+    IndexExpr t0 = K - oneIE;
+    IndexExpr kdTerm = t0 * d + oneIE; // (k - 1) * d + 1
+    IndexExpr t1 = I - oneIE;
+    if (outputShapeOpt.has_value()) {
+      // Set output dim
+      LiteralIndexExpr O(ArrayAttrIntVal(outputShapeOpt, i));
+      outputDims.emplace_back(O);
+      // Set pads
+      // P = max(0, s * (I - 1) + outPad + ((K - 1) * d + 1) - O);
+      IndexExpr pSum = IndexExpr::max(zeroIE, s * t1 + outPad + kdTerm - O);
+      IndexExpr pSmall = pSum.floorDiv(2);
+      if (autoPad == "SAME_UPPER") {
+        pads[i] = pSmall;
+        pads[i + spatialRank] = pSum - pSmall;
+      } else if (autoPad == "NOTSET" || autoPad == "VALID" ||
+                 autoPad == "SAME_LOWER") {
+        pads[i] = pSum - pSmall;
+        pads[i + spatialRank] = pSmall;
+      } else {
+        return op->emitError("auto_pad of unknown/unsupported value");
+      }
+    } else {
+      // Set pads
+      IndexExpr pSum;
+      if (autoPad == "NOTSET") {
+        pSum = pads[i] + pads[i + spatialRank]; // Sum both pads.
+        // pads already set, nothing more to do.
+      } else if (autoPad == "VALID") {
+        pSum = zeroIE;
+        pads[i] = zeroIE;
+        pads[i + spatialRank] = zeroIE;
+      } else if (autoPad == "SAME_UPPER" || autoPad == "SAME_LOWER") {
+        return op->emitError(
+            "auto_pad of SAME_UPPER/SAME_LOWER not unsupported yet");
+      } else {
+        return op->emitError("auto_pad of unknown/unsupported value");
+      }
+      // Set output dim
+      // O = s * (I - 1) + outPad + ((K - 1) * d + 1) - P
+      IndexExpr O = s * t1 + outPad + kdTerm - pSum;
+      outputDims.emplace_back(O); // Set output dim
+    }
+  }
+
+  // Set type for the first output.
+  dimsForOutput() = outputDims;
+  return success();
+}
+
 } // namespace onnx_mlir
 
 //===----------------------------------------------------------------------===//
@@ -490,7 +616,7 @@ static void insertConvTransposePads(SmallVectorImpl<int64_t> &inferedPads,
   inferedPads.resize(spatialRank * 2);
   for (unsigned int i = 0; i < spatialRank; ++i) {
     auto inputSize = xShape[spatialOffset + i];
-    auto outputSize = ArrayAttrIntVal(outputShapeOpt, spatialOffset + i);
+    auto outputSize = ArrayAttrIntVal(outputShapeOpt, i);
     auto kernelSize = ArrayAttrIntVal(kernelShape, i);
     if (dilationsOpt.has_value())
       dilationVal = ArrayAttrIntVal(dilationsOpt, i);
@@ -614,37 +740,32 @@ LogicalResult ONNXConvTransposeOp::inferShapes(
   auto stridesOpt = strides();
   auto padsOpt = pads();
   auto outputPads = output_padding();
-  auto outputShape = output_shape();
+  auto outputShape = output_shape();  // spatial dimensions only
+
   llvm::SmallVector<int64_t, 4> outputShapeFinal;
+  // First two output dimensions consist of the number of batches and the
+  // number of kernels being applied.
+  // Insert batch size.
+  outputShapeFinal.emplace_back(xShape[0]);
+  // Insert number of filters being applied (number of output channels *
+  // groups).
+  outputShapeFinal.emplace_back(outChannels);
 
+  // Compute and insert spatial dims for outputShapeFinal.
   if (outputShape.has_value()) {
-    if (xShape[0] != ArrayAttrIntVal(outputShape, 0)) {
-      return emitOpError("mismatch in batch size");
-    }
-    if (outChannels != ArrayAttrIntVal(outputShape, 1)) {
-      return emitOpError("mismatch in output channel size");
-    }
     SmallVector<int64_t, 4> inferedPads;
     // Determine padding values based on output shape.
     auto autoPad = auto_pad();
     insertConvTransposePads(inferedPads, autoPad, xShape, kernelShape, padsOpt,
         stridesOpt, outputPads, outputShape, dilationsOpt);
     padsAttr(builder.getI64ArrayAttr(inferedPads));
-    for (uint64_t i = 0; i < xShape.size(); ++i) {
+    auto spatialRank = ArrayAttrSize(kernelShape);
+    for (uint64_t i = 0; i < spatialRank; ++i) {
       outputShapeFinal.emplace_back(ArrayAttrIntVal(outputShape, i));
     }
   } else {
-    // First two output dimensions consist of the number of batches and the
-    // number of kernels being applied.
-    // Insert batch size.
-    outputShapeFinal.emplace_back(xShape[0]);
-    // Insert number of filters being applied (number of output channels *
-    // groups).
-    outputShapeFinal.emplace_back(outChannels);
-    // Compute and insert spatial dims.
     insertConvTransposeSpatialDim(outputShapeFinal, xShape, kernelShape,
         padsOpt, stridesOpt, outputPads, outputShape, dilationsOpt);
-    output_shapeAttr(builder.getI64ArrayAttr(outputShapeFinal));
   }
   getResult().setType(
       RankedTensorType::get(outputShapeFinal, xTy.getElementType()));
diff --git a/src/Dialect/ONNX/ONNXOps/ShapeHelper.cpp b/src/Dialect/ONNX/ONNXOps/ShapeHelper.cpp
index 0c92556c..354c4805 100644
--- a/src/Dialect/ONNX/ONNXOps/ShapeHelper.cpp
+++ b/src/Dialect/ONNX/ONNXOps/ShapeHelper.cpp
@@ -539,6 +539,7 @@ template struct ONNXOpShapeHelper<ONNXCompressOp>;
 template struct ONNXOpShapeHelper<ONNXConcatOp>;
 template struct ONNXOpShapeHelper<ONNXConcatShapeTransposeOp>;
 template struct ONNXOpShapeHelper<ONNXConvOp>;
+template struct ONNXOpShapeHelper<ONNXConvTransposeOp>;
 template struct ONNXOpShapeHelper<ONNXDepthToSpaceOp>;
 template struct ONNXOpShapeHelper<ONNXExpandOp>;
 template struct ONNXOpShapeHelper<ONNXFlattenOp>;
diff --git a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
index 00a56e9d..e0f01656 100644
--- a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
+++ b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
@@ -407,6 +407,27 @@ struct ONNXRoiAlignOpShapeHelper
   llvm::SmallVector<IndexExpr, 1> batchIndicesDims; // Dim of batch_indices.
 };
 
+// Shape for ONNXConvTransposeOp
+struct ONNXConvTransposeOpShapeHelper
+    : public ONNXOpShapeHelper<mlir::ONNXConvTransposeOp> {
+  ONNXConvTransposeOpShapeHelper(
+      mlir::ONNXConvTransposeOp *newOp, IndexExprScope *inScope = nullptr);
+  ONNXConvTransposeOpShapeHelper(mlir::ONNXConvTransposeOp *newOp,
+      mlir::OpBuilder *rewriter,
+      ArrayValueIndexCapture::GetDenseVal fGetDenseVal,
+      ArrayValueIndexCapture::LoadVal fLoadVal,
+      IndexExprScope *inScope = nullptr);
+  mlir::LogicalResult computeShape(
+      mlir::ONNXConvTransposeOpAdaptor operandAdaptor);
+  // Additional data for ConvTransposeOp.
+  // Values set by Compute.
+  llvm::SmallVector<IndexExpr, 2> kernelShape;
+  llvm::SmallVector<IndexExpr, 4> pads;
+  llvm::SmallVector<int64_t, 2> strides;
+  llvm::SmallVector<int64_t, 2> dilations;
+  llvm::SmallVector<int64_t, 2> outputPadding;
+};
+
 #define DECLARE_POOL_SHAPE_HELPER(OpName)                                      \
   class OpName##ShapeHelper : public ONNXGenericPoolShapeHelper<mlir::OpName,  \
                                   mlir::OpName##Adaptor> {                     \
diff --git a/src/Transform/ONNX/ConstProp.td b/src/Transform/ONNX/ConstProp.td
index f3f61599..44a34739 100644
--- a/src/Transform/ONNX/ConstProp.td
+++ b/src/Transform/ONNX/ConstProp.td
@@ -396,7 +396,7 @@ def SliceofConst :  Pat<
     (CreateSliceOfConst $resOp, $input),
     [(IsFromDenseONNXConstantOp:$input), (IsFromTrueDenseONNXConstantOp:$starts),
      (IsFromTrueDenseONNXConstantOp:$ends), (IsFromTrueDenseONNXConstantOpOrNone:$axes),
-     (IsFromTrueDenseONNXConstantOpOrNone:$steps)]>;
+     (IsFromTrueDenseONNXConstantOpOrNone:$steps), (HasStaticShape:$resOp)]>;
 
 //===----------------------------------------------------------------------===//
 // Patterns to enable opportunities with Concat operations.
diff --git a/test/mlir/conversion/onnx_to_mhlo/Math/Clip.mlir b/test/mlir/conversion/onnx_to_mhlo/Math/Clip.mlir
new file mode 100644
index 00000000..ce59e365
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/Math/Clip.mlir
@@ -0,0 +1,58 @@
+// RUN: onnx-mlir-opt --convert-onnx-to-mhlo --canonicalize %s -split-input-file | FileCheck %s
+
+func.func @test_clip(%arg0: tensor<3xf32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<3xf32> attributes {input_names = ["x", "min", "max"], output_names = ["y"]} {
+  %0 = "onnx.Clip"(%arg0, %arg1, %arg2) : (tensor<3xf32>, tensor<f32>, tensor<f32>) -> tensor<3xf32>
+  return %0 : tensor<3xf32>
+// CHECK-LABEL:  func @test_clip
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<3xf32>, [[PARAM_1_:%.+]]: tensor<f32>, [[PARAM_2_:%.+]]: tensor<f32>) -> tensor<3xf32>
+// CHECK-SAME:   attributes {input_names = ["x", "min", "max"], output_names = ["y"]} {
+// CHECK-NEXT:     [[VAR_0_:%.+]] = mhlo.clamp [[PARAM_1_]], [[PARAM_0_]], [[PARAM_2_]] : (tensor<f32>, tensor<3xf32>, tensor<f32>) -> tensor<3xf32>
+// CHECK-NEXT:     return [[VAR_0_]] : tensor<3xf32>
+// CHECK-NEXT:   }
+}
+
+// -----
+
+// Test when min is none
+func.func @test_clip_default_min_f32(%arg0: tensor<3xf32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<3xf32> attributes {input_names = ["x", "min", "max"], output_names = ["y"]} {
+  %cst = "onnx.NoValue"() {value} : () -> none
+  %0 = "onnx.Clip"(%arg0, %cst, %arg2) : (tensor<3xf32>, none, tensor<f32>) -> tensor<3xf32>
+  return %0 : tensor<3xf32>
+// CHECK-LABEL:  func @test_clip_default_min_f32
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<3xf32>, [[PARAM_1_:%.+]]: tensor<f32>, [[PARAM_2_:%.+]]: tensor<f32>) -> tensor<3xf32>
+// CHECK-SAME:   attributes {input_names = ["x", "min", "max"], output_names = ["y"]} {
+// CHECK-NEXT:     [[VAR_0_:%.+]] = mhlo.constant dense<-3.40282347E+38> : tensor<f32>
+// CHECK-NEXT:     [[VAR_1_:%.+]] = mhlo.clamp [[VAR_0_]], [[PARAM_0_]], [[PARAM_2_]] : (tensor<f32>, tensor<3xf32>, tensor<f32>) -> tensor<3xf32>
+// CHECK-NEXT:     return [[VAR_1_]] : tensor<3xf32>
+// CHECK-NEXT:   }
+}
+
+// Test when min is none
+func.func @test_clip_default_min_f64(%arg0: tensor<3xf64>, %arg1: tensor<f64>, %arg2: tensor<f64>) -> tensor<3xf64> attributes {input_names = ["x", "min", "max"], output_names = ["y"]} {
+  %cst = "onnx.NoValue"() {value} : () -> none
+  %0 = "onnx.Clip"(%arg0, %cst, %arg2) : (tensor<3xf64>, none, tensor<f64>) -> tensor<3xf64>
+  return %0 : tensor<3xf64>
+// CHECK-LABEL:  func @test_clip_default_min_f64
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<3xf64>, [[PARAM_1_:%.+]]: tensor<f64>, [[PARAM_2_:%.+]]: tensor<f64>) -> tensor<3xf64>
+// CHECK-SAME:   attributes {input_names = ["x", "min", "max"], output_names = ["y"]} {
+// CHECK-NEXT:     [[VAR_0_:%.+]] = mhlo.constant dense<-1.7976931348623157E+308> : tensor<f64>
+// CHECK-NEXT:     [[VAR_1_:%.+]] = mhlo.clamp [[VAR_0_]], [[PARAM_0_]], [[PARAM_2_]] : (tensor<f64>, tensor<3xf64>, tensor<f64>) -> tensor<3xf64>
+// CHECK-NEXT:     return [[VAR_1_]] : tensor<3xf64>
+// CHECK-NEXT:   }
+}
+
+// -----
+
+// Test when max is none
+func.func @test_clip_default_max(%arg0: tensor<3xi32>, %arg1: tensor<i32>, %arg2: tensor<i32>) -> tensor<3xi32> attributes {input_names = ["x", "min", "max"], output_names = ["y"]} {
+  %cst = "onnx.NoValue"() {value} : () -> none
+  %0 = "onnx.Clip"(%arg0, %arg1, %cst) : (tensor<3xi32>, tensor<i32>, none) -> tensor<3xi32>
+  return %0 : tensor<3xi32>
+// CHECK-LABEL:  func @test_clip_default_max
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<3xi32>, [[PARAM_1_:%.+]]: tensor<i32>, [[PARAM_2_:%.+]]: tensor<i32>) -> tensor<3xi32>
+// CHECK-SAME:   attributes {input_names = ["x", "min", "max"], output_names = ["y"]} {
+// CHECK-NEXT:     [[VAR_0_:%.+]] = mhlo.constant dense<2147483647> : tensor<i32>
+// CHECK-NEXT:     [[VAR_1_:%.+]] = mhlo.clamp [[PARAM_1_]], [[PARAM_0_]], [[VAR_0_]] : (tensor<i32>, tensor<3xi32>, tensor<i32>) -> tensor<3xi32>
+// CHECK-NEXT:     return [[VAR_1_]] : tensor<3xi32>
+// CHECK-NEXT:   }
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir b/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
index 8e93f630..1828de6e 100644
--- a/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
+++ b/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
@@ -152,7 +152,6 @@ func.func @test_binary_elementwise_op_template_unknown_dims(%arg0: tensor<?x4x5x
 // CHECK: %3 = "mhlo.dynamic_broadcast_in_dim"(%arg0, %2) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<?x4x5xf32>, tensor<3xindex>) -> tensor<?x4x5xf32>
 // CHECK: %4 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %2) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<1x?x1xf32>, tensor<3xindex>) -> tensor<?x4x5xf32>
 // CHECK: %5 = mhlo.compare LT, %3, %4, NOTYPE : (tensor<?x4x5xf32>, tensor<?x4x5xf32>) -> tensor<?x4x5xi1>
-
 }
 
 func.func @test_less_unknown_dims_2(%arg0: tensor<?x?x5xf32>, %arg1: tensor<?x4x5xf32>) -> tensor<?x4x5xi1> {
@@ -172,10 +171,10 @@ func.func @test_pow_verifier_1(%arg0: tensor<1x2x3x4xf32>, %arg1: tensor<f32>) -
 // CHECK: %1 = mhlo.power %arg0, %0 : tensor<1x2x3x4xf32>
 }
 
-func.func @test_mul_unknown_dims(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x?xf32>) -> tensor<10x10xf32> {
+func.func @test_mul_dynamic_shape(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x?xf32>) -> tensor<10x10xf32> {
   %0 = "onnx.Mul"(%arg0, %arg1) : (tensor<10x10xf32>, tensor<10x?xf32>) -> tensor<10x10xf32>
   "func.return"(%0) : (tensor<10x10xf32>) -> ()
-// CHECK-LABEL: func @test_mul_unknown_dims
+// CHECK-LABEL: func @test_mul_dynamic_shape
 // CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<10x10xf32>, [[PARAM_1_:%.+]]: tensor<10x?xf32>) -> tensor<10x10xf32> {
 // CHECK: %3 = "mhlo.dynamic_broadcast_in_dim"(%arg1, %2) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<10x?xf32>, tensor<2xindex>) -> tensor<10x10xf32>
 // CHECK: %4 = mhlo.multiply %arg0, %3 : tensor<10x10xf32>
@@ -186,4 +185,44 @@ func.func @test_sqrt(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
   "func.return"(%0) : (tensor<?x10xf32>) -> ()
 // CHECK-LABEL: func @test_sqrt
 // CHECK: %0 = mhlo.sqrt %arg0 : tensor<?x10xf32>
+}
+
+func.func @test_log(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
+  %0 = "onnx.Log"(%arg0) : (tensor<?x10xf32>) -> tensor<?x10xf32>
+  "func.return"(%0) : (tensor<?x10xf32>) -> ()
+// CHECK-LABEL: func @test_log
+// CHECK: %0 = mhlo.log %arg0 : tensor<?x10xf32>
+}
+
+func.func @test_tanh(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
+  %0 = "onnx.Tanh"(%arg0) : (tensor<?x10xf32>) -> tensor<?x10xf32>
+  "func.return"(%0) : (tensor<?x10xf32>) -> ()
+// CHECK-LABEL: func @test_tanh
+// CHECK: %0 = mhlo.tanh %arg0 : tensor<?x10xf32>
+}
+
+func.func @test_max(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x10xf32>) -> tensor<10x10xf32> {
+  %0 = "onnx.Max"(%arg0, %arg1) : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<10x10xf32>
+  "func.return"(%0) : (tensor<10x10xf32>) -> ()
+// CHECK-LABEL:  func @test_max
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<10x10xf32>, [[PARAM_1_:%.+]]: tensor<10x10xf32>) -> tensor<10x10xf32> {
+// CHECK-NEXT:      [[VAR_0_:%.+]] = mhlo.maximum [[PARAM_0_]], [[PARAM_1_]] : tensor<10x10xf32>
+// CHECK-NEXT:      return [[VAR_0_]] : tensor<10x10xf32>
+}
+
+func.func @test_leakyrelu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
+  %0 = "onnx.LeakyRelu"(%arg0) {alpha=0.5:f32} : (tensor<?x10xf32>) -> tensor<?x10xf32>
+  "func.return"(%0) : (tensor<?x10xf32>) -> ()
+// CHECK-LABEL:  func @test_leakyrelu_dynamic
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<?x10xf32>) -> tensor<?x10xf32> {
+// CHECK-DAG:      [[VAR_0_:%.+]] = mhlo.constant dense<5.000000e-01> : tensor<f32>
+// CHECK-DAG:      [[VAR_1_:%.+]] = shape.shape_of [[PARAM_0_]] : tensor<?x10xf32> -> tensor<2xindex>
+// CHECK-DAG:      [[VAR_2_:%.+]] = "mhlo.dynamic_broadcast_in_dim"([[VAR_0_]], [[VAR_1_]]) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<2xindex>) -> tensor<?x10xf32>
+// CHECK-DAG:      [[VAR_3_:%.+]] = mhlo.multiply [[PARAM_0_]], [[VAR_2_]] : tensor<?x10xf32>
+// CHECK-DAG:      [[VAR_4_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<f32>
+// CHECK-DAG:      [[VAR_5_:%.+]] = shape.shape_of [[PARAM_0_]] : tensor<?x10xf32> -> tensor<2xindex>
+// CHECK-DAG:      [[VAR_6_:%.+]] = "mhlo.dynamic_broadcast_in_dim"([[VAR_4_]], [[VAR_5_]]) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<2xindex>) -> tensor<?x10xf32>
+// CHECK-DAG:      [[VAR_7_:%.+]] = mhlo.compare  GT, %arg0, %6,  NOTYPE : (tensor<?x10xf32>, tensor<?x10xf32>) -> tensor<?x10xi1>
+// CHECK-NEXT:     [[VAR_8_:%.+]] = mhlo.select [[VAR_7_]], [[PARAM_0_]], [[VAR_3_]] : tensor<?x10xi1>, tensor<?x10xf32>
+// CHECK-NEXT:     return [[VAR_8_]] : tensor<?x10xf32>
 }
\ No newline at end of file
diff --git a/test/mlir/conversion/onnx_to_mhlo/NN/ConvTranspose.mlir b/test/mlir/conversion/onnx_to_mhlo/NN/ConvTranspose.mlir
new file mode 100644
index 00000000..0a80a904
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/NN/ConvTranspose.mlir
@@ -0,0 +1,81 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-mhlo --canonicalize -split-input-file %s | FileCheck %s
+func.func @test_grouped(%arg0 : tensor<1x72x8x14xf32>, %arg1 : tensor<72x24x4x4xf32>, %arg2 : tensor<72xf32>) -> tensor<1x72x16x28xf32> {
+  %0 = "onnx.ConvTranspose"(%arg0, %arg1, %arg2) {group = 3 : si64, kernel_shape = [4, 4], pads = [1, 1, 1, 1], strides = [2, 2]} : (tensor<1x72x8x14xf32>, tensor<72x24x4x4xf32>, tensor<72xf32>) -> tensor<1x72x16x28xf32>
+  "func.return"(%0) : (tensor<1x72x16x28xf32>) -> ()
+// CHECK-LABEL: @test_grouped
+// CHECK{LITERAL}: %0 = "mhlo.reverse"(%arg1) {dimensions = dense<[2, 3]> : tensor<2xi64>} : (tensor<72x24x4x4xf32>) -> tensor<72x24x4x4xf32>
+// CHECK{LITERAL}: %1 = mhlo.reshape %0 : (tensor<72x24x4x4xf32>) -> tensor<3x24x24x4x4xf32>
+// CHECK{LITERAL}: %2 = "mhlo.transpose"(%1) {permutation = dense<[0, 2, 1, 3, 4]> : tensor<5xi64>} : (tensor<3x24x24x4x4xf32>) -> tensor<3x24x24x4x4xf32>
+// CHECK{LITERAL}: %3 = mhlo.reshape %2 : (tensor<3x24x24x4x4xf32>) -> tensor<72x24x4x4xf32>
+// CHECK{LITERAL}: %4 = mhlo.convolution(%arg0, %3) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[2, 2], [2, 2]], lhs_dilate = [2, 2], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 3 : i64} : (tensor<1x72x8x14xf32>, tensor<72x24x4x4xf32>) -> tensor<1x72x16x28xf32>
+// CHECK{LITERAL}: %5 = "mhlo.broadcast_in_dim"(%arg2) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<72xf32>) -> tensor<1x72x16x28xf32>
+// CHECK{LITERAL}: %6 = mhlo.add %4, %5 : tensor<1x72x16x28xf32>
+// CHECK{LITERAL}: return %6 : tensor<1x72x16x28xf32>
+}
+
+func.func @test_dynamic_shape(%arg0 : tensor<?x2x3x3xf32>, %arg1 : tensor<2x2x3x3xf32>) -> tensor<?x2x5x5xf32> {
+  %bias = "onnx.NoValue"() {value} : () -> none
+  %0 = "onnx.ConvTranspose"(%arg0, %arg1, %bias) : (tensor<?x2x3x3xf32>, tensor<2x2x3x3xf32>, none) -> tensor<?x2x5x5xf32>
+  "func.return"(%0) : (tensor<?x2x5x5xf32>) -> ()
+// CHECK-LABEL: @test_dynamic_shape
+// CHECK{LITERAL}: %0 = "mhlo.reverse"(%arg1) {dimensions = dense<[2, 3]> : tensor<2xi64>} : (tensor<2x2x3x3xf32>) -> tensor<2x2x3x3xf32>
+// CHECK{LITERAL}: %1 = "mhlo.transpose"(%0) {permutation = dense<[1, 0, 2, 3]> : tensor<4xi64>} : (tensor<2x2x3x3xf32>) -> tensor<2x2x3x3xf32>
+// CHECK{LITERAL}: %2 = mhlo.convolution(%arg0, %1) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[2, 2], [2, 2]], lhs_dilate = [1, 1], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<?x2x3x3xf32>, tensor<2x2x3x3xf32>) -> tensor<?x2x5x5xf32>
+// CHECK{LITERAL}: return %2 : tensor<?x2x5x5xf32>
+}
+
+func.func @test_valid(%arg0 : tensor<1x2x3x3xf32>, %arg1 : tensor<2x2x3x3xf32>) -> tensor<1x2x5x5xf32> {
+  %bias = "onnx.NoValue"() {value} : () -> none
+  %0 = "onnx.ConvTranspose"(%arg0, %arg1, %bias) {auto_pad = "VALID"} : (tensor<1x2x3x3xf32>, tensor<2x2x3x3xf32>, none) -> tensor<1x2x5x5xf32>
+  "func.return"(%0) : (tensor<1x2x5x5xf32>) -> ()
+// CHECK-LABEL: @test_valid
+// CHECK{LITERAL}: %0 = "mhlo.reverse"(%arg1) {dimensions = dense<[2, 3]> : tensor<2xi64>} : (tensor<2x2x3x3xf32>) -> tensor<2x2x3x3xf32>
+// CHECK{LITERAL}: %1 = "mhlo.transpose"(%0) {permutation = dense<[1, 0, 2, 3]> : tensor<4xi64>} : (tensor<2x2x3x3xf32>) -> tensor<2x2x3x3xf32>
+// CHECK{LITERAL}: %2 = mhlo.convolution(%arg0, %1) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[2, 2], [2, 2]], lhs_dilate = [1, 1], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x2x3x3xf32>, tensor<2x2x3x3xf32>) -> tensor<1x2x5x5xf32>
+// CHECK{LITERAL}: return %2 : tensor<1x2x5x5xf32>
+}
+
+func.func @test_attributes_0(%arg0 : tensor<1x1x3x3xf32>, %arg1 : tensor<1x2x3x3xf32>) -> tensor<1x2x9x7xf32> {
+  %bias = "onnx.NoValue"() {value} : () -> none
+  %0 = "onnx.ConvTranspose"(%arg0, %arg1, %bias) {strides = [3, 2], output_shape = [9, 7]} : (tensor<1x1x3x3xf32>, tensor<1x2x3x3xf32>, none) -> tensor<1x2x9x7xf32>
+  "func.return"(%0) : (tensor<1x2x9x7xf32>) -> ()
+// CHECK-LABEL: @test_attributes_0
+// CHECK{LITERAL}: %0 = "mhlo.reverse"(%arg1) {dimensions = dense<[2, 3]> : tensor<2xi64>} : (tensor<1x2x3x3xf32>) -> tensor<1x2x3x3xf32>
+// CHECK{LITERAL}: %1 = mhlo.reshape %0 : (tensor<1x2x3x3xf32>) -> tensor<2x1x3x3xf32>
+// CHECK{LITERAL}: %2 = mhlo.convolution(%arg0, %1) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[2, 2], [2, 2]], lhs_dilate = [3, 2], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x3x3xf32>, tensor<2x1x3x3xf32>) -> tensor<1x2x9x7xf32>
+// CHECK{LITERAL}: return %2 : tensor<1x2x9x7xf32>
+}
+
+func.func @test_attributes_1(%arg0 : tensor<1x1x3x3xf32>, %arg1 : tensor<1x2x3x3xf32>) -> tensor<1x2x10x8xf32> {
+  %bias = "onnx.NoValue"() {value} : () -> none
+  %0 = "onnx.ConvTranspose"(%arg0, %arg1, %bias) {strides = [3, 2], output_padding = [1, 1]} : (tensor<1x1x3x3xf32>, tensor<1x2x3x3xf32>, none) -> tensor<1x2x10x8xf32>
+  "func.return"(%0) : (tensor<1x2x10x8xf32>) -> ()
+// CHECK-LABEL: @test_attributes_1
+// CHECK{LITERAL}: %0 = mhlo.constant dense<0.000000e+00> : tensor<f32>
+// CHECK{LITERAL}: %1 = "mhlo.reverse"(%arg1) {dimensions = dense<[2, 3]> : tensor<2xi64>} : (tensor<1x2x3x3xf32>) -> tensor<1x2x3x3xf32>
+// CHECK{LITERAL}: %2 = mhlo.reshape %1 : (tensor<1x2x3x3xf32>) -> tensor<2x1x3x3xf32>
+// CHECK{LITERAL}: %3 = mhlo.convolution(%arg0, %2) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[2, 2], [2, 2]], lhs_dilate = [3, 2], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x3x3xf32>, tensor<2x1x3x3xf32>) -> tensor<1x2x9x7xf32>
+// CHECK{LITERAL}: %4 = "mhlo.pad"(%3, %0) {edge_padding_high = dense<[0, 0, 1, 1]> : vector<4xi64>, edge_padding_low = dense<0> : vector<4xi64>, interior_padding = dense<0> : vector<4xi64>} : (tensor<1x2x9x7xf32>, tensor<f32>) -> tensor<1x2x10x8xf32>
+// CHECK{LITERAL}: return %4 : tensor<1x2x10x8xf32>
+}
+
+func.func @test_dilations(%arg0 : tensor<1x1x3x3xf32>, %arg1 : tensor<1x1x2x2xf32>) -> tensor<1x1x5x5xf32> {
+  %bias = "onnx.NoValue"() {value} : () -> none
+  %0 = "onnx.ConvTranspose"(%arg0, %arg1, %bias) {dilations = [2, 2]} : (tensor<1x1x3x3xf32>, tensor<1x1x2x2xf32>, none) -> tensor<1x1x5x5xf32>
+  "func.return"(%0) : (tensor<1x1x5x5xf32>) -> ()
+// CHECK-LABEL: @test_dilations
+// CHECK{LITERAL}: %0 = "mhlo.reverse"(%arg1) {dimensions = dense<[2, 3]> : tensor<2xi64>} : (tensor<1x1x2x2xf32>) -> tensor<1x1x2x2xf32>
+// CHECK{LITERAL}: %1 = mhlo.convolution(%arg0, %0) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[2, 2], [2, 2]], lhs_dilate = [1, 1], rhs_dilate = [2, 2]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x3x3xf32>, tensor<1x1x2x2xf32>) -> tensor<1x1x5x5xf32>
+// CHECK{LITERAL}: return %1 : tensor<1x1x5x5xf32>
+}
+
+func.func @test_pads(%arg0 : tensor<1x1x3x3xf32>, %arg1 : tensor<1x2x3x3xf32>) -> tensor<1x2x7x3xf32> {
+  %bias = "onnx.NoValue"() {value} : () -> none
+  %0 = "onnx.ConvTranspose"(%arg0, %arg1, %bias) {strides = [3, 2], pads = [1, 2, 1, 2]} : (tensor<1x1x3x3xf32>, tensor<1x2x3x3xf32>, none) -> tensor<1x2x7x3xf32>
+  "func.return"(%0) : (tensor<1x2x7x3xf32>) -> ()
+// CHECK-LABEL: @test_pads
+// CHECK{LITERAL}: %0 = "mhlo.reverse"(%arg1) {dimensions = dense<[2, 3]> : tensor<2xi64>} : (tensor<1x2x3x3xf32>) -> tensor<1x2x3x3xf32>
+// CHECK{LITERAL}: %1 = mhlo.reshape %0 : (tensor<1x2x3x3xf32>) -> tensor<2x1x3x3xf32>
+// CHECK{LITERAL}: %2 = mhlo.convolution(%arg0, %1) dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1], window = {stride = [1, 1], pad = [[1, 1], [0, 0]], lhs_dilate = [3, 2], rhs_dilate = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64} : (tensor<1x1x3x3xf32>, tensor<2x1x3x3xf32>) -> tensor<1x2x7x3xf32>
+// CHECK{LITERAL}: return %2 : tensor<1x2x7x3xf32>
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/NN/Pooling.mlir b/test/mlir/conversion/onnx_to_mhlo/NN/Pooling.mlir
index 2c3af9bb..1ef8e238 100644
--- a/test/mlir/conversion/onnx_to_mhlo/NN/Pooling.mlir
+++ b/test/mlir/conversion/onnx_to_mhlo/NN/Pooling.mlir
@@ -1,6 +1,6 @@
-// RUN: onnx-mlir-opt --convert-onnx-to-mhlo %s -split-input-file | FileCheck %s
+// RUN: onnx-mlir-opt --convert-onnx-to-mhlo --canonicalize %s -split-input-file | FileCheck %s
 
-/// Test the default behavior of Max Pool with no padding (pad are set but shoudl be ignored)
+/// Test the default behavior of Max Pool with no padding (pad are set but should be ignored)
 func.func @test_default_maxpoolsingleout(%arg0 : tensor<5x5x32x32xf32>) -> tensor<5x5x30x30xf32> {
   %0 = "onnx.MaxPoolSingleOut"(%arg0) {auto_pad = "VALID", ceil_mode = 0 : si64, kernel_shape = [3,3]} : (tensor<5x5x32x32xf32>) -> tensor<5x5x30x30xf32>
   "func.return"(%0) : (tensor<5x5x30x30xf32>) -> ()
@@ -12,7 +12,6 @@ func.func @test_default_maxpoolsingleout(%arg0 : tensor<5x5x32x32xf32>) -> tenso
 // CHECK-NEXT:                mhlo.return %2 : tensor<f32>
 // CHECK-NEXT{LITERAL}:   }) {padding = dense<0> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<5x5x32x32xf32>, tensor<f32>) -> tensor<5x5x30x30xf32>
 
-
 // -----
 
 /// Test the default behavior of Max Pool with no padding (pad are not set, default to zero)
@@ -27,7 +26,6 @@ func.func @test_default_maxpoolsingleout_defpad(%arg0 : tensor<5x5x32x32xf32>) -
 // CHECK-NEXT:                mhlo.return %2 : tensor<f32>
 // CHECK-NEXT{LITERAL}:   }) {padding = dense<0> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<5x5x32x32xf32>, tensor<f32>) -> tensor<5x5x30x30xf32>
 
-
 // -----
 
 /// Test the default behavior of Max Pool with uniform padding
@@ -42,7 +40,6 @@ func.func @test_default_maxpoolsingleout_pad(%arg0 : tensor<5x5x32x32xf32>) -> t
 // CHECK-NEXT:                 mhlo.return %2 : tensor<f32>
 // CHECK-NEXT{LITERAL}:    }) {padding = dense<[[0, 0], [0, 0], [1, 1], [1, 1]]> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<5x5x32x32xf32>, tensor<f32>) -> tensor<5x5x32x32xf32>
 
-
 // -----
 
 /// Test the default behavior of Max Pool with non uniform padding
@@ -99,7 +96,6 @@ func.func @test_default_maxpoolsingleout_strides_nonunifpad_ceil(%arg0 : tensor<
 // CHECK-NEXT:                 mhlo.return %2 : tensor<f32>
 // CHECK-NEXT{LITERAL}:    }) {padding = dense<[[0, 0], [0, 0], [1, 1], [0, 0]]> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 2, 2]> : tensor<4xi64>, window_strides = dense<[1, 1, 2, 2]> : tensor<4xi64>} : (tensor<5x5x30x32xf32>, tensor<f32>) -> tensor<5x5x16x16xf32>
 
-
 // -----
 
 /// Test the default behavior of Max Pool with dilatation
@@ -128,7 +124,6 @@ func.func @test_default_maxpoolsingleout_upper(%arg0 : tensor<5x5x16x13xf32>) ->
 // CHECK-NEXT:                   mhlo.return %2 : tensor<f32>
 // CHECK-NEXT{LITERAL}:    }) {padding = dense<[[0, 0], [0, 0], [0, 0], [1, 2]]> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 4, 4]> : tensor<4xi64>, window_strides = dense<[1, 1, 4, 4]> : tensor<4xi64>} : (tensor<5x5x16x13xf32>, tensor<f32>) -> tensor<5x5x4x4xf32>
 
-
 // -----
 
 /// Test the default behavior of Max Pool with dilatation
@@ -142,3 +137,92 @@ func.func @test_default_maxpoolsingleout_lower(%arg0 : tensor<5x5x16x13xf32>) ->
 // CHECK-NEXT:                %2 = mhlo.maximum %arg1, %arg2 : tensor<f32>
 // CHECK-NEXT:                mhlo.return %2 : tensor<f32>
 // CHECK-NEXT{LITERAL}:   }) {padding = dense<[[0, 0], [0, 0], [0, 0], [2, 1]]> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 4, 4]> : tensor<4xi64>, window_strides = dense<[1, 1, 4, 4]> : tensor<4xi64>} : (tensor<5x5x16x13xf32>, tensor<f32>) -> tensor<5x5x4x4xf32>
+
+// -----
+
+/// Test the default behavior of Average Pool with no padding
+func.func @test_averagepool_default(%arg0 : tensor<5x5x32x32xf32>) -> tensor<5x5x30x30xf32> {
+  %0 = "onnx.AveragePool"(%arg0) {kernel_shape = [3,3]} : (tensor<5x5x32x32xf32>) -> tensor<5x5x30x30xf32>
+  "func.return"(%0) : (tensor<5x5x30x30xf32>) -> ()
+// CHECK-LABEL: test_averagepool_default
+// CHECK-DAG:     [[VAR_0_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<f32>
+// CHECK-DAG:     [[VAR_1_:%.+]] = mhlo.constant dense<1.000000e+00> : tensor<5x5x32x32xf32>
+// CHECK-NEXT:    [[VAR_2_:%.+]] = "mhlo.reduce_window"(%arg0, [[VAR_0_]]) ({
+// CHECK-NEXT:      ^bb0([[PARAM_1_:%.+]]: tensor<f32>, [[PARAM_2_:%.+]]: tensor<f32>):
+// CHECK-NEXT:        [[VAR_5_:%.+]] = mhlo.add [[PARAM_1_]], [[PARAM_2_]] : tensor<f32>
+// CHECK-NEXT:        mhlo.return [[VAR_5_]] : tensor<f32>
+// CHECK-NEXT:    }) {padding = dense<0> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<5x5x32x32xf32>, tensor<f32>) -> tensor<5x5x30x30xf32>
+// CHECK-NEXT:    [[VAR_3_:%.+]] = "mhlo.reduce_window"([[VAR_1_]], [[VAR_0_]]) ({
+// CHECK-NEXT:      ^bb0([[PARAM_1_:%.+]]: tensor<f32>, [[PARAM_2_:%.+]]: tensor<f32>):
+// CHECK-NEXT:        [[VAR_5_:%.+]] = mhlo.add [[PARAM_1_]], [[PARAM_2_]] : tensor<f32>
+// CHECK-NEXT:        mhlo.return [[VAR_5_]] : tensor<f32>
+// CHECK-NEXT:    }) {padding = dense<0> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<5x5x32x32xf32>, tensor<f32>) -> tensor<5x5x30x30xf32>
+// CHECK-NEXT:    [[VAR_4_:%.+]] = mhlo.divide [[VAR_2_]], [[VAR_3_]] : tensor<5x5x30x30xf32>
+// CHECK-NEXT:    return [[VAR_4_]] : tensor<5x5x30x30xf32>
+}
+
+// -----
+
+/// Test the behavior of Average Pool with padding
+func.func @test_averagepool_pad(%arg0 : tensor<5x5x32x32xf32>) -> tensor<5x5x32x32xf32> {
+  %0 = "onnx.AveragePool"(%arg0) {kernel_shape = [3,3], pads = [1, 1, 1, 1]} : (tensor<5x5x32x32xf32>) -> tensor<5x5x32x32xf32>
+  "func.return"(%0) : (tensor<5x5x32x32xf32>) -> ()
+// CHECK-LABEL: test_averagepool_pad
+// CHECK-DAG:              [[VAR_0_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<f32>
+// CHECK-DAG:              [[VAR_1_:%.+]] = mhlo.constant dense<1.000000e+00> : tensor<5x5x32x32xf32>
+// CHECK-NEXT:             [[VAR_2_:%.+]] = "mhlo.reduce_window"(%arg0, [[VAR_0_]]) ({
+// CHECK-NEXT:               ^bb0([[PARAM_1_:%.+]]: tensor<f32>, [[PARAM_2_:%.+]]: tensor<f32>):
+// CHECK-NEXT:                 [[VAR_5_:%.+]] = mhlo.add [[PARAM_1_]], [[PARAM_2_]] : tensor<f32>
+// CHECK-NEXT:                 mhlo.return [[VAR_5_]] : tensor<f32>
+// CHECK-NEXT{LITERAL}:    }) {padding = dense<[[0, 0], [0, 0], [1, 1], [1, 1]]> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<5x5x32x32xf32>, tensor<f32>) -> tensor<5x5x32x32xf32>
+// CHECK-NEXT:             [[VAR_3_:%.+]] = "mhlo.reduce_window"([[VAR_1_]], [[VAR_0_]]) ({
+// CHECK-NEXT:               ^bb0([[PARAM_1_:%.+]]: tensor<f32>, [[PARAM_2_:%.+]]: tensor<f32>):
+// CHECK-NEXT:                 [[VAR_5_:%.+]] = mhlo.add [[PARAM_1_]], [[PARAM_2_]] : tensor<f32>
+// CHECK-NEXT:                 mhlo.return [[VAR_5_]] : tensor<f32>
+// CHECK-NEXT{LITERAL}:    }) {padding = dense<[[0, 0], [0, 0], [1, 1], [1, 1]]> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<5x5x32x32xf32>, tensor<f32>) -> tensor<5x5x32x32xf32>
+// CHECK-NEXT:             [[VAR_4_:%.+]] = mhlo.divide [[VAR_2_]], [[VAR_3_]] : tensor<5x5x32x32xf32>
+// CHECK-NEXT:             return [[VAR_4_]] : tensor<5x5x32x32xf32>
+}
+
+// -----
+
+/// Test the behavior of Average Pool with count_include_pad set
+func.func @test_averagepool_count_include_pad(%arg0 : tensor<5x5x32x32xf32>) -> tensor<5x5x30x30xf32> {
+  %0 = "onnx.AveragePool"(%arg0) {kernel_shape = [3,3], count_include_pad = 1: si64} : (tensor<5x5x32x32xf32>) -> tensor<5x5x30x30xf32>
+  "func.return"(%0) : (tensor<5x5x30x30xf32>) -> ()
+// CHECK-LABEL: test_averagepool_count_include_pad
+// CHECK-DAG:     [[VAR_0_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<f32>
+// CHECK-DAG:     [[VAR_1_:%.+]] = mhlo.constant dense<9.000000e+00> : tensor<5x5x30x30xf32>
+// CHECK-NEXT:    [[VAR_2_:%.+]] = "mhlo.reduce_window"(%arg0, [[VAR_0_]]) ({
+// CHECK-NEXT:      ^bb0([[PARAM_1_:%.+]]: tensor<f32>, [[PARAM_2_:%.+]]: tensor<f32>):
+// CHECK-NEXT:        [[VAR_4_:%.+]] = mhlo.add %arg1, %arg2 : tensor<f32>
+// CHECK-NEXT:        mhlo.return [[VAR_4_]] : tensor<f32>
+// CHECK-NEXT:    }) {padding = dense<0> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<5x5x32x32xf32>, tensor<f32>) -> tensor<5x5x30x30xf32>
+// CHECK-NEXT:    [[VAR_3_:%.+]] = mhlo.divide [[VAR_2_]], [[VAR_1_]] : tensor<5x5x30x30xf32>
+// CHECK-NEXT:    return [[VAR_3_]] : tensor<5x5x30x30xf32>
+}
+
+// -----
+
+/// Test the behavior of Average Pool with dynamic batch size
+func.func @test_averagepool_dynamic_shape(%arg0 : tensor<?x5x32x32xf32>) -> tensor<?x5x30x30xf32> {
+  %0 = "onnx.AveragePool"(%arg0) {kernel_shape = [3,3]} : (tensor<?x5x32x32xf32>) -> tensor<?x5x30x30xf32>
+  "func.return"(%0) : (tensor<?x5x30x30xf32>) -> ()
+// CHECK-LABEL: test_averagepool_dynamic_shape
+// CHECK-DAG:     [[VAR_0_:%.+]] = mhlo.constant dense<0.000000e+00> : tensor<f32>
+// CHECK-DAG:     [[VAR_1_:%.+]] = mhlo.constant dense<1.000000e+00> : tensor<f32>
+// CHECK-NEXT:    [[VAR_2_:%.+]] = "mhlo.reduce_window"(%arg0, [[VAR_0_]]) ({
+// CHECK-NEXT:      ^bb0([[PARAM_1_:%.+]]: tensor<f32>, [[PARAM_2_:%.+]]: tensor<f32>):
+// CHECK-NEXT:        [[VAR_7_:%.+]] = mhlo.add [[PARAM_1_]], [[PARAM_2_]] : tensor<f32>
+// CHECK-NEXT:        mhlo.return [[VAR_7_]] : tensor<f32>
+// CHECK-NEXT:    }) {padding = dense<0> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<?x5x32x32xf32>, tensor<f32>) -> tensor<?x5x30x30xf32>
+// CHECK-NEXT:    [[VAR_3_:%.+]] = shape.shape_of %arg0 : tensor<?x5x32x32xf32> -> tensor<4xindex>
+// CHECK-NEXT:    [[VAR_4_:%.+]] = "mhlo.dynamic_broadcast_in_dim"([[VAR_1_]], [[VAR_3_]]) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<4xindex>) -> tensor<?x5x32x32xf32>
+// CHECK-NEXT:    [[VAR_5_:%.+]] = "mhlo.reduce_window"([[VAR_4_]], [[VAR_0_]]) ({
+// CHECK-NEXT:      ^bb0([[PARAM_1_:%.+]]: tensor<f32>, [[PARAM_2_:%.+]]: tensor<f32>):
+// CHECK-NEXT:        [[VAR_7_:%.+]] = mhlo.add [[PARAM_1_]], [[PARAM_2_]] : tensor<f32>
+// CHECK-NEXT:        mhlo.return [[VAR_7_]] : tensor<f32>
+// CHECK-NEXT:    }) {padding = dense<0> : tensor<4x2xi64>, window_dilations = dense<1> : tensor<4xi64>, window_dimensions = dense<[1, 1, 3, 3]> : tensor<4xi64>, window_strides = dense<1> : tensor<4xi64>} : (tensor<?x5x32x32xf32>, tensor<f32>) -> tensor<?x5x30x30xf32>
+// CHECK-NEXT:    [[VAR_6_:%.+]] = mhlo.divide [[VAR_2_]], [[VAR_5_]] : tensor<?x5x30x30xf32>
+// CHECK-NEXT:    return [[VAR_6_]] : tensor<?x5x30x30xf32>
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/Tensor/Concat.mlir b/test/mlir/conversion/onnx_to_mhlo/Tensor/Concat.mlir
index 9da511df..b3af138a 100644
--- a/test/mlir/conversion/onnx_to_mhlo/Tensor/Concat.mlir
+++ b/test/mlir/conversion/onnx_to_mhlo/Tensor/Concat.mlir
@@ -6,8 +6,8 @@ func.func @test_concat_dynamic_shape(%arg0 : tensor<5x5x?x32xf32>, %arg1 : tenso
   "func.return"(%0) : (tensor<5x5x?x32xf32>) -> ()
 // CHECK-LABEL:  func @test_concat_dynamic_shape
 // CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<5x5x?x32xf32>, [[PARAM_1_:%.+]]: tensor<5x5x?x32xf32>) -> tensor<5x5x?x32xf32> {
-// CHECK-NEXT:    [[VAR_0_:%.+]] = "mhlo.concatenate"(%arg0, %arg1) {dimension = 2 : i64} : (tensor<5x5x?x32xf32>, tensor<5x5x?x32xf32>) -> tensor<5x5x?x32xf32>
-// CHECK-NEXT:   return [[VAR_0_]] : tensor<5x5x?x32xf32>
+// CHECK-NEXT:    [[VAR_0_:%.+]] = "mhlo.concatenate"([[PARAM_0_]], [[PARAM_1_]]) {dimension = 2 : i64} : (tensor<5x5x?x32xf32>, tensor<5x5x?x32xf32>) -> tensor<5x5x?x32xf32>
+// CHECK-NEXT:    return [[VAR_0_]] : tensor<5x5x?x32xf32>
 // CHECK-NEXT:   }
 }
 
@@ -19,7 +19,7 @@ func.func @test_concat_negative_axis(%arg0 : tensor<5x5x1x32xf32>, %arg1 : tenso
   "func.return"(%0) : (tensor<5x5x4x32xf32>) -> ()
 // CHECK-LABEL:  func @test_concat_negative_axis
 // CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<5x5x1x32xf32>, [[PARAM_1_:%.+]]: tensor<5x5x3x32xf32>) -> tensor<5x5x4x32xf32> {
-// CHECK-NEXT:    [[VAR_0_:%.+]] = "mhlo.concatenate"(%arg0, %arg1) {dimension = 2 : i64} : (tensor<5x5x1x32xf32>, tensor<5x5x3x32xf32>) -> tensor<5x5x4x32xf32>
+// CHECK-NEXT:    [[VAR_0_:%.+]] = "mhlo.concatenate"([[PARAM_0_]], [[PARAM_1_]]) {dimension = 2 : i64} : (tensor<5x5x1x32xf32>, tensor<5x5x3x32xf32>) -> tensor<5x5x4x32xf32>
 // CHECK-NEXT:   return [[VAR_0_]] : tensor<5x5x4x32xf32>
 // CHECK-NEXT:   }
-}
\ No newline at end of file
+}
diff --git a/test/mlir/onnx/onnx_shape_inference.mlir b/test/mlir/onnx/onnx_shape_inference.mlir
index ebd03b80..e8746089 100644
--- a/test/mlir/onnx/onnx_shape_inference.mlir
+++ b/test/mlir/onnx/onnx_shape_inference.mlir
@@ -490,7 +490,7 @@ func.func @test_conv_transpose_1(%arg0 : tensor<1x64x36x48xf32>, %arg1 : tensor<
 
   // CHECK-LABEL: test_conv_transpose_1
   // CHECK: [[CST:%.+]] = "onnx.NoValue"() {value} : () -> none  
-  // CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, [[CST]]) {auto_pad = "NOTSET", dilations = [1, 1], group = 1 : si64, kernel_shape = [2, 2], output_shape = [1, 1, 72, 96], pads = [0, 0, 0, 0], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<1x1x72x96xf32>
+  // CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, [[CST]]) {auto_pad = "NOTSET", dilations = [1, 1], group = 1 : si64, kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<1x1x72x96xf32>
   // CHECK: return [[RES_ATTR]] : tensor<1x1x72x96xf32>
 }
 
@@ -501,7 +501,7 @@ func.func @test_conv_transpose_2(%arg0 : tensor<1x64x36x48xf32>, %arg1 : tensor<
 
   // CHECK-LABEL: test_conv_transpose_2
   // CHECK: [[CST:%.+]] = "onnx.NoValue"() {value} : () -> none  
-  // CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, [[CST]]) {auto_pad = "NOTSET", dilations = [1, 1], group = 64 : si64, kernel_shape = [2, 2], output_shape = [1, 64, 72, 96], pads = [0, 0, 0, 0], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<1x64x72x96xf32>
+  // CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, [[CST]]) {auto_pad = "NOTSET", dilations = [1, 1], group = 64 : si64, kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<1x64x72x96xf32>
   // CHECK: return [[RES_ATTR]] : tensor<1x64x72x96xf32>
 }
 
@@ -514,7 +514,7 @@ func.func @test_conv_transpose_3(%arg0: tensor<1x1x3x3xf32>, %arg1: tensor<1x2x3
 
 // CHECK-LABEL: test_conv_transpose_3
 // CHECK: [[CST:%.+]] = "onnx.NoValue"() {value} : () -> none
-// CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, %0) {auto_pad = "NOTSET", dilations = [1, 1], group = 1 : si64, kernel_shape = [3, 3], output_padding = [1, 1], output_shape = [1, 2, 10, 8], pads = [0, 0, 0, 0], strides = [3, 2]} : (tensor<1x1x3x3xf32>, tensor<1x2x3x3xf32>, none) -> tensor<1x2x10x8xf32>
+// CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, %0) {auto_pad = "NOTSET", dilations = [1, 1], group = 1 : si64, kernel_shape = [3, 3], output_padding = [1, 1], pads = [0, 0, 0, 0], strides = [3, 2]} : (tensor<1x1x3x3xf32>, tensor<1x2x3x3xf32>, none) -> tensor<1x2x10x8xf32>
 // CHECK: return [[RES_ATTR]] : tensor<1x2x10x8xf32>
 }
 
@@ -527,7 +527,7 @@ func.func @test_conv_transpose_4(%arg0: tensor<1x1x3x3xf32>, %arg1: tensor<1x2x3
 
 // CHECK-LABEL: test_conv_transpose_4
 // CHECK: [[CST:%.+]] = "onnx.NoValue"() {value} : () -> none
-// CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, %0) {auto_pad = "NOTSET", dilations = [1, 1], group = 1 : si64, kernel_shape = [3, 3], output_shape = [1, 2, 7, 3], pads = [1, 2, 1, 2], strides = [3, 2]} : (tensor<1x1x3x3xf32>, tensor<1x2x3x3xf32>, none) -> tensor<1x2x7x3xf32>
+// CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, %0) {auto_pad = "NOTSET", dilations = [1, 1], group = 1 : si64, kernel_shape = [3, 3], pads = [1, 2, 1, 2], strides = [3, 2]} : (tensor<1x1x3x3xf32>, tensor<1x2x3x3xf32>, none) -> tensor<1x2x7x3xf32>
 // CHECK: return [[RES_ATTR]] : tensor<1x2x7x3xf32>
 }
 
@@ -540,7 +540,7 @@ func.func @test_conv_transpose_pads(%arg0 : tensor<1x64x36x48xf32>, %arg1 : tens
 
   // CHECK-LABEL: test_conv_transpose_pads
   // CHECK: [[CST:%.+]] = "onnx.NoValue"() {value} : () -> none
-  // CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, [[CST]]) {auto_pad = "NOTSET", dilations = [1, 1], group = 64 : si64, kernel_shape = [2, 2], output_shape = [1, 64, 72, 94], pads = [0, 1, 0, 1], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<1x64x72x94xf32>
+  // CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, [[CST]]) {auto_pad = "NOTSET", dilations = [1, 1], group = 64 : si64, kernel_shape = [2, 2], pads = [0, 1, 0, 1], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<1x64x72x94xf32>
   // CHECK: return [[RES_ATTR]] : tensor<1x64x72x94xf32>
 }
 
@@ -548,12 +548,12 @@ func.func @test_conv_transpose_pads(%arg0 : tensor<1x64x36x48xf32>, %arg1 : tens
 
 func.func @test_conv_transpose_output_shape(%arg0 : tensor<1x64x36x48xf32>, %arg1 : tensor<64x1x2x2xf32>) -> tensor<*xf32> {
   %cst = "onnx.NoValue"() {value} : () -> none
-  %0 = "onnx.ConvTranspose"(%arg0, %arg1, %cst) {dilations = [1, 1], group = 64 : si64, kernel_shape = [2, 2], output_shape = [1, 64, 72, 94], pads = [0, 0, 0, 0], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<*xf32>
+  %0 = "onnx.ConvTranspose"(%arg0, %arg1, %cst) {dilations = [1, 1], group = 64 : si64, kernel_shape = [2, 2], output_shape = [72, 94], pads = [0, 0, 0, 0], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<*xf32>
   "func.return"(%0) : (tensor<*xf32>) -> ()
 
   // CHECK-LABEL: test_conv_transpose_output_shape
   // CHECK: [[CST:%.+]] = "onnx.NoValue"() {value} : () -> none
-  // CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, [[CST]]) {auto_pad = "NOTSET", dilations = [1, 1], group = 64 : si64, kernel_shape = [2, 2], output_shape = [1, 64, 72, 94], pads = [0, 1, 0, 1], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<1x64x72x94xf32>
+  // CHECK-NEXT: [[RES_ATTR:%.+]] = "onnx.ConvTranspose"(%arg0, %arg1, [[CST]]) {auto_pad = "NOTSET", dilations = [1, 1], group = 64 : si64, kernel_shape = [2, 2], output_shape = [72, 94], pads = [0, 1, 0, 1], strides = [2, 2]} : (tensor<1x64x36x48xf32>, tensor<64x1x2x2xf32>, none) -> tensor<1x64x72x94xf32>
   // CHECK: return [[RES_ATTR]] : tensor<1x64x72x94xf32>
 }
 
-- 
2.30.2


From 7588aec921372dab44873fb78fe35376d6fe31ef Mon Sep 17 00:00:00 2001
From: "chongsong.chen" <chongsong.chen@bytedance.com>
Date: Sun, 11 Dec 2022 17:37:20 +0800
Subject: [PATCH 2/3] [Mhlo] converts pad op to mhlo in constant pad mode

Signed-off-by: chongsong.chen <chongsong.chen@bytedance.com>
---
 src/Conversion/ONNXToMhlo/CMakeLists.txt      |  1 +
 .../ONNXToMhlo/ConvertONNXToMhlo.cpp          |  1 +
 .../ONNXToMhlo/ONNXToMhloCommon.cpp           | 15 +++
 .../ONNXToMhlo/ONNXToMhloCommon.hpp           |  7 ++
 src/Conversion/ONNXToMhlo/Tensor/Pad.cpp      | 94 +++++++++++++++++++
 5 files changed, 118 insertions(+)
 create mode 100644 src/Conversion/ONNXToMhlo/Tensor/Pad.cpp

diff --git a/src/Conversion/ONNXToMhlo/CMakeLists.txt b/src/Conversion/ONNXToMhlo/CMakeLists.txt
index 5d9bcedb..1c52503d 100644
--- a/src/Conversion/ONNXToMhlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToMhlo/CMakeLists.txt
@@ -42,6 +42,7 @@ add_onnx_mlir_library(OMONNXToMhlo
   Tensor/Gather.cpp
   Tensor/Identity.cpp
   Tensor/Reshape.cpp
+  Tensor/Pad.cpp
   Tensor/Shape.cpp
   Tensor/Slice.cpp
   Tensor/Split.cpp
diff --git a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
index ad631a17..a41a24cf 100644
--- a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
+++ b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
@@ -41,6 +41,7 @@ void populateONNXToMhloConversionPattern(
   populateLoweringONNXGatherOpToMhloPattern(patterns, ctx);
   populateLoweringONNXIdentityOpToMhloPattern(patterns, ctx);
   populateLoweringONNXReshapeOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXPadOpToMhloPattern(patterns, ctx);
   populateLoweringONNXShapeOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSliceOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSplitOpToMhloPattern(patterns, ctx);
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
index 4e4adfc1..55c791ba 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
@@ -93,4 +93,19 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   }
   return broadcastedOperands;
 }
+
+// This function satisfies the ArrayValueIndexCapture::DenseElementsAttr lambda
+// type, using MHLO and ONNX operations.
+DenseElementsAttr getDenseElementAttributeFromMhloValue(Value value) {
+  auto definingOp = value.getDefiningOp();
+  if (auto constantOp = dyn_cast_or_null<mhlo::ConstantOp>(definingOp)) {
+    return constantOp.getValue().dyn_cast<DenseElementsAttr>();
+  } else if (auto constantOp =
+                 dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
+    if (constantOp.value().has_value())
+      return constantOp.valueAttr().dyn_cast<DenseElementsAttr>();
+  }
+  return nullptr;
+}
+
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
index c27de18b..1d8367f3 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
@@ -114,6 +114,11 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     llvm::SmallVector<Value, 4> &operands, Type outputType,
     ConversionPatternRewriter &rewriter, Location loc, int64_t outputRank);
 
+// This function satisfies the ArrayValueIndexCapture::DenseElementsAttr lambda
+// type, using MHLO and ONNX operations.
+mlir::DenseElementsAttr getDenseElementAttributeFromMhloValue(
+    mlir::Value value);
+
 // `Math` directory methods:
 void populateLoweringONNXClipOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
@@ -151,6 +156,8 @@ void populateLoweringONNXGatherOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXIdentityOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXPadOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXReshapeOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXShapeOpToMhloPattern(
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
new file mode 100644
index 00000000..2f7c8a63
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
@@ -0,0 +1,94 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===----------- Pad.cpp - Lowering Pad Op ------------===//
+//
+// Copyright 2022
+//
+// =============================================================================
+//
+// This file lowers ONNX Pad Operators to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXPadOpLoweringToMhlo : public ConversionPattern {
+  ONNXPadOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(mlir::ONNXPadOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+
+    Location loc = op->getLoc();
+    ONNXPadOpAdaptor operandAdaptor(operands, op->getAttrDictionary());
+    Value data = operandAdaptor.data();
+    Value constantValue = operandAdaptor.constant_value();
+    Value pads = operandAdaptor.pads();
+    StringRef padMode = operandAdaptor.mode();
+
+    if (!padMode.equals_insensitive("constant"))
+        return failure();
+    assert(isRankedShapedType(data.getType()) && "Expected Ranked ShapedType");
+    ShapedType inputType = data.getType().cast<ShapedType>();
+    Type elemType = inputType.getElementType();
+    int64_t rank = inputType.getRank();
+
+    Type outputType = *op->result_type_begin();
+    if (!constantValue || constantValue.getType().isa<NoneType>()) {
+      // Pad with zeros by default
+      constantValue = rewriter.create<mhlo::ConstantOp>(
+          loc, DenseElementsAttr::get(mlir::RankedTensorType::get({}, elemType),
+              rewriter.getZeroAttr(elemType)));
+    }
+    SmallVector<int64_t> edgePaddingLowVec(rank, 0);
+    SmallVector<int64_t> edgePaddingHighVec(rank, 0);
+    SmallVector<int64_t> interiorPaddingVec(rank, 0);
+    if (auto valueAttribute = getDenseElementAttributeFromMhloValue(pads)) {
+      // If `pads` are constants, read them."
+      int64_t idx = 0;
+      for (IntegerAttr value : valueAttribute.getValues<IntegerAttr>()) {
+        int64_t padValue = value.getInt();
+        if (padValue < 0)
+          return failure();
+        if (idx < rank)
+          edgePaddingLowVec[idx] = padValue;
+        else
+          edgePaddingHighVec[idx - rank] = padValue;
+        idx++;
+      }
+    } else {
+      assert(false && "Pads must be known at compile time");
+    }
+    
+    mlir::DenseIntElementsAttr edgePaddingLow =
+        rewriter.getI64VectorAttr(edgePaddingLowVec);
+    mlir::DenseIntElementsAttr edgePaddingHigh =
+        rewriter.getI64VectorAttr(edgePaddingHighVec);
+    mlir::DenseIntElementsAttr interiorPadding =
+        rewriter.getI64VectorAttr(interiorPaddingVec);
+    Value padResult = rewriter.create<mhlo::PadOp>(loc, outputType, data,
+        constantValue, edgePaddingLow, edgePaddingHigh, interiorPadding);
+
+    rewriter.replaceOp(op, padResult);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXPadOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXPadOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
-- 
2.30.2


From 5462f9a3a939a2219da7f53f7d77326f7dad370c Mon Sep 17 00:00:00 2001
From: "chongsong.chen" <chongsong.chen@bytedance.com>
Date: Fri, 6 Jan 2023 17:40:53 +0800
Subject: [PATCH 3/3] Can call shape inference on mhlo ops

---
 src/Transform/ONNX/ShapeInferencePass.cpp | 4 +---
 1 file changed, 1 insertion(+), 3 deletions(-)

diff --git a/src/Transform/ONNX/ShapeInferencePass.cpp b/src/Transform/ONNX/ShapeInferencePass.cpp
index f763f779..93966d58 100644
--- a/src/Transform/ONNX/ShapeInferencePass.cpp
+++ b/src/Transform/ONNX/ShapeInferencePass.cpp
@@ -123,9 +123,7 @@ public:
         // Attempt to infer the shape of the produced output(s).
         if (failed(shape_op.inferShapes(doShapeInference)))
           return op.emitError("shape inference failed");
-      } else if (!llvm::dyn_cast<CallOpInterface>(op))
-        return op.emitError("unable to infer shape of operation without shape "
-                            "inference interface");
+      }
     }
     return success();
   }
-- 
2.30.2

