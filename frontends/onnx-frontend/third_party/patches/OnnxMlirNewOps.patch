diff --git a/src/Conversion/ONNXToMhlo/CMakeLists.txt b/src/Conversion/ONNXToMhlo/CMakeLists.txt
index bd7283a9..e669c26d 100644
--- a/src/Conversion/ONNXToMhlo/CMakeLists.txt
+++ b/src/Conversion/ONNXToMhlo/CMakeLists.txt
@@ -44,8 +44,12 @@ add_onnx_mlir_library(OMONNXToMhlo
   Tensor/Expand.cpp
   Tensor/Flatten.cpp
   Tensor/Gather.cpp
+  Tensor/GatherElements.cpp
   Tensor/Identity.cpp
+  Tensor/OneHot.cpp
+  Tensor/Pad.cpp
   Tensor/Reshape.cpp
+  Tensor/ScatterND.cpp
   Tensor/Shape.cpp
   Tensor/Slice.cpp
   Tensor/Split.cpp
diff --git a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
index 73330c37..3231964d 100644
--- a/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
+++ b/src/Conversion/ONNXToMhlo/ConvertONNXToMhlo.cpp
@@ -41,8 +41,12 @@ void populateONNXToMhloConversionPattern(
   populateLoweringONNXExpandOpToMhloPattern(patterns, ctx);
   populateLoweringONNXFlattenOpToMhloPattern(patterns, ctx);
   populateLoweringONNXGatherOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXGatherElementsOpToMhloPattern(patterns, ctx);
   populateLoweringONNXIdentityOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXOneHotOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXPadOpToMhloPattern(patterns, ctx);
   populateLoweringONNXReshapeOpToMhloPattern(patterns, ctx);
+  populateLoweringONNXScatterNDOpToMhloPattern(patterns, ctx);
   populateLoweringONNXShapeOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSliceOpToMhloPattern(patterns, ctx);
   populateLoweringONNXSplitOpToMhloPattern(patterns, ctx);
@@ -89,7 +93,8 @@ void FrontendToMhloLoweringPass::runOnOperation() {
   // Added affine as some affine maps are generated by IndexExpression. It could
   // be disabled and/or replaced by shape max/min.
   target.addLegalDialect<mhlo::MhloDialect, func::FuncDialect,
-      arith::ArithDialect, shape::ShapeDialect, mlir::affine::AffineDialect>();
+      arith::ArithDialect, shape::ShapeDialect, affine::AffineDialect,
+      tensor::TensorDialect>();
   // Needed to support unsigned int computations. To be removed if we use a
   // scheme that does not rely on the UnrealizedConversionCastOp.
   target.addLegalOp<::mlir::UnrealizedConversionCastOp>();
diff --git a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
index 26c392b8..9d958ef5 100644
--- a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
@@ -66,6 +66,11 @@ struct MhloDialectOp<ONNXMaxOp> {
   using Op = mhlo::MaxOp;
 };

+template <>
+struct MhloDialectOp<ONNXMinOp> {
+  using Op = mhlo::MinOp;
+};
+
 template <>
 struct MhloDialectOp<ONNXMulOp> {
   using Op = mhlo::MulOp;
@@ -106,6 +111,11 @@ struct MhloDialectOp<ONNXTanhOp> {
   using Op = mhlo::TanhOp;
 };

+template <>
+struct MhloDialectOp<ONNXWhereOp> {
+  using Op = mhlo::SelectOp;
+};
+
 namespace {

 template <typename ONNXOp>
@@ -293,6 +303,40 @@ struct ONNXElementwiseBinaryOpLoweringToMhlo : public ConversionPattern {
   }
 };

+// ONNXPReluOp(x) = alpha * x if x < 0 else x.
+template <>
+struct ONNXElementwiseBinaryOpLoweringToMhlo<ONNXPReluOp>
+    : public ConversionPattern {
+  ONNXElementwiseBinaryOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(ONNXPReluOp::getOperationName(), 1, ctx) {}
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    Location loc = op->getLoc();
+    // Prior code here used the "analysis" version that did not generate code.
+    // Since code is actually not needed here at this time, one could use
+    // IndexExprBuilderForAnalysis createIE(loc) instead.
+    IndexExprBuilderForMhlo createShapeIE(rewriter, loc);
+    ONNXBroadcastOpShapeHelper shapeHelper(op, operands, &createShapeIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    int64_t outputRank = shapeHelper.outputRank;
+    llvm::SmallVector<Value, 4> broadcastedOperands =
+        getBroadcastedOperands(op, rewriter, loc, outputRank);
+    Value inp = broadcastedOperands[0];
+    Value broadcastedSlope = broadcastedOperands[1];
+    Type resultType = *op->result_type_begin();
+    Value PReluActivationVal =
+        rewriter.create<mhlo::MulOp>(loc, inp, broadcastedSlope);
+    Value broadcastedZero = getShapedZero(loc, rewriter, inp);
+    Value compareGtZero = rewriter.create<mhlo::CompareOp>(
+        loc, inp, broadcastedZero, mhlo::ComparisonDirection::GT);
+    Value resultOp = rewriter.create<mhlo::SelectOp>(
+        loc, resultType, compareGtZero, inp, PReluActivationVal);
+    rewriter.replaceOp(op, resultOp);
+    return success();
+  }
+};
+
 // Element-wise variadic ops lowering to Mhlo dialect.
 //===----------------------------------------------------------------------===//
 template <typename ElementwiseVariadicOp>
@@ -343,12 +387,15 @@ void populateLoweringONNXElementwiseOpToMhloPattern(
       ONNXElementwiseCompareBinaryOpLoweringToMhlo<ONNXLessOp>,
       ONNXElementwiseCompareBinaryOpLoweringToMhlo<ONNXLessOrEqualOp>,
       ONNXElementwiseBinaryOpLoweringToMhlo<ONNXPowOp>,
+      ONNXElementwiseBinaryOpLoweringToMhlo<ONNXPReluOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXAddOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXAndOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXDivOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMaxOp>,
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMinOp>,
       ONNXElementwiseVariadicOpLoweringToMhlo<ONNXMulOp>,
-      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXSubOp>>(ctx);
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXSubOp>,
+      ONNXElementwiseVariadicOpLoweringToMhlo<ONNXWhereOp>>(ctx);
 }

 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
index 4e4adfc1..652d6006 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.cpp
@@ -44,11 +44,6 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(Operation *op,
   Type outputType = *op->result_type_begin();
   assert(outputType.isa<ShapedType>() && "output type is not shaped");
   ShapedType outputShapedType = outputType.cast<ShapedType>();
-  Type elementType =
-      op->getOperands()[0].getType().dyn_cast<ShapedType>().getElementType();
-  RankedTensorType broadcastedOutputType =
-      RankedTensorType::get(outputShapedType.getShape(), elementType);
-
   Value resultExtents =
       mlir::hlo::computeNaryElementwiseBroadcastingResultExtents(
           loc, op->getOperands(), rewriter);
@@ -58,6 +53,10 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(Operation *op,
     assert(operandType != nullptr && "operand type is not ranked");
     SmallVector<int64_t, 4> broadcastDimensions = llvm::to_vector<4>(
         llvm::seq<int64_t>(outputRank - operandType.getRank(), outputRank));
+    Type elementType =
+        operand.getType().dyn_cast<ShapedType>().getElementType();
+    RankedTensorType broadcastedOutputType =
+        RankedTensorType::get(outputShapedType.getShape(), elementType);
     Value broadcast = rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc,
         broadcastedOutputType, operand, resultExtents,
         rewriter.getI64TensorAttr(broadcastDimensions));
@@ -72,11 +71,6 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   llvm::SmallVector<Value, 4> broadcastedOperands;
   assert(outputType.isa<ShapedType>() && "output type is not shaped");
   ShapedType outputShapedType = outputType.cast<ShapedType>();
-  Type elementType =
-      operands[0].getType().dyn_cast<ShapedType>().getElementType();
-  RankedTensorType broadcastedOutputType =
-      RankedTensorType::get(outputShapedType.getShape(), elementType);
-
   Value resultExtents =
       mlir::hlo::computeNaryElementwiseBroadcastingResultExtents(
           loc, operands, rewriter);
@@ -86,6 +80,10 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     assert(operandType != nullptr && "operand type is not ranked");
     SmallVector<int64_t, 4> broadcastDimensions = llvm::to_vector<4>(
         llvm::seq<int64_t>(outputRank - operandType.getRank(), outputRank));
+    Type elementType =
+        operand.getType().dyn_cast<ShapedType>().getElementType();
+    RankedTensorType broadcastedOutputType =
+        RankedTensorType::get(outputShapedType.getShape(), elementType);
     Value broadcast = rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc,
         broadcastedOutputType, operand, resultExtents,
         rewriter.getI64TensorAttr(broadcastDimensions));
@@ -93,4 +91,24 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
   }
   return broadcastedOperands;
 }
+
+ElementsAttr getElementAttributeFromMhloValue(Value value) {
+  auto definingOp = value.getDefiningOp();
+  if (auto constantOp = dyn_cast_or_null<mhlo::ConstantOp>(definingOp)) {
+    return constantOp.getValue().dyn_cast<ElementsAttr>();
+  } else if (auto constantOp =
+                 dyn_cast_or_null<mlir::ONNXConstantOp>(definingOp)) {
+    if (constantOp.getValue().has_value())
+      return constantOp.getValueAttr().dyn_cast<ElementsAttr>();
+  }
+  return nullptr;
+}
+
+DenseIntElementsAttr GetI64ElementsAttr(
+    ArrayRef<int64_t> values, Builder *builder) {
+  RankedTensorType ty = RankedTensorType::get(
+      {static_cast<int64_t>(values.size())}, builder->getIntegerType(64));
+  return DenseIntElementsAttr::get(ty, values);
+}
+
 } // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
index ec5a9f2b..c133ca7b 100644
--- a/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
+++ b/src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp
@@ -19,6 +19,7 @@
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
 #include "mlir/Dialect/Math/IR/Math.h"
 #include "mlir/Dialect/MemRef/IR/MemRef.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
 #include "mlir/IR/PatternMatch.h"
 #include "mlir/Pass/Pass.h"
 #include "mlir/Transforms/DialectConversion.h"
@@ -113,6 +114,11 @@ llvm::SmallVector<Value, 4> getBroadcastedOperands(
     llvm::SmallVector<Value, 4> &operands, Type outputType,
     ConversionPatternRewriter &rewriter, Location loc, int64_t outputRank);

+mlir::ElementsAttr getElementAttributeFromMhloValue(mlir::Value value);
+
+DenseIntElementsAttr GetI64ElementsAttr(
+    ArrayRef<int64_t> values, Builder *builder);
+
 // `Math` directory methods:
 void populateLoweringONNXClipOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
@@ -148,10 +154,17 @@ void populateLoweringONNXFlattenOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXGatherOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXGatherElementsOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXIdentityOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXOneHotOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXPadOpToMhloPattern(RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXReshapeOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
+void populateLoweringONNXScatterNDOpToMhloPattern(
+    RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXShapeOpToMhloPattern(
     RewritePatternSet &, MLIRContext *);
 void populateLoweringONNXSliceOpToMhloPattern(
diff --git a/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp b/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp
new file mode 100644
index 00000000..09d7a6c2
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/GatherElements.cpp
@@ -0,0 +1,139 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===-------- GatherElements.cpp - Lowering GatherElements Op -------------===//
+//
+// Copyright 2020-2022 The IBM Research Authors.
+//
+// =============================================================================
+//
+// This file lowers the ONNX GatherElements Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXGatherElementsOpLoweringToMhlo : public ConversionPattern {
+  ONNXGatherElementsOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(
+            mlir::ONNXGatherElementsOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+    ONNXGatherElementsOpAdaptor operandAdaptor(operands);
+    ONNXGatherElementsOp gatherOp = cast<ONNXGatherElementsOp>(op);
+    Location loc = op->getLoc();
+
+    IndexExprBuilderForMhlo createIE(rewriter, loc);
+    ONNXGatherElementsOpShapeHelper shapeHelper(op, operands, &createIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+
+    Type outputType = *op->result_type_begin();
+    assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+
+    // Operands and attributes.
+    Value data = operandAdaptor.getData();
+    Value indices = operandAdaptor.getIndices();
+    int64_t axisLit = gatherOp.getAxis();
+
+    ShapedType inputType = data.getType().cast<ShapedType>();
+    int64_t rank = inputType.getRank(); // indices has the same rank
+    ShapedType indicesType = indices.getType().cast<ShapedType>();
+    Type indexElemType = indicesType.getElementType();
+    // Negative value means counting dimensions from the back.
+    axisLit = axisLit < 0 ? axisLit + rank : axisLit;
+
+    // make sure all index values >= 0
+    Value zero = getShapedZero(loc, rewriter, indices);
+    Value inputShape = rewriter.create<shape::ShapeOfOp>(loc, data);
+    Value indicesShape = rewriter.create<shape::ShapeOfOp>(loc, indices);
+    Value axisDimSize =
+        rewriter.create<shape::GetExtentOp>(loc, inputShape, axisLit);
+    axisDimSize =
+        rewriter.create<arith::IndexCastOp>(loc, indexElemType, axisDimSize);
+    axisDimSize = rewriter.create<tensor::FromElementsOp>(loc, axisDimSize);
+    axisDimSize = rewriter.create<mhlo::ReshapeOp>(loc,
+        RankedTensorType::get(SmallVector<int64_t>{}, indexElemType),
+        axisDimSize);
+    Value broadcastedAxisDimSize =
+        rewriter.create<mhlo::DynamicBroadcastInDimOp>(loc, indicesType,
+            axisDimSize, indicesShape, rewriter.getI64TensorAttr({}));
+    Value isNegative = rewriter.create<mhlo::CompareOp>(
+        loc, indices, zero, mhlo::ComparisonDirection::LT);
+    Value positiveIndices = rewriter.create<mhlo::AddOp>(
+        loc, indicesType, indices, broadcastedAxisDimSize);
+    indices = rewriter.create<mhlo::SelectOp>(
+        loc, indicesType, isNegative, positiveIndices, indices);
+
+    // start indices
+    Value toConcatIndexShape;
+    SmallVector<Value> toConcatIndexShapeValueVec;
+    for (size_t i = 0; i < rank; i++) {
+      toConcatIndexShapeValueVec.push_back(
+          rewriter.create<shape::GetExtentOp>(loc, indicesShape, i));
+    }
+    toConcatIndexShapeValueVec.push_back(
+        rewriter.create<arith::ConstantIndexOp>(loc, 1));
+    toConcatIndexShape = rewriter.create<tensor::FromElementsOp>(
+        loc, toConcatIndexShapeValueVec);
+
+    ArrayRef<int64_t> indicesShapeVec = indicesType.getShape();
+    SmallVector<int64_t> toConcatIndexShapeVec(
+        indicesShapeVec.begin(), indicesShapeVec.end());
+    toConcatIndexShapeVec.push_back(1);
+    RankedTensorType toConcatIndexType =
+        RankedTensorType::get(toConcatIndexShapeVec, indexElemType);
+
+    SmallVector<Value> toConcat;
+    for (int64_t i = 0; i < inputType.getRank(); ++i) {
+      if (i == axisLit) {
+        toConcat.push_back(rewriter.create<mhlo::DynamicReshapeOp>(
+            loc, toConcatIndexType, indices, toConcatIndexShape));
+      } else {
+        toConcat.push_back(
+            rewriter.create<mhlo::DynamicIotaOp>(loc, toConcatIndexType,
+                toConcatIndexShape, rewriter.getI64IntegerAttr(i)));
+      }
+    }
+    auto gatherIndicies = rewriter.create<mhlo::ConcatenateOp>(
+        loc, toConcat, static_cast<uint64_t>(inputType.getRank()));
+
+    // dimsAttr
+    SmallVector<int64_t> collapsedDims;
+    SmallVector<int64_t> startIndexMap;
+    for (int64_t i = 0; i < rank; i++) {
+      collapsedDims.push_back(i);
+      startIndexMap.push_back(i);
+    }
+    auto dimsAttr = mhlo::GatherDimensionNumbersAttr::get(rewriter.getContext(),
+        /*offsetDims=*/{},
+        /*collapsedSliceDims=*/collapsedDims,
+        /*startIndexMap=*/startIndexMap,
+        /*indexVecDim=*/rank);
+    SmallVector<int64_t> sliceSizes(inputType.getRank(), 1);
+
+    Value gatherValue = rewriter.create<mhlo::GatherOp>(loc, outputType, data,
+        gatherIndicies, dimsAttr, rewriter.getI64TensorAttr(sliceSizes));
+    rewriter.replaceOp(op, gatherValue);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXGatherElementsOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXGatherElementsOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/OneHot.cpp b/src/Conversion/ONNXToMhlo/Tensor/OneHot.cpp
new file mode 100644
index 00000000..4da65847
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/OneHot.cpp
@@ -0,0 +1,127 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===---------------- OneHot.cpp - Lowering OneHot Op -------------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX OneHot Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/DialectBuilder.hpp"
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+#include <numeric>
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+struct ONNXOneHotOpLoweringToMhlo : public OpConversionPattern<ONNXOneHotOp> {
+  ONNXOneHotOpLoweringToMhlo(MLIRContext *ctx) : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXOneHotOp onehotOp,
+      ONNXOneHotOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = onehotOp.getOperation();
+    Location loc = ONNXLoc<ONNXOneHotOp>(op);
+    ValueRange operands = adaptor.getOperands();
+    Value indices = adaptor.getIndices();
+    Value depthValue = adaptor.getDepth();
+    Value values = adaptor.getValues();
+    Type outputType = *op->result_type_begin();
+
+    IndexExprBuilderForMhlo createIE(rewriter, loc);
+    ONNXOneHotOpShapeHelper shapeHelper(op, operands, &createIE);
+    shapeHelper.computeShapeAndAssertOnFailure();
+    int64_t axis = shapeHelper.axis;
+
+    RankedTensorType indicesType =
+        indices.getType().dyn_cast<RankedTensorType>();
+    if (!indicesType || !indicesType.hasStaticShape())
+      return failure();
+    ArrayRef<int64_t> indicesShape = indicesType.getShape();
+    Type indicesElementType = indicesType.getElementType();
+
+    DenseIntElementsAttr depthAttr;
+    if (!matchPattern(depthValue, m_Constant(&depthAttr))) {
+      return failure();
+    }
+
+    int64_t depth = depthAttr.getValues<APInt>()[0].getSExtValue();
+
+    llvm::SmallVector<int64_t, 4> broadcastDims(indicesShape.size());
+    std::iota(broadcastDims.begin(), broadcastDims.begin() + axis, 0);
+    std::iota(broadcastDims.begin() + axis, broadcastDims.end(), axis + 1);
+
+    llvm::SmallVector<int64_t, 4> outputDims = llvm::to_vector<4>(indicesShape);
+    outputDims.insert(outputDims.begin() + axis, depth);
+
+    RankedTensorType indexType =
+        RankedTensorType::get(llvm::ArrayRef(outputDims), indicesElementType);
+
+    Value iota = rewriter.create<mhlo::IotaOp>(
+        loc, indexType, IntegerAttr::get(rewriter.getIntegerType(64), axis));
+    Value broadcastIndices = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, indices, GetI64ElementsAttr(broadcastDims, &rewriter));
+    Value zero = rewriter.create<mhlo::ConstantOp>(loc,
+        DenseIntElementsAttr::get(RankedTensorType::get({}, indicesElementType),
+            ArrayRef<int64_t>{0}));
+    Value broadcastZero = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, zero, rewriter.getI64TensorAttr({}));
+    Value broadcastDepth;
+    int64_t depthRank = depthValue.getType().cast<RankedTensorType>().getRank();
+    if (depthRank == 1)
+      broadcastDepth = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, depthValue, rewriter.getI64TensorAttr({0}));
+    else
+      broadcastDepth = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, indexType, depthValue, rewriter.getI64TensorAttr({}));
+    Value compareGeZero = rewriter.create<mhlo::CompareOp>(
+        loc, broadcastIndices, broadcastZero, mhlo::ComparisonDirection::GE);
+    Value positiveIndices =
+        rewriter.create<mhlo::AddOp>(loc, broadcastIndices, broadcastDepth);
+    Value normalizedIndices = rewriter.create<mhlo::SelectOp>(
+        loc, indexType, compareGeZero, broadcastIndices, positiveIndices);
+    Value compare = rewriter.create<mhlo::CompareOp>(
+        loc, normalizedIndices, iota, mhlo::ComparisonDirection::EQ);
+    Type indexElementType = rewriter.getI64Type();
+    Type valueType = values.getType().cast<ShapedType>().getElementType();
+    Value offValue = rewriter.create<mhlo::SliceOp>(loc,
+        RankedTensorType::get({1}, valueType), values,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{0}),
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
+        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
+            ArrayRef<int64_t>{1}));
+    Value onValue = rewriter.create<mhlo::SliceOp>(loc,
+        RankedTensorType::get({1}, valueType), values,
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{1}),
+        DenseIntElementsAttr::get(
+            RankedTensorType::get({1}, indexElementType), ArrayRef<int64_t>{2}),
+        DenseIntElementsAttr::get(RankedTensorType::get({1}, indexElementType),
+            ArrayRef<int64_t>{1}));
+    Value offValueBroadcast = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, outputType, offValue, rewriter.getI64TensorAttr({0}));
+    Value onValueBroadcast = rewriter.create<mhlo::BroadcastInDimOp>(
+        loc, outputType, onValue, rewriter.getI64TensorAttr({0}));
+    Value result = rewriter.create<mhlo::SelectOp>(
+        loc, outputType, compare, onValueBroadcast, offValueBroadcast);
+    rewriter.replaceOp(op, {result});
+    return success();
+  }
+};
+
+void populateLoweringONNXOneHotOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXOneHotOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
new file mode 100644
index 00000000..1482e9ee
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/Pad.cpp
@@ -0,0 +1,103 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===----------- Pad.cpp - Lowering Pad Op ------------===//
+//
+// Copyright 2022
+//
+// =============================================================================
+//
+// This file lowers ONNX Pad Operators to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ElementsAttr/DisposableElementsAttr.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+#include "src/Support/TypeUtilities.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+namespace {
+
+struct ONNXPadOpLoweringToMhlo : public ConversionPattern {
+  ONNXPadOpLoweringToMhlo(MLIRContext *ctx)
+      : ConversionPattern(mlir::ONNXPadOp::getOperationName(), 1, ctx) {}
+
+  LogicalResult matchAndRewrite(Operation *op, ArrayRef<Value> operands,
+      ConversionPatternRewriter &rewriter) const final {
+
+    Location loc = op->getLoc();
+    ONNXPadOpAdaptor operandAdaptor(operands, op->getAttrDictionary());
+    Value data = operandAdaptor.getData();
+    Value constantValue = operandAdaptor.getConstantValue();
+    Value pads = operandAdaptor.getPads();
+    Value axes = operandAdaptor.getAxes();
+    StringRef padMode = operandAdaptor.getMode();
+
+    if (!padMode.equals_insensitive("constant"))
+        return failure();
+    // only support axes is None
+    if (!isa<ONNXNoneOp>(axes.getDefiningOp()))
+        return failure();
+    assert(isRankedShapedType(data.getType()) && "Expected Ranked ShapedType");
+    ShapedType inputType = data.getType().cast<ShapedType>();
+    Type elemType = inputType.getElementType();
+    int64_t rank = inputType.getRank();
+
+    Type outputType = *op->result_type_begin();
+    if (!constantValue || isNoneValue(constantValue)) {
+      // Pad with zeros by default
+      constantValue = rewriter.create<mhlo::ConstantOp>(
+          loc, DenseElementsAttr::get(mlir::RankedTensorType::get({}, elemType),
+              rewriter.getZeroAttr(elemType)));
+    } else {
+      // constantValue might be 1D tensor, reshape it to scalar
+      constantValue = rewriter.create<mhlo::ReshapeOp>(
+        loc, RankedTensorType::get({}, elemType), constantValue);
+    }
+    SmallVector<int64_t> edgePaddingLowVec(rank, 0);
+    SmallVector<int64_t> edgePaddingHighVec(rank, 0);
+    SmallVector<int64_t> interiorPaddingVec(rank, 0);
+    if (auto valueAttribute = getElementAttributeFromMhloValue(pads)) {
+      // If `pads` are constants, read them."
+      int64_t idx = 0;
+      for (IntegerAttr value : valueAttribute.getValues<IntegerAttr>()) {
+        int64_t padValue = value.getInt();
+        if (padValue < 0)
+          return failure();
+        if (idx < rank)
+          edgePaddingLowVec[idx] = padValue;
+        else
+          edgePaddingHighVec[idx - rank] = padValue;
+        idx++;
+      }
+    } else {
+      assert(false && "Pads must be known at compile time");
+    }
+
+    mlir::DenseIntElementsAttr edgePaddingLow =
+        rewriter.getI64VectorAttr(edgePaddingLowVec);
+    mlir::DenseIntElementsAttr edgePaddingHigh =
+        rewriter.getI64VectorAttr(edgePaddingHighVec);
+    mlir::DenseIntElementsAttr interiorPadding =
+        rewriter.getI64VectorAttr(interiorPaddingVec);
+    Value padResult = rewriter.create<mhlo::PadOp>(loc, outputType, data,
+        constantValue, edgePaddingLow, edgePaddingHigh, interiorPadding);
+
+    rewriter.replaceOp(op, padResult);
+    return success();
+  }
+};
+
+} // namespace
+
+void populateLoweringONNXPadOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXPadOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Conversion/ONNXToMhlo/Tensor/ScatterND.cpp b/src/Conversion/ONNXToMhlo/Tensor/ScatterND.cpp
new file mode 100644
index 00000000..acb95fe9
--- /dev/null
+++ b/src/Conversion/ONNXToMhlo/Tensor/ScatterND.cpp
@@ -0,0 +1,93 @@
+/*
+ * SPDX-License-Identifier: Apache-2.0
+ */
+
+//===--------------- ScatterND.cpp - Lowering ScatterND Op ----------------===//
+//
+// Copyright 2023
+//
+// =============================================================================
+//
+// This file lowers the ONNX ScatterND Operator to Mhlo dialect.
+//
+//===----------------------------------------------------------------------===//
+
+#include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp"
+
+using namespace mlir;
+
+namespace onnx_mlir {
+
+struct ONNXScatterNDOpLoweringToMhlo
+    : public OpConversionPattern<ONNXScatterNDOp> {
+  ONNXScatterNDOpLoweringToMhlo(MLIRContext *ctx) : OpConversionPattern(ctx) {}
+
+  LogicalResult matchAndRewrite(ONNXScatterNDOp scatterNDOp,
+      ONNXScatterNDOpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const final {
+    Operation *op = scatterNDOp.getOperation();
+    Location loc = ONNXLoc<ONNXScatterNDOp>(op);
+
+    // Operands and attributes.
+    Value data = adaptor.getData();
+    Value updates = adaptor.getUpdates();
+    Value indices = adaptor.getIndices();
+    auto dataType = data.getType().cast<ShapedType>();
+    auto indicesType = indices.getType().cast<ShapedType>();
+    auto updatesType = updates.getType().cast<ShapedType>();
+    int64_t dataRank = dataType.getRank();
+    int64_t updatesRank = updatesType.getRank();
+    int64_t indicesRank = indicesType.getRank();
+    assert(indicesType.hasStaticShape() &&
+           "only support indices with static shape");
+    int64_t partialIdxDim = indicesType.getDimSize(indicesRank - 1);
+
+    assert(dataRank >= 1 && "The rank of 'data' must be >= 1");
+    assert(indicesRank >= 1 && "The rank of 'indices' must be >= 1");
+
+    Type outputType = *op->result_type_begin();
+    assert(isRankedShapedType(outputType) && "Expected Ranked ShapedType");
+    ShapedType outputShapedType = outputType.cast<ShapedType>();
+    int64_t outputRank = outputShapedType.getRank();
+    assert(outputRank == dataRank && "Output rank not equal to data rank");
+    auto scatter_dimension_numbers =
+        mlir::mhlo::ScatterDimensionNumbersAttr::get(
+            /*context=*/rewriter.getContext(),
+            /*updateWindowDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(partialIdxDim, dataRank)),
+            /*insertedWindowDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(0, partialIdxDim)),
+            /*scatterDimsToOperandDims*/
+            llvm::to_vector<4>(llvm::seq<int64_t>(0, partialIdxDim)),
+            /*indexVectorDim=*/indicesRank - 1);
+    auto scatterOp = rewriter.create<mhlo::ScatterOp>(
+        loc, outputType, data, indices, updates, scatter_dimension_numbers);
+    // config update computation function: just return the element from src.
+    Block &block = scatterOp.getUpdateComputation().emplaceBlock();
+    // add block arguments
+    auto blockArgumentType =
+        RankedTensorType::get({}, dataType.getElementType());
+    block.addArgument(blockArgumentType, loc);
+    block.addArgument(blockArgumentType, loc);
+
+    auto *lhsArg = block.args_begin();
+    auto *rhsArg = std::next(lhsArg);
+
+    {
+      OpBuilder::InsertionGuard guard(rewriter);
+      rewriter.setInsertionPointToStart(&block);
+      rewriter.create<mhlo::ReturnOp>(loc, *rhsArg);
+    }
+
+    rewriter.replaceOp(op, scatterOp.getResults());
+    return success();
+  }
+};
+
+void populateLoweringONNXScatterNDOpToMhloPattern(
+    RewritePatternSet &patterns, MLIRContext *ctx) {
+  patterns.insert<ONNXScatterNDOpLoweringToMhlo>(ctx);
+}
+
+} // namespace onnx_mlir
diff --git a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
index beb50392..b821ec34 100644
--- a/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
+++ b/src/Dialect/ONNX/ONNXOps/ShapeHelper.hpp
@@ -544,6 +544,16 @@ struct ONNXPadOpShapeHelper : public ONNXOpShapeHelper {
   llvm::SmallVector<IndexExpr, 4> pads;
 };

+struct ONNXPadV13OpShapeHelper : public ONNXOpShapeHelper {
+  ONNXPadV13OpShapeHelper(mlir::Operation *op, mlir::ValueRange operands,
+      IndexExprBuilder *ieBuilder = nullptr, IndexExprScope *scope = nullptr)
+      : ONNXOpShapeHelper(op, operands, ieBuilder, scope), pads() {}
+  virtual ~ONNXPadV13OpShapeHelper() {}
+  mlir::LogicalResult computeShape() final;
+  // Additional data for PadOp.
+  llvm::SmallVector<IndexExpr, 4> pads;
+};
+
 //===----------------------------------------------------------------------===//
 // OneHot Op
 //===----------------------------------------------------------------------===//
diff --git a/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp b/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
index b00edc4a..f33bc18a 100644
--- a/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
+++ b/src/Dialect/ONNX/ONNXOps/Tensor/Pad.cpp
@@ -67,6 +67,49 @@ LogicalResult ONNXPadOpShapeHelper::computeShape() {
   return success();
 }

+LogicalResult ONNXPadV13OpShapeHelper::computeShape() {
+  ONNXPadOpAdaptor operandAdaptor(operands);
+  Value dataOperand = operandAdaptor.getData();
+  Value padsOperand = operandAdaptor.getPads();
+  DimsExpr outputDims;
+
+  // Get info about input data operand.
+  uint64_t dataRank = createIE->getShapedTypeRank(dataOperand);
+
+  // Initialize context and results (pads & output)
+  pads.resize(2 * dataRank); // pads two sides of each axis.
+  outputDims.resize(dataRank);
+
+  // `pads` format is : [x1_begin, x2_begin,...,x1_end, x2_end,...],
+  // where
+  // - xi_begin: the number of pad values added at the beginning of axis `i`
+  // - xi_end: the number of pad values added at the end of axis `i`.
+
+  // Calculate output dimension sizes.
+  for (uint64_t i = 0; i < dataRank; i++) {
+    // Get begin/end pads.
+    SymbolIndexExpr padBegin(createIE->getIntFromArrayAsSymbol(padsOperand, i));
+    SymbolIndexExpr padEnd(
+        createIE->getIntFromArrayAsSymbol(padsOperand, i + dataRank));
+    if (padBegin.isUndefined() || padEnd.isUndefined())
+      return op->emitError("pad parameter could not be processed");
+    // Get input dim.
+    DimIndexExpr dimInput(createIE->getShapeAsDim(dataOperand, i));
+
+    // Calculation for output size.
+    IndexExpr dimOutputFinal = (padBegin + dimInput) + padEnd;
+
+    // Save results.
+    pads[i] = padBegin;
+    pads[i + dataRank] = padEnd;
+    outputDims[i] = dimOutputFinal;
+  }
+
+  // Save the final result.
+  setOutputDims(outputDims);
+  return success();
+}
+
 } // namespace onnx_mlir

 //===----------------------------------------------------------------------===//
@@ -108,3 +151,15 @@ LogicalResult ONNXPadOp::inferShapes(
   ONNXPadOpShapeHelper shapeHelper(getOperation(), {});
   return shapeHelper.computeShapeAndUpdateType(elementType);
 }
+
+LogicalResult ONNXPadV13Op::inferShapes(
+    std::function<void(Region &)> doShapeInference) {
+  // Cannot infer shape if no shape exists.
+  if (!hasShapeAndRank(getData()) || !hasShapeAndRank(getPads()))
+    return success();
+
+  Type elementType = getData().getType().cast<ShapedType>().getElementType();
+
+  ONNXPadV13OpShapeHelper shapeHelper(getOperation(), {});
+  return shapeHelper.computeShapeAndUpdateType(elementType);
+}
diff --git a/src/Dialect/ONNX/ONNXUnsupportedOps.hpp b/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
index 758635e4..f5c07c76 100644
--- a/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
+++ b/src/Dialect/ONNX/ONNXUnsupportedOps.hpp
@@ -56,7 +56,6 @@ UNSUPPORTED_OPS(ONNXMomentumOp)
 UNSUPPORTED_OPS(ONNXMultinomialOp)
 UNSUPPORTED_OPS(ONNXNegativeLogLikelihoodLossOp)
 UNSUPPORTED_OPS(ONNXNormalizerOp)
-UNSUPPORTED_OPS(ONNXPadV13Op)
 UNSUPPORTED_OPS(ONNXPadV11Op)
 UNSUPPORTED_OPS(ONNXPadV2Op)
 UNSUPPORTED_OPS(ONNXRandomUniformLikeOp)
diff --git a/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir b/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
index 834471e3..ff14f8d8 100644
--- a/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
+++ b/test/mlir/conversion/onnx_to_mhlo/Math/Elementwise.mlir
@@ -256,6 +256,15 @@ func.func @test_max(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x10xf32>) -> ten
 // CHECK-NEXT:      return [[VAR_0_]] : tensor<10x10xf32>
 }

+func.func @test_min(%arg0 : tensor<10x10xf32>, %arg1 : tensor<10x10xf32>) -> tensor<10x10xf32> {
+  %0 = "onnx.Min"(%arg0, %arg1) : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<10x10xf32>
+  "func.return"(%0) : (tensor<10x10xf32>) -> ()
+// CHECK-LABEL:  func @test_min
+// CHECK-SAME:   ([[PARAM_0_:%.+]]: tensor<10x10xf32>, [[PARAM_1_:%.+]]: tensor<10x10xf32>) -> tensor<10x10xf32> {
+// CHECK-NEXT:      [[VAR_0_:%.+]] = mhlo.minimum [[PARAM_0_]], [[PARAM_1_]] : tensor<10x10xf32>
+// CHECK-NEXT:      return [[VAR_0_]] : tensor<10x10xf32>
+}
+
 func.func @test_leakyrelu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32> {
   %0 = "onnx.LeakyRelu"(%arg0) {alpha=0.5:f32} : (tensor<?x10xf32>) -> tensor<?x10xf32>
   "func.return"(%0) : (tensor<?x10xf32>) -> ()
@@ -275,6 +284,16 @@ func.func @test_leakyrelu_dynamic(%arg0 : tensor<?x10xf32>) -> tensor<?x10xf32>

 // -----

+func.func @test_prelu_dynamic(%arg0 : tensor<?x10x12x12xf32>, %arg1: tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32> {
+  %0 = "onnx.PRelu"(%arg0, %arg1) : (tensor<?x10x12x12xf32>, tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32>
+  "func.return"(%0) : (tensor<?x10x12x12xf32>) -> ()
+// CHECK-LABEL:  func.func @test_prelu_dynamic
+// CHECK-SAME:   (%arg0: tensor<?x10x12x12xf32>, %arg1: tensor<10x1x1xf32>) -> tensor<?x10x12x12xf32> {
+// CHECK:        [[VAR_0_:%.+]] = mhlo.multiply [[INP:%.+]], [[SLOPE:%.+]] : tensor<?x10x12x12xf32>
+// CHECK:        [[VAR_1_:%.+]] = mhlo.compare  GT, [[INP]], [[ZEROS:%.+]],  NOTYPE : (tensor<?x10x12x12xf32>, tensor<?x10x12x12xf32>) -> tensor<?x10x12x12xi1>
+// CHECK:        [[VAR_2_:%.+]] = mhlo.select [[VAR_1_]], [[INP]], [[VAR_0_]] : tensor<?x10x12x12xi1>, tensor<?x10x12x12xf32>
+}
+
 func.func @test_neg(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
   %0 = "onnx.Neg"(%arg0) : (tensor<10x10xf32>) -> tensor<10x10xf32>
   "func.return"(%0) : (tensor<10x10xf32>) -> ()
@@ -290,3 +309,10 @@ func.func @test_sin(%arg0 : tensor<10x10xf32>) -> tensor<10x10xf32> {
 // CHECK-LABEL: func @test_sin
 // CHECK: %0 = mhlo.sine %arg0 : tensor<10x10xf32>
 }
+
+func.func @test_where(%arg0 : tensor<16x24x36xi1>, %arg1 : tensor<16x24x36xi64>, %arg2 : tensor<16x24x36xi64>) -> tensor<16x24x36xi64> {
+  %0 = "onnx.Where"(%arg0, %arg1, %arg2) : (tensor<16x24x36xi1>, tensor<16x24x36xi64>, tensor<16x24x36xi64>) -> tensor<16x24x36xi64>
+  "func.return"(%0) : (tensor<16x24x36xi64>) -> ()
+// CHECK-LABEL: func.func @test_where
+// CHECK:   %0 = mhlo.select %arg0, %arg1, %arg2 : tensor<16x24x36xi1>, tensor<16x24x36xi64>
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/Tensor/OneHot.mlir b/test/mlir/conversion/onnx_to_mhlo/Tensor/OneHot.mlir
new file mode 100644
index 00000000..1d802b4f
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/Tensor/OneHot.mlir
@@ -0,0 +1,19 @@
+// RUN: onnx-mlir-opt --shape-inference --convert-onnx-to-mhlo %s --canonicalize -split-input-file | FileCheck %s
+
+func.func @test_onehot(%arg0 : tensor<2x3x4xi64>) -> tensor<*xi64> {
+  %0 = onnx.Constant dense<64> : tensor<1xi64>
+  %1 = onnx.Constant dense<[0, 1]> : tensor<2xi64>
+  %2 = "onnx.OneHot"(%arg0, %0, %1) {axis = -1 : si64} : (tensor<2x3x4xi64>, tensor<1xi64>, tensor<2xi64>) -> tensor<*xi64>
+  "func.return"(%2) : (tensor<*xi64>) -> ()
+// CHECK-LABEL: func.func @test_onehot
+// CHECK-SAME: (%[[ARG0:.+]]: tensor<2x3x4xi64>) -> tensor<2x3x4x64xi64> {
+// CHECK: %[[IOTA:.+]] = "mhlo.iota"() {iota_dimension = 0 : i64} : () -> tensor<64xi64>
+// CHECK: %[[BCAST_IOTA:.+]] = "mhlo.broadcast_in_dim"(%[[IOTA]]) {broadcast_dimensions = dense<3> : tensor<1xi64>} : (tensor<64xi64>) -> tensor<2x3x4x64xi64>
+// CHECK: %[[BCAST_ARG0:.+]] = "mhlo.broadcast_in_dim"(%[[ARG0]]) {broadcast_dimensions = dense<[0, 1, 2]> : tensor<3xi64>} : (tensor<2x3x4xi64>) -> tensor<2x3x4x64xi64>
+// CHECK: %[[GE_ZERO:.+]] = mhlo.compare  GE, %[[BCAST_ARG0]], %[[BCAST_ZERO:.+]],  NOTYPE : (tensor<2x3x4x64xi64>, tensor<2x3x4x64xi64>) -> tensor<2x3x4x64xi1>
+// CHECK: %[[POS_ARG:.+]] = mhlo.add %[[BCAST_ARG0]], %[[BCAST_DEPTH:.+]] : tensor<2x3x4x64xi64>
+// CHECK: %[[NORM_ARG:.+]] = mhlo.select %[[GE_ZERO]], %[[BCAST_ARG0]], %[[POS_ARG]] : tensor<2x3x4x64xi1>, tensor<2x3x4x64xi64>
+// CHECK: %[[COMPARE:.+]] = mhlo.compare EQ, %[[NORM_ARG]], %[[BCAST_IOTA]], NOTYPE : (tensor<2x3x4x64xi64>, tensor<2x3x4x64xi64>) -> tensor<2x3x4x64xi1>
+// CHECK: %[[RESULT:.+]] = mhlo.select %[[COMPARE]], %[[ON_VALUE:.+]], %[[OFF_VALUE:.+]] : tensor<2x3x4x64xi1>, tensor<2x3x4x64xi64>
+// CHECK: return %[[RESULT]] : tensor<2x3x4x64xi64>
+}
diff --git a/test/mlir/conversion/onnx_to_mhlo/Tensor/ScatterND.mlir b/test/mlir/conversion/onnx_to_mhlo/Tensor/ScatterND.mlir
new file mode 100644
index 00000000..bffb2b21
--- /dev/null
+++ b/test/mlir/conversion/onnx_to_mhlo/Tensor/ScatterND.mlir
@@ -0,0 +1,23 @@
+// RUN: onnx-mlir-opt --canonicalize --convert-onnx-to-mhlo %s -split-input-file | FileCheck %s
+
+func.func @test_scatternd_1(%arg0 : tensor<8xf32>, %arg1 : tensor<4x1xi64>, %arg2 : tensor<4xf32>) -> tensor<8xf32> {
+  %0 = "onnx.ScatterND"(%arg0, %arg1, %arg2) : (tensor<8xf32>, tensor<4x1xi64>, tensor<4xf32>) -> tensor<8xf32>
+  return %0 : tensor<8xf32>
+// CHECK-LABEL: func.func @test_scatternd_1(%arg0: tensor<8xf32>, %arg1: tensor<4x1xi64>, %arg2: tensor<4xf32>) -> tensor<8xf32> {
+// CHECK-NEXT:      %0 = "mhlo.scatter"(%arg0, %arg1, %arg2) ({
+// CHECK-NEXT:      ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
+// CHECK-NEXT:        mhlo.return %arg4 : tensor<f32>
+// CHECK-NEXT:      }) {indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<8xf32>, tensor<4x1xi64>, tensor<4xf32>) -> tensor<8xf32>
+// CHECK-NEXT:      return %0 : tensor<8xf32>
+}
+
+func.func @test_scatternd_2(%arg0 : tensor<4x4x4xi32>, %arg1 : tensor<2x1xi64>, %arg2 : tensor<2x4x4xi32>) -> tensor<4x4x4xi32> {
+  %0 = "onnx.ScatterND"(%arg0, %arg1, %arg2) : (tensor<4x4x4xi32>, tensor<2x1xi64>, tensor<2x4x4xi32>) -> tensor<4x4x4xi32>
+  return %0 : tensor<4x4x4xi32>
+// CHECK-LABEL: func.func @test_scatternd_2(%arg0: tensor<4x4x4xi32>, %arg1: tensor<2x1xi64>, %arg2: tensor<2x4x4xi32>) -> tensor<4x4x4xi32> {
+// CHECK-NEXT:      %0 = "mhlo.scatter"(%arg0, %arg1, %arg2) ({
+// CHECK-NEXT:      ^bb0(%arg3: tensor<i32>, %arg4: tensor<i32>):
+// CHECK-NEXT:        mhlo.return %arg4 : tensor<i32>
+// CHECK-NEXT:      }) {indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<update_window_dims = [1, 2], inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false} : (tensor<4x4x4xi32>, tensor<2x1xi64>, tensor<2x4x4xi32>) -> tensor<4x4x4xi32>
+// CHECK-NEXT:      return %0 : tensor<4x4x4xi32>
+}
