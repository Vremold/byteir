diff --git a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
index c669e5ec..26c392b8 100644
--- a/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
+++ b/src/Conversion/ONNXToMhlo/Math/Elementwise.cpp
@@ -71,6 +71,11 @@ struct MhloDialectOp<ONNXMulOp> {
   using Op = mhlo::MulOp;
 };
 
+template <>
+struct MhloDialectOp<ONNXNegOp> {
+  using Op = mhlo::NegOp;
+};
+
 template <>
 struct MhloDialectOp<ONNXPowOp> {
   using Op = mhlo::PowOp;
@@ -81,6 +86,11 @@ struct MhloDialectOp<ONNXSigmoidOp> {
   using Op = mhlo::LogisticOp;
 };
 
+template <>
+struct MhloDialectOp<ONNXSinOp> {
+  using Op = mhlo::SineOp;
+};
+
 template <>
 struct MhloDialectOp<ONNXSqrtOp> {
   using Op = mhlo::SqrtOp;
@@ -321,7 +331,9 @@ void populateLoweringONNXElementwiseOpToMhloPattern(
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXExpOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXLeakyReluOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXLogOp>,
+      ONNXElementwiseUnaryOpLoweringToMhlo<ONNXNegOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXSigmoidOp>,
+      ONNXElementwiseUnaryOpLoweringToMhlo<ONNXSinOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXSqrtOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXReluOp>,
       ONNXElementwiseUnaryOpLoweringToMhlo<ONNXTanhOp>,
diff --git a/src/Conversion/ONNXToMhlo/Math/Reduction.cpp b/src/Conversion/ONNXToMhlo/Math/Reduction.cpp
index 3b734e0b..5d8d393c 100644
--- a/src/Conversion/ONNXToMhlo/Math/Reduction.cpp
+++ b/src/Conversion/ONNXToMhlo/Math/Reduction.cpp
@@ -24,43 +24,41 @@ namespace {
 
 template <typename Op>
 Value getIdentityValue(
-    ConversionPatternRewriter &rewriter, Location loc, FloatType elemType) {
+    ConversionPatternRewriter &rewriter, Location loc, Type elemType) {
   return nullptr;
 }
 
 template <>
 Value getIdentityValue<ONNXReduceMaxOp>(
-    ConversionPatternRewriter &rewriter, Location loc, FloatType elemType) {
+    ConversionPatternRewriter &rewriter, Location loc, Type elemType) {
+  MathBuilder createMath(rewriter, loc);
   return rewriter.create<mhlo::ConstantOp>(
-      loc, rewriter.getFloatAttr(
-               elemType, APFloat::getInf(elemType.getFloatSemantics(),
-                             /*isNegative=*/true)));
+      loc, createMath.negativeInfAttr(elemType));
 }
 
 template <>
 Value getIdentityValue<ONNXReduceMinOp>(
-    ConversionPatternRewriter &rewriter, Location loc, FloatType elemType) {
+    ConversionPatternRewriter &rewriter, Location loc, Type elemType) {
+  MathBuilder createMath(rewriter, loc);
   return rewriter.create<mhlo::ConstantOp>(
-      loc, rewriter.getFloatAttr(
-               elemType, APFloat::getInf(elemType.getFloatSemantics(),
-                             /*isNegative=*/false)));
+      loc, createMath.positiveInfAttr(elemType));
 }
 
 template <>
 Value getIdentityValue<ONNXReduceSumOp>(
-    ConversionPatternRewriter &rewriter, Location loc, FloatType elemType) {
+    ConversionPatternRewriter &rewriter, Location loc, Type elemType) {
   return rewriter.create<mhlo::ConstantOp>(loc, rewriter.getZeroAttr(elemType));
 }
 
 template <>
 Value getIdentityValue<ONNXReduceSumV11Op>(
-    ConversionPatternRewriter &rewriter, Location loc, FloatType elemType) {
+    ConversionPatternRewriter &rewriter, Location loc, Type elemType) {
   return rewriter.create<mhlo::ConstantOp>(loc, rewriter.getZeroAttr(elemType));
 }
 
 template <>
 Value getIdentityValue<ONNXReduceMeanOp>(
-    ConversionPatternRewriter &rewriter, Location loc, FloatType elemType) {
+    ConversionPatternRewriter &rewriter, Location loc, Type elemType) {
   return rewriter.create<mhlo::ConstantOp>(loc, rewriter.getZeroAttr(elemType));
 }
 
@@ -86,10 +84,9 @@ llvm::SmallVector<int64_t, 4> getDefinedAxes<ONNXReduceSumOp>(Operation *op) {
   // Assume it is verified that axes are known. Convert DenseElementsAttr to
   // ArrayAttr.
   if (!isFromNone(axesValue) && getONNXConstantOp(axesValue)) {
-    mlir::DenseElementsAttr constAxes =
+    mlir::ElementsAttr constAxes =
         getONNXConstantOp(axesValue)
-            .getValueAttr()
-            .dyn_cast_or_null<mlir::DenseElementsAttr>();
+            .getValueAttr().dyn_cast_or_null<mlir::ElementsAttr>();
     for (mlir::IntegerAttr element : constAxes.getValues<IntegerAttr>())
       definedAxes.push_back(element.getInt());
     return definedAxes;
@@ -273,7 +270,7 @@ struct ONNXReductionOpLoweringToMhlo : public ConversionPattern {
     ShapedType outputType = resultType.cast<ShapedType>();
     if (outputType == nullptr)
       return failure();
-    FloatType elemType = inputType.getElementType().cast<FloatType>();
+    Type elemType = inputType.getElementType();
     Value identity = getIdentityValue<ONNXReductionOp>(rewriter, loc, elemType);
     int64_t inRank = inputType.getRank();
 
@@ -311,8 +308,13 @@ struct ONNXReductionOpLoweringToMhlo : public ConversionPattern {
         reduceResult =
             rewriter.create<mhlo::DivOp>(loc, reduceResult, reduceFactorValue);
       } else {
-        Value ones = rewriter.create<mhlo::ConstantOp>(
-            loc, rewriter.getFloatAttr(elemType, 1.0));
+        Value ones;
+        if (elemType.isa<IntegerType>())
+          ones = rewriter.create<mhlo::ConstantOp>(
+              loc, rewriter.getIntegerAttr(elemType, 1));
+        else
+          ones = rewriter.create<mhlo::ConstantOp>(
+              loc, rewriter.getFloatAttr(elemType, 1.0));
         Value inputShape = rewriter.create<shape::ShapeOfOp>(loc, input);
         Value broadcastedOne = rewriter.create<mhlo::DynamicBroadcastInDimOp>(
             loc, inputType, ones, inputShape, rewriter.getI64TensorAttr({}));
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Constant.cpp b/src/Conversion/ONNXToMhlo/Tensor/Constant.cpp
index 910c0d40..49ebfee0 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Constant.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Constant.cpp
@@ -13,6 +13,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "src/Conversion/ONNXToMhlo/ONNXToMhloCommon.hpp"
+#include "src/Dialect/ONNX/ElementsAttr/DisposableElementsAttr.hpp"
 
 using namespace mlir;
 
@@ -32,8 +33,12 @@ struct ONNXConstantOpLoweringToMhlo : public ConversionPattern {
     if (constantOp.getSparseValue().has_value())
       return constantOp.emitWarning("Only support dense values at this time");
     assert(constantOp.getValue().has_value() && "Value is not set");
-    Value result =
-        rewriter.create<mhlo::ConstantOp>(loc, constantOp.getValue().value());
+    Value result;
+    auto attr = constantOp.getValue().value();
+    if (auto disposable = attr.dyn_cast<DisposableElementsAttr>())
+      result = rewriter.create<mhlo::ConstantOp>(loc, disposable.toDenseElementsAttr());
+    else
+      result = rewriter.create<mhlo::ConstantOp>(loc, attr);
     rewriter.replaceOp(op, result);
     return success();
   }
diff --git a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
index 6352e180..861c57a4 100644
--- a/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
+++ b/src/Conversion/ONNXToMhlo/Tensor/Expand.cpp
@@ -53,8 +53,13 @@ struct ONNXExpandOpLoweringToMhlo : public ConversionPattern {
     int64_t outputRank = outputShapedType.getRank();
 
     Operation *shapeDefOp = shape.getDefiningOp();
-    Value ones = rewriter.create<mhlo::ConstantOp>(
-        loc, rewriter.getFloatAttr(elementType, 1.0));
+    Value ones;
+    if (elementType.isa<IntegerType>())
+      ones = rewriter.create<mhlo::ConstantOp>(
+          loc, rewriter.getIntegerAttr(elementType, 1));
+    else
+      ones = rewriter.create<mhlo::ConstantOp>(
+          loc, rewriter.getFloatAttr(elementType, 1.0));
     Value broadcastedOnes;
     if (ONNXShapeOp shapeOp = dyn_cast_or_null<ONNXShapeOp>(shapeDefOp)) {
       assert(shapeOp.getData().getType().isa<ShapedType>() &&
@@ -68,10 +73,8 @@ struct ONNXExpandOpLoweringToMhlo : public ConversionPattern {
     } else if (ONNXConstantOp shapeOp =
                    dyn_cast_or_null<ONNXConstantOp>(shapeDefOp)) {
       llvm::SmallVector<int64_t, 4> shapeValues;
-      mlir::DenseElementsAttr constShape =
-          getONNXConstantOp(shapeOp)
-              .getValueAttr()
-              .dyn_cast_or_null<mlir::DenseElementsAttr>();
+      mlir::ElementsAttr constShape =
+          shapeOp.getValueAttr().dyn_cast<ElementsAttr>();
       for (mlir::IntegerAttr element : constShape.getValues<IntegerAttr>())
         shapeValues.push_back(element.getInt());
       RankedTensorType broadcastedType =
diff --git a/src/Dialect/ONNX/ONNXOps/Quantize/DequantizeLinear.cpp b/src/Dialect/ONNX/ONNXOps/Quantize/DequantizeLinear.cpp
index 4ead6093..3f9002d2 100644
--- a/src/Dialect/ONNX/ONNXOps/Quantize/DequantizeLinear.cpp
+++ b/src/Dialect/ONNX/ONNXOps/Quantize/DequantizeLinear.cpp
@@ -68,8 +68,6 @@ LogicalResult ONNXDequantizeLinearOpShapeHelper::computeShape() {
     if (!outputDims[a].isLiteral()) {
       outputDims[a] = LiteralIndexExpr(d);
     }
-    llvm::dbgs() << "literal: " << outputDims[a].getLiteral() << " d = " << d
-                 << "\n";
     // Checked in verify.
     assert(outputDims[a].getLiteral() == d &&
            "x_scale and x_zero_point 1-D tensor length must match the input "
